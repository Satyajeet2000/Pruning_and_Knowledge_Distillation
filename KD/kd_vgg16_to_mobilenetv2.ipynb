{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kphlI_nRNb7i",
        "outputId": "2eafc57f-26c0-49e5-bc1d-1e893344f43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: codecarbon in /usr/local/lib/python3.10/dist-packages (2.7.4)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from codecarbon) (11.5.3)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.0.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from codecarbon) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.9.0.20241003)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: prompt_toolkit<=3.0.36,>=2.0 in /usr/local/lib/python3.10/dist-packages (from questionary->codecarbon) (3.0.36)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTaxkqsltpRz",
        "outputId": "c1b1bcb2-38c1-4c91-85ef-ca8a87eb6950"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzB2k71JpRpK",
        "outputId": "cda115c1-0e22-4f53-bf57-30910040653f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 40000\n",
            "Validation set size: 10000\n",
            "Test set size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 99.8MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 22:55:00] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:55:00] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:55:00] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:55:00] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:55:00] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:55:01] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:55:01] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:01] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:55:01]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:55:01]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 22:55:01]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 22:55:01]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:55:01]   CPU count: 2\n",
            "[codecarbon INFO @ 22:55:01]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:02]   GPU count: 1\n",
            "[codecarbon INFO @ 22:55:02]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:55:02] Saving emissions data to file /content/emissions.csv\n",
            "Epoch 1/30 - Training:   0%|          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/30 - Training:  21%|██        | 132/625 [00:14<00:59,  8.27it/s][codecarbon INFO @ 22:55:17] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:17] Energy consumed for all GPUs : 0.000235 kWh. Total GPU Power : 56.235059806619006 W\n",
            "[codecarbon INFO @ 22:55:17] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:17] 0.000432 kWh of electricity used since the beginning.\n",
            "Epoch 1/30 - Training:  45%|████▌     | 284/625 [00:29<00:35,  9.69it/s][codecarbon INFO @ 22:55:32] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:32] Energy consumed for all GPUs : 0.000494 kWh. Total GPU Power : 62.38596909861048 W\n",
            "[codecarbon INFO @ 22:55:32] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:32] 0.000888 kWh of electricity used since the beginning.\n",
            "Epoch 1/30 - Training:  69%|██████▉   | 432/625 [00:44<00:27,  6.92it/s][codecarbon INFO @ 22:55:47] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:47] Energy consumed for all GPUs : 0.000752 kWh. Total GPU Power : 61.960587184748306 W\n",
            "[codecarbon INFO @ 22:55:47] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:47] 0.001343 kWh of electricity used since the beginning.\n",
            "Epoch 1/30 - Training:  93%|█████████▎| 581/625 [00:59<00:04, 10.53it/s][codecarbon INFO @ 22:56:02] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:02] Energy consumed for all GPUs : 0.001014 kWh. Total GPU Power : 62.81251814185176 W\n",
            "[codecarbon INFO @ 22:56:02] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:02] 0.001802 kWh of electricity used since the beginning.\n",
            "Epoch 1/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.77it/s]\n",
            "Epoch 1/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/30 - Validation:  95%|█████████▍| 149/157 [00:10<00:00, 15.55it/s][codecarbon INFO @ 22:56:17] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:17] Energy consumed for all GPUs : 0.001215 kWh. Total GPU Power : 48.19245084924849 W\n",
            "[codecarbon INFO @ 22:56:17] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:17] 0.002199 kWh of electricity used since the beginning.\n",
            "Epoch 1/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Train Loss: 1.3502, Train Accuracy: 53.02%\n",
            "Epoch [1/30], Val Loss: 0.9097, Val Accuracy: 68.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30 - Training:  23%|██▎       | 141/625 [00:14<00:45, 10.70it/s][codecarbon INFO @ 22:56:32] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:32] Energy consumed for all GPUs : 0.001475 kWh. Total GPU Power : 62.55606156417331 W\n",
            "[codecarbon INFO @ 22:56:32] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:32] 0.002657 kWh of electricity used since the beginning.\n",
            "Epoch 2/30 - Training:  46%|████▋     | 290/625 [00:29<00:30, 10.88it/s][codecarbon INFO @ 22:56:47] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:47] Energy consumed for all GPUs : 0.001744 kWh. Total GPU Power : 64.61350633632375 W\n",
            "[codecarbon INFO @ 22:56:47] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 2/30 - Training:  47%|████▋     | 292/625 [00:29<00:30, 10.76it/s][codecarbon INFO @ 22:56:47] 0.003123 kWh of electricity used since the beginning.\n",
            "Epoch 2/30 - Training:  71%|███████   | 441/625 [00:44<00:16, 10.91it/s][codecarbon INFO @ 22:57:02] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:02] Energy consumed for all GPUs : 0.002013 kWh. Total GPU Power : 64.75421492002526 W\n",
            "[codecarbon INFO @ 22:57:02] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:02] 0.003588 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:57:02] 0.007998 g.CO2eq/s mean an estimation of 252.23591425852703 kg.CO2eq/year\n",
            "Epoch 2/30 - Training:  95%|█████████▍| 592/625 [00:59<00:02, 11.02it/s][codecarbon INFO @ 22:57:17] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:17] Energy consumed for all GPUs : 0.002282 kWh. Total GPU Power : 64.35325453708018 W\n",
            "[codecarbon INFO @ 22:57:17] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:17] 0.004054 kWh of electricity used since the beginning.\n",
            "Epoch 2/30 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.97it/s]\n",
            "Epoch 2/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 2/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30], Train Loss: 0.8434, Train Accuracy: 72.91%\n",
            "Epoch [2/30], Val Loss: 0.6709, Val Accuracy: 78.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30 - Training:   1%|          | 4/625 [00:00<01:48,  5.74it/s][codecarbon INFO @ 22:57:32] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:32] Energy consumed for all GPUs : 0.002478 kWh. Total GPU Power : 47.15068169191433 W\n",
            "Epoch 3/30 - Training:   1%|          | 6/625 [00:00<01:21,  7.63it/s][codecarbon INFO @ 22:57:32] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:32] 0.004447 kWh of electricity used since the beginning.\n",
            "Epoch 3/30 - Training:  25%|██▍       | 155/625 [00:15<00:42, 11.12it/s][codecarbon INFO @ 22:57:47] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:47] Energy consumed for all GPUs : 0.002745 kWh. Total GPU Power : 64.16133111987796 W\n",
            "[codecarbon INFO @ 22:57:47] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:47] 0.004911 kWh of electricity used since the beginning.\n",
            "Epoch 3/30 - Training:  49%|████▉     | 306/625 [00:30<00:28, 11.23it/s][codecarbon INFO @ 22:58:02] Energy consumed for RAM : 0.000237 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:02] Energy consumed for all GPUs : 0.003015 kWh. Total GPU Power : 64.96215249971868 W\n",
            "[codecarbon INFO @ 22:58:02] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:02] 0.005378 kWh of electricity used since the beginning.\n",
            "Epoch 3/30 - Training:  72%|███████▏  | 453/625 [00:45<00:15, 10.98it/s][codecarbon INFO @ 22:58:17] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:17] Energy consumed for all GPUs : 0.003285 kWh. Total GPU Power : 64.74316108816932 W\n",
            "[codecarbon INFO @ 22:58:17] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:17] 0.005844 kWh of electricity used since the beginning.\n",
            "Epoch 3/30 - Training:  97%|█████████▋| 604/625 [01:00<00:01, 11.32it/s][codecarbon INFO @ 22:58:32] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:32] Energy consumed for all GPUs : 0.003553 kWh. Total GPU Power : 64.27262456967765 W\n",
            "[codecarbon INFO @ 22:58:32] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:32] 0.006310 kWh of electricity used since the beginning.\n",
            "Epoch 3/30 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.94it/s]\n",
            "Epoch 3/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 3/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30], Train Loss: 0.7048, Train Accuracy: 77.12%\n",
            "Epoch [3/30], Val Loss: 0.6491, Val Accuracy: 78.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30 - Training:   3%|▎         | 16/625 [00:01<00:56, 10.79it/s][codecarbon INFO @ 22:58:47] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:47] Energy consumed for all GPUs : 0.003748 kWh. Total GPU Power : 46.85734871134298 W\n",
            "[codecarbon INFO @ 22:58:47] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:47] 0.006702 kWh of electricity used since the beginning.\n",
            "Epoch 4/30 - Training:  27%|██▋       | 167/625 [00:16<00:40, 11.22it/s][codecarbon INFO @ 22:59:02] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:02] Energy consumed for all GPUs : 0.004017 kWh. Total GPU Power : 64.48452518473901 W\n",
            "[codecarbon INFO @ 22:59:02] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:02] 0.007167 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:59:02] 0.007975 g.CO2eq/s mean an estimation of 251.50932165553414 kg.CO2eq/year\n",
            "Epoch 4/30 - Training:  51%|█████     | 317/625 [00:32<00:28, 10.94it/s][codecarbon INFO @ 22:59:17] Energy consumed for RAM : 0.000336 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:17] Energy consumed for all GPUs : 0.004286 kWh. Total GPU Power : 64.56401303960014 W\n",
            "[codecarbon INFO @ 22:59:17] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:17] 0.007633 kWh of electricity used since the beginning.\n",
            "Epoch 4/30 - Training:  75%|███████▍  | 466/625 [00:47<00:14, 11.13it/s][codecarbon INFO @ 22:59:32] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:32] Energy consumed for all GPUs : 0.004556 kWh. Total GPU Power : 64.92258669042295 W\n",
            "[codecarbon INFO @ 22:59:32] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:32] 0.008101 kWh of electricity used since the beginning.\n",
            "Epoch 4/30 - Training:  99%|█████████▊| 616/625 [01:02<00:00, 10.92it/s][codecarbon INFO @ 22:59:47] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:47] Energy consumed for all GPUs : 0.004824 kWh. Total GPU Power : 64.30872962594204 W\n",
            "[codecarbon INFO @ 22:59:47] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:47] 0.008566 kWh of electricity used since the beginning.\n",
            "Epoch 4/30 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.95it/s]\n",
            "Epoch 4/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 4/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30], Train Loss: 0.6388, Train Accuracy: 79.17%\n",
            "Epoch [4/30], Val Loss: 0.5965, Val Accuracy: 80.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30 - Training:   4%|▍         | 27/625 [00:02<00:54, 10.99it/s][codecarbon INFO @ 23:00:02] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:00:02] Energy consumed for all GPUs : 0.005020 kWh. Total GPU Power : 47.04314152457144 W\n",
            "[codecarbon INFO @ 23:00:02] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:00:02] 0.008959 kWh of electricity used since the beginning.\n",
            "Epoch 5/30 - Training:  28%|██▊       | 174/625 [00:18<00:41, 10.88it/s][codecarbon INFO @ 23:00:17] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:00:17] Energy consumed for all GPUs : 0.005283 kWh. Total GPU Power : 63.15291303715398 W\n",
            "[codecarbon INFO @ 23:00:17] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:00:17] 0.009419 kWh of electricity used since the beginning.\n",
            "Epoch 5/30 - Training:  51%|█████     | 320/625 [00:33<00:28, 10.61it/s][codecarbon INFO @ 23:00:32] Energy consumed for RAM : 0.000435 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:00:32] Energy consumed for all GPUs : 0.005552 kWh. Total GPU Power : 64.55284607091814 W\n",
            "[codecarbon INFO @ 23:00:32] Energy consumed for all CPUs : 0.003897 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:00:32] 0.009884 kWh of electricity used since the beginning.\n",
            "Epoch 5/30 - Training:  75%|███████▍  | 467/625 [00:48<00:15, 10.52it/s][codecarbon INFO @ 23:00:47] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:00:47] Energy consumed for all GPUs : 0.005822 kWh. Total GPU Power : 64.97598434724854 W\n",
            "[codecarbon INFO @ 23:00:47] Energy consumed for all CPUs : 0.004074 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:00:47] 0.010352 kWh of electricity used since the beginning.\n",
            "Epoch 5/30 - Training:  98%|█████████▊| 613/625 [01:03<00:01, 10.81it/s][codecarbon INFO @ 23:01:02] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:01:02] Energy consumed for all GPUs : 0.006088 kWh. Total GPU Power : 63.78174738178446 W\n",
            "[codecarbon INFO @ 23:01:02] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:01:02] 0.010814 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:01:02] 0.008127 g.CO2eq/s mean an estimation of 256.27985592476466 kg.CO2eq/year\n",
            "Epoch 5/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.73it/s]\n",
            "Epoch 5/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 5/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Train Loss: 0.5823, Train Accuracy: 81.04%\n",
            "Epoch [5/30], Val Loss: 0.5844, Val Accuracy: 81.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30 - Training:   3%|▎         | 20/625 [00:02<01:02,  9.75it/s][codecarbon INFO @ 23:01:17] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:01:17] Energy consumed for all GPUs : 0.006279 kWh. Total GPU Power : 45.925419937243184 W\n",
            "[codecarbon INFO @ 23:01:17] Energy consumed for all CPUs : 0.004428 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:01:17] 0.011202 kWh of electricity used since the beginning.\n",
            "Epoch 6/30 - Training:  27%|██▋       | 168/625 [00:17<00:41, 10.94it/s][codecarbon INFO @ 23:01:32] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 6/30 - Training:  27%|██▋       | 170/625 [00:17<00:42, 10.83it/s][codecarbon INFO @ 23:01:32] Energy consumed for all GPUs : 0.006546 kWh. Total GPU Power : 63.86369880498191 W\n",
            "[codecarbon INFO @ 23:01:32] Energy consumed for all CPUs : 0.004606 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:01:32] 0.011666 kWh of electricity used since the beginning.\n",
            "Epoch 6/30 - Training:  51%|█████     | 317/625 [00:32<00:28, 10.78it/s][codecarbon INFO @ 23:01:47] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:01:47] Energy consumed for all GPUs : 0.006815 kWh. Total GPU Power : 64.73187869034881 W\n",
            "[codecarbon INFO @ 23:01:47] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:01:47] 0.012133 kWh of electricity used since the beginning.\n",
            "Epoch 6/30 - Training:  75%|███████▍  | 466/625 [00:47<00:14, 10.95it/s][codecarbon INFO @ 23:02:02] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:02:02] Energy consumed for all GPUs : 0.007084 kWh. Total GPU Power : 64.60161516466582 W\n",
            "[codecarbon INFO @ 23:02:02] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:02:02] 0.012598 kWh of electricity used since the beginning.\n",
            "Epoch 6/30 - Training:  99%|█████████▉| 618/625 [01:02<00:00, 11.19it/s][codecarbon INFO @ 23:02:17] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:02:17] Energy consumed for all GPUs : 0.007354 kWh. Total GPU Power : 64.80414428377877 W\n",
            "[codecarbon INFO @ 23:02:17] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:02:17] 0.013064 kWh of electricity used since the beginning.\n",
            "Epoch 6/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.89it/s]\n",
            "Epoch 6/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 6/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30], Train Loss: 0.5440, Train Accuracy: 82.16%\n",
            "Epoch [6/30], Val Loss: 0.5435, Val Accuracy: 82.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30 - Training:   5%|▍         | 29/625 [00:03<00:54, 11.01it/s][codecarbon INFO @ 23:02:32] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:02:32] Energy consumed for all GPUs : 0.007548 kWh. Total GPU Power : 46.621339836139846 W\n",
            "[codecarbon INFO @ 23:02:32] Energy consumed for all CPUs : 0.005314 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:02:32] 0.013456 kWh of electricity used since the beginning.\n",
            "Epoch 7/30 - Training:  28%|██▊       | 177/625 [00:18<00:40, 11.16it/s][codecarbon INFO @ 23:02:47] Energy consumed for RAM : 0.000613 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:02:47] Energy consumed for all GPUs : 0.007814 kWh. Total GPU Power : 63.9717725510934 W\n",
            "[codecarbon INFO @ 23:02:47] Energy consumed for all CPUs : 0.005491 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:02:47] 0.013919 kWh of electricity used since the beginning.\n",
            "Epoch 7/30 - Training:  52%|█████▏    | 324/625 [00:33<00:27, 10.79it/s][codecarbon INFO @ 23:03:02] Energy consumed for RAM : 0.000633 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:03:02] Energy consumed for all GPUs : 0.008083 kWh. Total GPU Power : 64.38944099274522 W\n",
            "[codecarbon INFO @ 23:03:02] Energy consumed for all CPUs : 0.005668 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:03:02] 0.014384 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:03:02] 0.007956 g.CO2eq/s mean an estimation of 250.91005389883946 kg.CO2eq/year\n",
            "Epoch 7/30 - Training:  76%|███████▌  | 474/625 [00:48<00:13, 11.19it/s][codecarbon INFO @ 23:03:17] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:03:17] Energy consumed for all GPUs : 0.008352 kWh. Total GPU Power : 64.69486188857243 W\n",
            "[codecarbon INFO @ 23:03:17] Energy consumed for all CPUs : 0.005845 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:03:17] 0.014850 kWh of electricity used since the beginning.\n",
            "Epoch 7/30 - Training: 100%|█████████▉| 624/625 [01:03<00:00, 11.87it/s][codecarbon INFO @ 23:03:32] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:03:32] Energy consumed for all GPUs : 0.008620 kWh. Total GPU Power : 64.44490400395385 W\n",
            "[codecarbon INFO @ 23:03:32] Energy consumed for all CPUs : 0.006022 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:03:32] 0.015315 kWh of electricity used since the beginning.\n",
            "Epoch 7/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.87it/s]\n",
            "Epoch 7/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 7/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Train Loss: 0.5105, Train Accuracy: 83.38%\n",
            "Epoch [7/30], Val Loss: 0.5357, Val Accuracy: 82.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30 - Training:   6%|▌         | 35/625 [00:03<00:53, 10.95it/s][codecarbon INFO @ 23:03:47] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:03:47] Energy consumed for all GPUs : 0.008815 kWh. Total GPU Power : 46.83070062756882 W\n",
            "[codecarbon INFO @ 23:03:47] Energy consumed for all CPUs : 0.006200 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:03:47] 0.015708 kWh of electricity used since the beginning.\n",
            "Epoch 8/30 - Training:  29%|██▉       | 183/625 [00:18<00:40, 10.99it/s][codecarbon INFO @ 23:04:02] Energy consumed for RAM : 0.000712 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:04:02] Energy consumed for all GPUs : 0.009080 kWh. Total GPU Power : 63.725664463915365 W\n",
            "[codecarbon INFO @ 23:04:02] Energy consumed for all CPUs : 0.006376 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:04:02] 0.016169 kWh of electricity used since the beginning.\n",
            "Epoch 8/30 - Training:  53%|█████▎    | 332/625 [00:33<00:27, 10.82it/s][codecarbon INFO @ 23:04:17] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:04:17] Energy consumed for all GPUs : 0.009349 kWh. Total GPU Power : 64.32025730029915 W\n",
            "[codecarbon INFO @ 23:04:17] Energy consumed for all CPUs : 0.006554 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:04:17] 0.016634 kWh of electricity used since the beginning.\n",
            "Epoch 8/30 - Training:  76%|███████▋  | 478/625 [00:48<00:13, 10.91it/s][codecarbon INFO @ 23:04:32] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:04:32] Energy consumed for all GPUs : 0.009617 kWh. Total GPU Power : 64.55326680422264 W\n",
            "[codecarbon INFO @ 23:04:32] Energy consumed for all CPUs : 0.006731 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:04:32] 0.017100 kWh of electricity used since the beginning.\n",
            "Epoch 8/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.83it/s]\n",
            "Epoch 8/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 23:04:47] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:04:47] Energy consumed for all GPUs : 0.009883 kWh. Total GPU Power : 63.80545990032005 W\n",
            "[codecarbon INFO @ 23:04:47] Energy consumed for all CPUs : 0.006908 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:04:47] 0.017563 kWh of electricity used since the beginning.\n",
            "Epoch 8/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30], Train Loss: 0.4953, Train Accuracy: 83.95%\n",
            "Epoch [8/30], Val Loss: 0.4984, Val Accuracy: 83.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30 - Training:   6%|▌         | 37/625 [00:03<00:53, 11.00it/s][codecarbon INFO @ 23:05:02] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:05:02] Energy consumed for all GPUs : 0.010078 kWh. Total GPU Power : 46.830258845519616 W\n",
            "[codecarbon INFO @ 23:05:02] Energy consumed for all CPUs : 0.007085 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:05:02] 0.017955 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:05:02] 0.007959 g.CO2eq/s mean an estimation of 250.9953896725362 kg.CO2eq/year\n",
            "Epoch 9/30 - Training:  29%|██▉       | 184/625 [00:18<00:43, 10.21it/s][codecarbon INFO @ 23:05:17] Energy consumed for RAM : 0.000811 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:05:17] Energy consumed for all GPUs : 0.010344 kWh. Total GPU Power : 63.77520523194874 W\n",
            "[codecarbon INFO @ 23:05:17] Energy consumed for all CPUs : 0.007262 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:05:17] 0.018417 kWh of electricity used since the beginning.\n",
            "Epoch 9/30 - Training:  54%|█████▎    | 335/625 [00:33<00:29,  9.69it/s][codecarbon INFO @ 23:05:32] Energy consumed for RAM : 0.000831 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:05:32] Energy consumed for all GPUs : 0.010613 kWh. Total GPU Power : 64.66024580848597 W\n",
            "[codecarbon INFO @ 23:05:32] Energy consumed for all CPUs : 0.007439 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:05:32] 0.018883 kWh of electricity used since the beginning.\n",
            "Epoch 9/30 - Training:  77%|███████▋  | 484/625 [00:48<00:14,  9.59it/s][codecarbon INFO @ 23:05:47] Energy consumed for RAM : 0.000851 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:05:47] Energy consumed for all GPUs : 0.010881 kWh. Total GPU Power : 64.57052634333677 W\n",
            "[codecarbon INFO @ 23:05:47] Energy consumed for all CPUs : 0.007616 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:05:47] 0.019348 kWh of electricity used since the beginning.\n",
            "Epoch 9/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.89it/s]\n",
            "Epoch 9/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 9/30 - Validation:   3%|▎         | 5/157 [00:00<00:20,  7.48it/s][codecarbon INFO @ 23:06:02] Energy consumed for RAM : 0.000871 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:06:02] Energy consumed for all GPUs : 0.011146 kWh. Total GPU Power : 63.50902488174022 W\n",
            "[codecarbon INFO @ 23:06:02] Energy consumed for all CPUs : 0.007793 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:06:02] 0.019810 kWh of electricity used since the beginning.\n",
            "Epoch 9/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Train Loss: 0.4495, Train Accuracy: 85.14%\n",
            "Epoch [9/30], Val Loss: 0.4857, Val Accuracy: 84.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30 - Training:   6%|▋         | 40/625 [00:04<01:09,  8.47it/s][codecarbon INFO @ 23:06:17] Energy consumed for RAM : 0.000890 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:06:17] Energy consumed for all GPUs : 0.011341 kWh. Total GPU Power : 46.740368511978694 W\n",
            "[codecarbon INFO @ 23:06:17] Energy consumed for all CPUs : 0.007970 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:06:17] 0.020202 kWh of electricity used since the beginning.\n",
            "Epoch 10/30 - Training:  30%|██▉       | 186/625 [00:19<00:49,  8.83it/s][codecarbon INFO @ 23:06:32] Energy consumed for RAM : 0.000910 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:06:32] Energy consumed for all GPUs : 0.011605 kWh. Total GPU Power : 63.32882000487867 W\n",
            "Epoch 10/30 - Training:  30%|██▉       | 187/625 [00:19<00:50,  8.76it/s][codecarbon INFO @ 23:06:32] Energy consumed for all CPUs : 0.008148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:06:32] 0.020663 kWh of electricity used since the beginning.\n",
            "Epoch 10/30 - Training:  54%|█████▎    | 335/625 [00:34<00:37,  7.78it/s][codecarbon INFO @ 23:06:47] Energy consumed for RAM : 0.000930 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:06:47] Energy consumed for all GPUs : 0.011874 kWh. Total GPU Power : 64.79665406482039 W\n",
            "[codecarbon INFO @ 23:06:47] Energy consumed for all CPUs : 0.008324 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:06:47] 0.021129 kWh of electricity used since the beginning.\n",
            "Epoch 10/30 - Training:  77%|███████▋  | 484/625 [00:49<00:16,  8.65it/s][codecarbon INFO @ 23:07:02] Energy consumed for RAM : 0.000950 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:07:02] Energy consumed for all GPUs : 0.012143 kWh. Total GPU Power : 64.55586202264101 W\n",
            "[codecarbon INFO @ 23:07:02] Energy consumed for all CPUs : 0.008502 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:07:02] 0.021595 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:07:02] 0.008111 g.CO2eq/s mean an estimation of 255.77608337484304 kg.CO2eq/year\n",
            "Epoch 10/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.86it/s]\n",
            "Epoch 10/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 10/30 - Validation:   5%|▌         | 8/157 [00:01<00:15,  9.50it/s][codecarbon INFO @ 23:07:17] Energy consumed for RAM : 0.000970 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:07:17] Energy consumed for all GPUs : 0.012407 kWh. Total GPU Power : 63.41677067866934 W\n",
            "[codecarbon INFO @ 23:07:17] Energy consumed for all CPUs : 0.008679 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:07:17] 0.022056 kWh of electricity used since the beginning.\n",
            "Epoch 10/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Train Loss: 0.4375, Train Accuracy: 85.86%\n",
            "Epoch [10/30], Val Loss: 0.5000, Val Accuracy: 83.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30 - Training:   7%|▋         | 42/625 [00:05<01:13,  7.91it/s][codecarbon INFO @ 23:07:32] Energy consumed for RAM : 0.000989 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:07:32] Energy consumed for all GPUs : 0.012604 kWh. Total GPU Power : 47.09115646942866 W\n",
            "[codecarbon INFO @ 23:07:32] Energy consumed for all CPUs : 0.008856 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:07:32] 0.022449 kWh of electricity used since the beginning.\n",
            "Epoch 11/30 - Training:  31%|███       | 191/625 [00:20<00:49,  8.73it/s][codecarbon INFO @ 23:07:47] Energy consumed for RAM : 0.001009 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:07:47] Energy consumed for all GPUs : 0.012869 kWh. Total GPU Power : 63.786926567479554 W\n",
            "[codecarbon INFO @ 23:07:47] Energy consumed for all CPUs : 0.009033 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:07:47] 0.022912 kWh of electricity used since the beginning.\n",
            "Epoch 11/30 - Training:  54%|█████▍    | 338/625 [00:35<00:38,  7.43it/s][codecarbon INFO @ 23:08:02] Energy consumed for RAM : 0.001029 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:08:02] Energy consumed for all GPUs : 0.013140 kWh. Total GPU Power : 65.00712822114488 W\n",
            "[codecarbon INFO @ 23:08:02] Energy consumed for all CPUs : 0.009210 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:08:02] 0.023379 kWh of electricity used since the beginning.\n",
            "Epoch 11/30 - Training:  78%|███████▊  | 486/625 [00:49<00:17,  7.92it/s][codecarbon INFO @ 23:08:17] Energy consumed for RAM : 0.001049 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 11/30 - Training:  78%|███████▊  | 487/625 [00:50<00:18,  7.49it/s][codecarbon INFO @ 23:08:17] Energy consumed for all GPUs : 0.013410 kWh. Total GPU Power : 64.88646131878596 W\n",
            "[codecarbon INFO @ 23:08:17] Energy consumed for all CPUs : 0.009387 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:08:17] 0.023846 kWh of electricity used since the beginning.\n",
            "Epoch 11/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.80it/s]\n",
            "Epoch 11/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 11/30 - Validation:   6%|▌         | 9/157 [00:01<00:19,  7.71it/s][codecarbon INFO @ 23:08:32] Energy consumed for RAM : 0.001069 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:08:32] Energy consumed for all GPUs : 0.013673 kWh. Total GPU Power : 63.345325884483664 W\n",
            "[codecarbon INFO @ 23:08:32] Energy consumed for all CPUs : 0.009564 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:08:32] 0.024306 kWh of electricity used since the beginning.\n",
            "Epoch 11/30 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30], Train Loss: 0.4247, Train Accuracy: 86.20%\n",
            "Epoch [11/30], Val Loss: 0.5165, Val Accuracy: 83.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30 - Training:   8%|▊         | 48/625 [00:06<01:20,  7.14it/s][codecarbon INFO @ 23:08:47] Energy consumed for RAM : 0.001088 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:08:47] Energy consumed for all GPUs : 0.013876 kWh. Total GPU Power : 48.820587249195235 W\n",
            "[codecarbon INFO @ 23:08:47] Energy consumed for all CPUs : 0.009742 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:08:47] 0.024706 kWh of electricity used since the beginning.\n",
            "Epoch 12/30 - Training:  31%|███       | 194/625 [00:21<01:00,  7.11it/s][codecarbon INFO @ 23:09:02] Energy consumed for RAM : 0.001108 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:09:02] Energy consumed for all GPUs : 0.014141 kWh. Total GPU Power : 63.64971733203727 W\n",
            "[codecarbon INFO @ 23:09:02] Energy consumed for all CPUs : 0.009919 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:09:02] 0.025168 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:09:02] 0.007961 g.CO2eq/s mean an estimation of 251.04713368382212 kg.CO2eq/year\n",
            "Epoch 12/30 - Training:  54%|█████▍    | 339/625 [00:36<00:36,  7.83it/s][codecarbon INFO @ 23:09:17] Energy consumed for RAM : 0.001128 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:09:17] Energy consumed for all GPUs : 0.014410 kWh. Total GPU Power : 64.56882077502311 W\n",
            "[codecarbon INFO @ 23:09:17] Energy consumed for all CPUs : 0.010096 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:09:17] 0.025634 kWh of electricity used since the beginning.\n",
            "Epoch 12/30 - Training:  78%|███████▊  | 485/625 [00:51<00:18,  7.57it/s][codecarbon INFO @ 23:09:32] Energy consumed for RAM : 0.001148 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:09:32] Energy consumed for all GPUs : 0.014678 kWh. Total GPU Power : 64.274706063798 W\n",
            "[codecarbon INFO @ 23:09:32] Energy consumed for all CPUs : 0.010273 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:09:32] 0.026099 kWh of electricity used since the beginning.\n",
            "Epoch 12/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.66it/s]\n",
            "Epoch 12/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 12/30 - Validation:  10%|█         | 16/157 [00:01<00:09, 14.71it/s][codecarbon INFO @ 23:09:47] Energy consumed for RAM : 0.001167 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:09:47] Energy consumed for all GPUs : 0.014943 kWh. Total GPU Power : 63.46630615582918 W\n",
            "[codecarbon INFO @ 23:09:47] Energy consumed for all CPUs : 0.010450 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:09:47] 0.026561 kWh of electricity used since the beginning.\n",
            "Epoch 12/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30], Train Loss: 0.4151, Train Accuracy: 86.44%\n",
            "Epoch [12/30], Val Loss: 0.5089, Val Accuracy: 83.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30 - Training:   8%|▊         | 51/625 [00:06<00:52, 10.97it/s][codecarbon INFO @ 23:10:02] Energy consumed for RAM : 0.001187 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:10:02] Energy consumed for all GPUs : 0.015149 kWh. Total GPU Power : 49.54546108818399 W\n",
            "[codecarbon INFO @ 23:10:02] Energy consumed for all CPUs : 0.010627 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:10:02] 0.026964 kWh of electricity used since the beginning.\n",
            "Epoch 13/30 - Training:  32%|███▏      | 198/625 [00:21<00:40, 10.52it/s][codecarbon INFO @ 23:10:17] Energy consumed for RAM : 0.001207 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:10:17] Energy consumed for all GPUs : 0.015416 kWh. Total GPU Power : 64.06367154805773 W\n",
            "[codecarbon INFO @ 23:10:17] Energy consumed for all CPUs : 0.010804 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:10:17] 0.027427 kWh of electricity used since the beginning.\n",
            "Epoch 13/30 - Training:  55%|█████▌    | 345/625 [00:36<00:26, 10.52it/s][codecarbon INFO @ 23:10:32] Energy consumed for RAM : 0.001227 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:10:32] Energy consumed for all GPUs : 0.015684 kWh. Total GPU Power : 64.48789984237554 W\n",
            "[codecarbon INFO @ 23:10:32] Energy consumed for all CPUs : 0.010981 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:10:32] 0.027892 kWh of electricity used since the beginning.\n",
            "Epoch 13/30 - Training:  79%|███████▊  | 491/625 [00:51<00:12, 10.54it/s][codecarbon INFO @ 23:10:47] Energy consumed for RAM : 0.001247 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:10:47] Energy consumed for all GPUs : 0.015953 kWh. Total GPU Power : 64.55444095077844 W\n",
            "[codecarbon INFO @ 23:10:47] Energy consumed for all CPUs : 0.011159 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:10:47] 0.028358 kWh of electricity used since the beginning.\n",
            "Epoch 13/30 - Training: 100%|██████████| 625/625 [01:05<00:00,  9.52it/s]\n",
            "Epoch 13/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 13/30 - Validation:   8%|▊         | 13/157 [00:01<00:09, 14.95it/s][codecarbon INFO @ 23:11:02] Energy consumed for RAM : 0.001266 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:11:02] Energy consumed for all GPUs : 0.016215 kWh. Total GPU Power : 62.73557739017216 W\n",
            "[codecarbon INFO @ 23:11:02] Energy consumed for all CPUs : 0.011336 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:11:02] 0.028817 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:11:02] 0.008130 g.CO2eq/s mean an estimation of 256.38881746603215 kg.CO2eq/year\n",
            "Epoch 13/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30], Train Loss: 0.3966, Train Accuracy: 87.02%\n",
            "Epoch [13/30], Val Loss: 0.4345, Val Accuracy: 85.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30 - Training:   8%|▊         | 49/625 [00:06<00:53, 10.70it/s][codecarbon INFO @ 23:11:17] Energy consumed for RAM : 0.001286 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:11:17] Energy consumed for all GPUs : 0.016416 kWh. Total GPU Power : 48.461112534648855 W\n",
            "[codecarbon INFO @ 23:11:17] Energy consumed for all CPUs : 0.011513 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:11:17] 0.029216 kWh of electricity used since the beginning.\n",
            "Epoch 14/30 - Training:  31%|███▏      | 196/625 [00:21<00:39, 10.79it/s][codecarbon INFO @ 23:11:32] Energy consumed for RAM : 0.001306 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:11:32] Energy consumed for all GPUs : 0.016682 kWh. Total GPU Power : 63.95263480193124 W\n",
            "[codecarbon INFO @ 23:11:32] Energy consumed for all CPUs : 0.011690 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:11:32] 0.029678 kWh of electricity used since the beginning.\n",
            "Epoch 14/30 - Training:  55%|█████▍    | 342/625 [00:36<00:26, 10.75it/s][codecarbon INFO @ 23:11:47] Energy consumed for RAM : 0.001326 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:11:47] Energy consumed for all GPUs : 0.016954 kWh. Total GPU Power : 65.05736716395768 W\n",
            "[codecarbon INFO @ 23:11:47] Energy consumed for all CPUs : 0.011867 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:11:47] 0.030147 kWh of electricity used since the beginning.\n",
            "Epoch 14/30 - Training:  78%|███████▊  | 489/625 [00:51<00:12, 10.71it/s][codecarbon INFO @ 23:12:02] Energy consumed for RAM : 0.001346 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:12:02] Energy consumed for all GPUs : 0.017223 kWh. Total GPU Power : 64.55466674848894 W\n",
            "[codecarbon INFO @ 23:12:02] Energy consumed for all CPUs : 0.012044 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:12:02] 0.030613 kWh of electricity used since the beginning.\n",
            "Epoch 14/30 - Training: 100%|██████████| 625/625 [01:05<00:00,  9.55it/s]\n",
            "Epoch 14/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 14/30 - Validation:   4%|▍         | 7/157 [00:00<00:15,  9.99it/s][codecarbon INFO @ 23:12:17] Energy consumed for RAM : 0.001365 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:12:17] Energy consumed for all GPUs : 0.017485 kWh. Total GPU Power : 62.986324574253366 W\n",
            "[codecarbon INFO @ 23:12:17] Energy consumed for all CPUs : 0.012221 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:12:17] 0.031072 kWh of electricity used since the beginning.\n",
            "Epoch 14/30 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Train Loss: 0.3784, Train Accuracy: 87.64%\n",
            "Epoch [14/30], Val Loss: 0.4387, Val Accuracy: 85.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30 - Training:   7%|▋         | 45/625 [00:05<00:52, 10.99it/s][codecarbon INFO @ 23:12:32] Energy consumed for RAM : 0.001385 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:12:32] Energy consumed for all GPUs : 0.017683 kWh. Total GPU Power : 47.48245492386893 W\n",
            "[codecarbon INFO @ 23:12:32] Energy consumed for all CPUs : 0.012399 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:12:32] 0.031467 kWh of electricity used since the beginning.\n",
            "Epoch 15/30 - Training:  31%|███       | 194/625 [00:20<00:40, 10.76it/s][codecarbon INFO @ 23:12:47] Energy consumed for RAM : 0.001405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:12:47] Energy consumed for all GPUs : 0.017950 kWh. Total GPU Power : 64.10660114638053 W\n",
            "[codecarbon INFO @ 23:12:47] Energy consumed for all CPUs : 0.012576 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:12:47] 0.031930 kWh of electricity used since the beginning.\n",
            "Epoch 15/30 - Training:  55%|█████▍    | 342/625 [00:35<00:25, 10.89it/s][codecarbon INFO @ 23:13:02] Energy consumed for RAM : 0.001425 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:13:02] Energy consumed for all GPUs : 0.018219 kWh. Total GPU Power : 64.73218397544217 W\n",
            "[codecarbon INFO @ 23:13:02] Energy consumed for all CPUs : 0.012753 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:13:02] 0.032397 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:13:02] 0.007979 g.CO2eq/s mean an estimation of 251.64094406391473 kg.CO2eq/year\n",
            "Epoch 15/30 - Training:  78%|███████▊  | 490/625 [00:50<00:12, 10.67it/s][codecarbon INFO @ 23:13:17] Energy consumed for RAM : 0.001445 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:13:17] Energy consumed for all GPUs : 0.018488 kWh. Total GPU Power : 64.54139199423588 W\n",
            "[codecarbon INFO @ 23:13:17] Energy consumed for all CPUs : 0.012930 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:13:17] 0.032862 kWh of electricity used since the beginning.\n",
            "Epoch 15/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.69it/s]\n",
            "Epoch 15/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 15/30 - Validation:   9%|▉         | 14/157 [00:01<00:10, 14.23it/s][codecarbon INFO @ 23:13:32] Energy consumed for RAM : 0.001464 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:13:32] Energy consumed for all GPUs : 0.018750 kWh. Total GPU Power : 62.87717801662858 W\n",
            "[codecarbon INFO @ 23:13:32] Energy consumed for all CPUs : 0.013107 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:13:32] 0.033321 kWh of electricity used since the beginning.\n",
            "Epoch 15/30 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Train Loss: 0.3719, Train Accuracy: 87.76%\n",
            "Epoch [15/30], Val Loss: 0.4285, Val Accuracy: 86.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30 - Training:   8%|▊         | 50/625 [00:05<00:53, 10.65it/s][codecarbon INFO @ 23:13:47] Energy consumed for RAM : 0.001484 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:13:47] Energy consumed for all GPUs : 0.018950 kWh. Total GPU Power : 47.95174375492769 W\n",
            "[codecarbon INFO @ 23:13:48] Energy consumed for all CPUs : 0.013284 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:13:48] 0.033718 kWh of electricity used since the beginning.\n",
            "Epoch 16/30 - Training:  32%|███▏      | 199/625 [00:20<00:37, 11.23it/s][codecarbon INFO @ 23:14:02] Energy consumed for RAM : 0.001504 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:14:02] Energy consumed for all GPUs : 0.019216 kWh. Total GPU Power : 64.05961730852434 W\n",
            "[codecarbon INFO @ 23:14:02] Energy consumed for all CPUs : 0.013461 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:14:02] 0.034181 kWh of electricity used since the beginning.\n",
            "Epoch 16/30 - Training:  56%|█████▌    | 349/625 [00:35<00:25, 10.98it/s][codecarbon INFO @ 23:14:17] Energy consumed for RAM : 0.001524 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:14:18] Energy consumed for all GPUs : 0.019487 kWh. Total GPU Power : 64.96448780701034 W\n",
            "[codecarbon INFO @ 23:14:18] Energy consumed for all CPUs : 0.013638 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:14:18] 0.034649 kWh of electricity used since the beginning.\n",
            "Epoch 16/30 - Training:  79%|███████▉  | 496/625 [00:50<00:12, 10.67it/s][codecarbon INFO @ 23:14:33] Energy consumed for RAM : 0.001543 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:14:33] Energy consumed for all GPUs : 0.019756 kWh. Total GPU Power : 64.57863691801907 W\n",
            "[codecarbon INFO @ 23:14:33] Energy consumed for all CPUs : 0.013816 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:14:33] 0.035115 kWh of electricity used since the beginning.\n",
            "Epoch 16/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.79it/s]\n",
            "Epoch 16/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 16/30 - Validation:  13%|█▎        | 21/157 [00:01<00:09, 14.71it/s][codecarbon INFO @ 23:14:48] Energy consumed for RAM : 0.001563 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:14:48] Energy consumed for all GPUs : 0.020017 kWh. Total GPU Power : 62.61803311259928 W\n",
            "[codecarbon INFO @ 23:14:48] Energy consumed for all CPUs : 0.013993 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:14:48] 0.035573 kWh of electricity used since the beginning.\n",
            "Epoch 16/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30], Train Loss: 0.3638, Train Accuracy: 88.15%\n",
            "Epoch [16/30], Val Loss: 0.4917, Val Accuracy: 84.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30 - Training:   8%|▊         | 50/625 [00:05<00:53, 10.68it/s][codecarbon INFO @ 23:15:03] Energy consumed for RAM : 0.001583 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:15:03] Energy consumed for all GPUs : 0.020217 kWh. Total GPU Power : 47.894281899620886 W\n",
            "[codecarbon INFO @ 23:15:03] Energy consumed for all CPUs : 0.014170 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:15:03] 0.035970 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:15:03] 0.007962 g.CO2eq/s mean an estimation of 251.08292005108262 kg.CO2eq/year\n",
            "Epoch 17/30 - Training:  31%|███▏      | 196/625 [00:20<00:39, 10.90it/s][codecarbon INFO @ 23:15:18] Energy consumed for RAM : 0.001603 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:15:18] Energy consumed for all GPUs : 0.020480 kWh. Total GPU Power : 63.29559289340435 W\n",
            "[codecarbon INFO @ 23:15:18] Energy consumed for all CPUs : 0.014347 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:15:18] 0.036430 kWh of electricity used since the beginning.\n",
            "Epoch 17/30 - Training:  55%|█████▌    | 344/625 [00:35<00:25, 10.82it/s][codecarbon INFO @ 23:15:33] Energy consumed for RAM : 0.001623 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:15:33] Energy consumed for all GPUs : 0.020751 kWh. Total GPU Power : 64.85125420579438 W\n",
            "[codecarbon INFO @ 23:15:33] Energy consumed for all CPUs : 0.014524 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:15:33] 0.036898 kWh of electricity used since the beginning.\n",
            "Epoch 17/30 - Training:  78%|███████▊  | 489/625 [00:50<00:12, 10.74it/s][codecarbon INFO @ 23:15:48] Energy consumed for RAM : 0.001642 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:15:48] Energy consumed for all GPUs : 0.021020 kWh. Total GPU Power : 64.54478189307038 W\n",
            "[codecarbon INFO @ 23:15:48] Energy consumed for all CPUs : 0.014702 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:15:48] 0.037364 kWh of electricity used since the beginning.\n",
            "Epoch 17/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.74it/s]\n",
            "Epoch 17/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 17/30 - Validation:   9%|▉         | 14/157 [00:01<00:09, 15.20it/s][codecarbon INFO @ 23:16:03] Energy consumed for RAM : 0.001662 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:16:03] Energy consumed for all GPUs : 0.021281 kWh. Total GPU Power : 62.78193928925023 W\n",
            "[codecarbon INFO @ 23:16:03] Energy consumed for all CPUs : 0.014879 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:16:03] 0.037822 kWh of electricity used since the beginning.\n",
            "Epoch 17/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30], Train Loss: 0.3585, Train Accuracy: 88.20%\n",
            "Epoch [17/30], Val Loss: 0.4371, Val Accuracy: 85.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30 - Training:   7%|▋         | 45/625 [00:04<00:52, 11.09it/s][codecarbon INFO @ 23:16:18] Energy consumed for RAM : 0.001682 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:16:18] Energy consumed for all GPUs : 0.021478 kWh. Total GPU Power : 47.34819529275269 W\n",
            "[codecarbon INFO @ 23:16:18] Energy consumed for all CPUs : 0.015056 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:16:18] 0.038216 kWh of electricity used since the beginning.\n",
            "Epoch 18/30 - Training:  31%|███       | 192/625 [00:19<00:39, 10.89it/s][codecarbon INFO @ 23:16:33] Energy consumed for RAM : 0.001702 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:16:33] Energy consumed for all GPUs : 0.021744 kWh. Total GPU Power : 63.71006010929557 W\n",
            "[codecarbon INFO @ 23:16:33] Energy consumed for all CPUs : 0.015233 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:16:33] 0.038678 kWh of electricity used since the beginning.\n",
            "Epoch 18/30 - Training:  54%|█████▍    | 336/625 [00:34<00:27, 10.61it/s][codecarbon INFO @ 23:16:48] Energy consumed for RAM : 0.001722 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:16:48] Energy consumed for all GPUs : 0.022012 kWh. Total GPU Power : 64.29076699846973 W\n",
            "[codecarbon INFO @ 23:16:48] Energy consumed for all CPUs : 0.015410 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:16:48] 0.039143 kWh of electricity used since the beginning.\n",
            "Epoch 18/30 - Training:  77%|███████▋  | 483/625 [00:49<00:12, 10.99it/s][codecarbon INFO @ 23:17:03] Energy consumed for RAM : 0.001741 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:17:03] Energy consumed for all GPUs : 0.022280 kWh. Total GPU Power : 64.48005745700699 W\n",
            "[codecarbon INFO @ 23:17:03] Energy consumed for all CPUs : 0.015587 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:17:03] 0.039609 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:17:03] 0.008109 g.CO2eq/s mean an estimation of 255.71326829973458 kg.CO2eq/year\n",
            "Epoch 18/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.73it/s]\n",
            "Epoch 18/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 18/30 - Validation:   1%|          | 1/157 [00:00<01:04,  2.41it/s][codecarbon INFO @ 23:17:18] Energy consumed for RAM : 0.001761 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:17:18] Energy consumed for all GPUs : 0.022546 kWh. Total GPU Power : 63.77967014106567 W\n",
            "[codecarbon INFO @ 23:17:18] Energy consumed for all CPUs : 0.015764 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:17:18] 0.040071 kWh of electricity used since the beginning.\n",
            "Epoch 18/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Train Loss: 0.3487, Train Accuracy: 88.44%\n",
            "Epoch [18/30], Val Loss: 0.4161, Val Accuracy: 86.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30 - Training:   6%|▌         | 38/625 [00:04<00:54, 10.68it/s][codecarbon INFO @ 23:17:33] Energy consumed for RAM : 0.001781 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:17:33] Energy consumed for all GPUs : 0.022742 kWh. Total GPU Power : 46.99836687739581 W\n",
            "[codecarbon INFO @ 23:17:33] Energy consumed for all CPUs : 0.015942 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:17:33] 0.040464 kWh of electricity used since the beginning.\n",
            "Epoch 19/30 - Training:  29%|██▉       | 184/625 [00:19<00:41, 10.72it/s][codecarbon INFO @ 23:17:48] Energy consumed for RAM : 0.001801 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:17:48] Energy consumed for all GPUs : 0.023005 kWh. Total GPU Power : 63.128082610365624 W\n",
            "[codecarbon INFO @ 23:17:48] Energy consumed for all CPUs : 0.016119 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:17:48] 0.040924 kWh of electricity used since the beginning.\n",
            "Epoch 19/30 - Training:  53%|█████▎    | 329/625 [00:34<00:27, 10.69it/s][codecarbon INFO @ 23:18:03] Energy consumed for RAM : 0.001821 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:18:03] Energy consumed for all GPUs : 0.023273 kWh. Total GPU Power : 64.51843739158343 W\n",
            "[codecarbon INFO @ 23:18:03] Energy consumed for all CPUs : 0.016296 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:18:03] 0.041390 kWh of electricity used since the beginning.\n",
            "Epoch 19/30 - Training:  76%|███████▋  | 478/625 [00:49<00:13, 10.91it/s][codecarbon INFO @ 23:18:18] Energy consumed for RAM : 0.001840 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:18:18] Energy consumed for all GPUs : 0.023543 kWh. Total GPU Power : 64.76377822203841 W\n",
            "[codecarbon INFO @ 23:18:18] Energy consumed for all CPUs : 0.016473 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:18:18] 0.041857 kWh of electricity used since the beginning.\n",
            "Epoch 19/30 - Training: 100%|██████████| 625/625 [01:04<00:00, 11.61it/s][codecarbon INFO @ 23:18:33] Energy consumed for RAM : 0.001860 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:18:33] Energy consumed for all GPUs : 0.023812 kWh. Total GPU Power : 64.60683517718982 W\n",
            "[codecarbon INFO @ 23:18:33] Energy consumed for all CPUs : 0.016650 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:18:33] 0.042323 kWh of electricity used since the beginning.\n",
            "Epoch 19/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.71it/s]\n",
            "Epoch 19/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 19/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Train Loss: 0.3376, Train Accuracy: 88.92%\n",
            "Epoch [19/30], Val Loss: 0.4810, Val Accuracy: 84.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30 - Training:   5%|▍         | 31/625 [00:03<00:55, 10.71it/s][codecarbon INFO @ 23:18:48] Energy consumed for RAM : 0.001880 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:18:48] Energy consumed for all GPUs : 0.024003 kWh. Total GPU Power : 45.87027691637213 W\n",
            "[codecarbon INFO @ 23:18:48] Energy consumed for all CPUs : 0.016827 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:18:48] 0.042710 kWh of electricity used since the beginning.\n",
            "Epoch 20/30 - Training:  29%|██▊       | 179/625 [00:18<00:43, 10.23it/s][codecarbon INFO @ 23:19:03] Energy consumed for RAM : 0.001900 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:19:03] Energy consumed for all GPUs : 0.024268 kWh. Total GPU Power : 63.53017583691681 W\n",
            "[codecarbon INFO @ 23:19:03] Energy consumed for all CPUs : 0.017004 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:19:03] 0.043172 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:19:03] 0.007940 g.CO2eq/s mean an estimation of 250.38571841675974 kg.CO2eq/year\n",
            "Epoch 20/30 - Training:  52%|█████▏    | 327/625 [00:33<00:26, 11.10it/s][codecarbon INFO @ 23:19:18] Energy consumed for RAM : 0.001920 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:19:18] Energy consumed for all GPUs : 0.024536 kWh. Total GPU Power : 64.49528315213708 W\n",
            "[codecarbon INFO @ 23:19:18] Energy consumed for all CPUs : 0.017181 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 20/30 - Training:  53%|█████▎    | 329/625 [00:33<00:26, 10.99it/s][codecarbon INFO @ 23:19:18] 0.043637 kWh of electricity used since the beginning.\n",
            "Epoch 20/30 - Training:  76%|███████▋  | 477/625 [00:48<00:13, 10.70it/s][codecarbon INFO @ 23:19:33] Energy consumed for RAM : 0.001939 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:19:33] Energy consumed for all GPUs : 0.024806 kWh. Total GPU Power : 64.87049459191196 W\n",
            "[codecarbon INFO @ 23:19:33] Energy consumed for all CPUs : 0.017358 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:19:33] 0.044103 kWh of electricity used since the beginning.\n",
            "Epoch 20/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.85it/s]\n",
            "Epoch 20/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 23:19:48] Energy consumed for RAM : 0.001959 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:19:48] Energy consumed for all GPUs : 0.025076 kWh. Total GPU Power : 64.73263303219457 W\n",
            "[codecarbon INFO @ 23:19:48] Energy consumed for all CPUs : 0.017535 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:19:48] 0.044570 kWh of electricity used since the beginning.\n",
            "Epoch 20/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 13.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30], Train Loss: 0.3422, Train Accuracy: 88.78%\n",
            "Epoch [20/30], Val Loss: 0.4704, Val Accuracy: 84.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30 - Training:   5%|▌         | 33/625 [00:03<00:56, 10.43it/s][codecarbon INFO @ 23:20:03] Energy consumed for RAM : 0.001979 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:20:03] Energy consumed for all GPUs : 0.025269 kWh. Total GPU Power : 46.49750747464096 W\n",
            "[codecarbon INFO @ 23:20:03] Energy consumed for all CPUs : 0.017713 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:20:03] 0.044961 kWh of electricity used since the beginning.\n",
            "Epoch 21/30 - Training:  29%|██▉       | 181/625 [00:18<00:40, 11.00it/s][codecarbon INFO @ 23:20:18] Energy consumed for RAM : 0.001999 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:20:18] Energy consumed for all GPUs : 0.025534 kWh. Total GPU Power : 63.64750500816824 W\n",
            "[codecarbon INFO @ 23:20:18] Energy consumed for all CPUs : 0.017890 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:20:18] 0.045422 kWh of electricity used since the beginning.\n",
            "Epoch 21/30 - Training:  52%|█████▏    | 328/625 [00:33<00:27, 10.88it/s][codecarbon INFO @ 23:20:33] Energy consumed for RAM : 0.002018 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:20:33] Energy consumed for all GPUs : 0.025803 kWh. Total GPU Power : 64.54022977747206 W\n",
            "[codecarbon INFO @ 23:20:33] Energy consumed for all CPUs : 0.018067 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:20:33] 0.045888 kWh of electricity used since the beginning.\n",
            "Epoch 21/30 - Training:  76%|███████▌  | 475/625 [00:48<00:14, 10.67it/s][codecarbon INFO @ 23:20:48] Energy consumed for RAM : 0.002038 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:20:48] Energy consumed for all GPUs : 0.026072 kWh. Total GPU Power : 64.74743509645322 W\n",
            "[codecarbon INFO @ 23:20:48] Energy consumed for all CPUs : 0.018244 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:20:48] 0.046354 kWh of electricity used since the beginning.\n",
            "Epoch 21/30 - Training: 100%|█████████▉| 622/625 [01:03<00:00, 11.19it/s][codecarbon INFO @ 23:21:03] Energy consumed for RAM : 0.002058 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:21:03] Energy consumed for all GPUs : 0.026342 kWh. Total GPU Power : 64.71801751457639 W\n",
            "[codecarbon INFO @ 23:21:03] Energy consumed for all CPUs : 0.018421 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:21:03] 0.046821 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:21:03] 0.008133 g.CO2eq/s mean an estimation of 256.48833593337497 kg.CO2eq/year\n",
            "Epoch 21/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.77it/s]\n",
            "Epoch 21/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 21/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30], Train Loss: 0.3244, Train Accuracy: 89.45%\n",
            "Epoch [21/30], Val Loss: 0.3920, Val Accuracy: 87.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30 - Training:   5%|▌         | 32/625 [00:03<01:12,  8.24it/s][codecarbon INFO @ 23:21:18] Energy consumed for RAM : 0.002078 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:21:18] Energy consumed for all GPUs : 0.026535 kWh. Total GPU Power : 46.400962950218755 W\n",
            "[codecarbon INFO @ 23:21:18] Energy consumed for all CPUs : 0.018598 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:21:18] 0.047211 kWh of electricity used since the beginning.\n",
            "Epoch 22/30 - Training:  29%|██▊       | 179/625 [00:18<00:59,  7.51it/s][codecarbon INFO @ 23:21:33] Energy consumed for RAM : 0.002098 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:21:33] Energy consumed for all GPUs : 0.026799 kWh. Total GPU Power : 63.57942611408264 W\n",
            "[codecarbon INFO @ 23:21:33] Energy consumed for all CPUs : 0.018775 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:21:33] 0.047672 kWh of electricity used since the beginning.\n",
            "Epoch 22/30 - Training:  52%|█████▏    | 326/625 [00:33<00:36,  8.16it/s][codecarbon INFO @ 23:21:48] Energy consumed for RAM : 0.002117 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:21:48] Energy consumed for all GPUs : 0.027068 kWh. Total GPU Power : 64.57495778090843 W\n",
            "[codecarbon INFO @ 23:21:48] Energy consumed for all CPUs : 0.018952 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:21:48] 0.048137 kWh of electricity used since the beginning.\n",
            "Epoch 22/30 - Training:  76%|███████▌  | 474/625 [00:48<00:18,  8.37it/s][codecarbon INFO @ 23:22:03] Energy consumed for RAM : 0.002137 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:22:03] Energy consumed for all GPUs : 0.027336 kWh. Total GPU Power : 64.23735299657012 W\n",
            "[codecarbon INFO @ 23:22:03] Energy consumed for all CPUs : 0.019129 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:22:03] 0.048602 kWh of electricity used since the beginning.\n",
            "Epoch 22/30 - Training: 100%|█████████▉| 624/625 [01:03<00:00, 10.55it/s][codecarbon INFO @ 23:22:18] Energy consumed for RAM : 0.002157 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:22:18] Energy consumed for all GPUs : 0.027604 kWh. Total GPU Power : 64.30085927485861 W\n",
            "[codecarbon INFO @ 23:22:18] Energy consumed for all CPUs : 0.019307 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:22:18] 0.049068 kWh of electricity used since the beginning.\n",
            "Epoch 22/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.79it/s]\n",
            "Epoch 22/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 22/30 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Train Loss: 0.3237, Train Accuracy: 89.45%\n",
            "Epoch [22/30], Val Loss: 0.4327, Val Accuracy: 85.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30 - Training:   6%|▌         | 35/625 [00:04<01:11,  8.27it/s][codecarbon INFO @ 23:22:33] Energy consumed for RAM : 0.002177 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:22:33] Energy consumed for all GPUs : 0.027800 kWh. Total GPU Power : 46.99673271454701 W\n",
            "[codecarbon INFO @ 23:22:33] Energy consumed for all CPUs : 0.019484 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:22:33] 0.049461 kWh of electricity used since the beginning.\n",
            "Epoch 23/30 - Training:  29%|██▉       | 182/625 [00:19<00:52,  8.38it/s][codecarbon INFO @ 23:22:48] Energy consumed for RAM : 0.002197 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:22:48] Energy consumed for all GPUs : 0.028067 kWh. Total GPU Power : 63.90837408438266 W\n",
            "[codecarbon INFO @ 23:22:48] Energy consumed for all CPUs : 0.019661 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:22:48] 0.049924 kWh of electricity used since the beginning.\n",
            "Epoch 23/30 - Training:  53%|█████▎    | 330/625 [00:34<00:38,  7.70it/s][codecarbon INFO @ 23:23:03] Energy consumed for RAM : 0.002216 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:23:03] Energy consumed for all GPUs : 0.028335 kWh. Total GPU Power : 64.4345326794456 W\n",
            "[codecarbon INFO @ 23:23:03] Energy consumed for all CPUs : 0.019838 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:23:03] 0.050389 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:23:03] 0.007953 g.CO2eq/s mean an estimation of 250.8130870639978 kg.CO2eq/year\n",
            "Epoch 23/30 - Training:  76%|███████▋  | 477/625 [00:49<00:19,  7.68it/s][codecarbon INFO @ 23:23:18] Energy consumed for RAM : 0.002236 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:23:18] Energy consumed for all GPUs : 0.028605 kWh. Total GPU Power : 64.87780621787212 W\n",
            "[codecarbon INFO @ 23:23:18] Energy consumed for all CPUs : 0.020015 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:23:18] 0.050857 kWh of electricity used since the beginning.\n",
            "Epoch 23/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.75it/s]\n",
            "Epoch 23/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 23/30 - Validation:   1%|          | 1/157 [00:00<01:08,  2.27it/s][codecarbon INFO @ 23:23:33] Energy consumed for RAM : 0.002256 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:23:33] Energy consumed for all GPUs : 0.028873 kWh. Total GPU Power : 64.2972071123454 W\n",
            "[codecarbon INFO @ 23:23:33] Energy consumed for all CPUs : 0.020193 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:23:33] 0.051322 kWh of electricity used since the beginning.\n",
            "Epoch 23/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Train Loss: 0.3167, Train Accuracy: 89.62%\n",
            "Epoch [23/30], Val Loss: 0.4086, Val Accuracy: 86.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30 - Training:   7%|▋         | 41/625 [00:05<01:02,  9.29it/s][codecarbon INFO @ 23:23:48] Energy consumed for RAM : 0.002276 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:23:48] Energy consumed for all GPUs : 0.029078 kWh. Total GPU Power : 49.14606775915539 W\n",
            "[codecarbon INFO @ 23:23:48] Energy consumed for all CPUs : 0.020370 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:23:48] 0.051723 kWh of electricity used since the beginning.\n",
            "Epoch 24/30 - Training:  31%|███       | 191/625 [00:20<00:40, 10.64it/s][codecarbon INFO @ 23:24:03] Energy consumed for RAM : 0.002296 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:24:03] Energy consumed for all GPUs : 0.029344 kWh. Total GPU Power : 63.844144167415294 W\n",
            "[codecarbon INFO @ 23:24:03] Energy consumed for all CPUs : 0.020547 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:24:03] 0.052186 kWh of electricity used since the beginning.\n",
            "Epoch 24/30 - Training:  54%|█████▍    | 339/625 [00:35<00:27, 10.52it/s][codecarbon INFO @ 23:24:18] Energy consumed for RAM : 0.002315 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:24:18] Energy consumed for all GPUs : 0.029614 kWh. Total GPU Power : 64.77676278033348 W\n",
            "[codecarbon INFO @ 23:24:18] Energy consumed for all CPUs : 0.020724 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:24:18] 0.052653 kWh of electricity used since the beginning.\n",
            "Epoch 24/30 - Training:  78%|███████▊  | 487/625 [00:50<00:12, 10.66it/s][codecarbon INFO @ 23:24:33] Energy consumed for RAM : 0.002335 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:24:33] Energy consumed for all GPUs : 0.029884 kWh. Total GPU Power : 64.77340744136235 W\n",
            "[codecarbon INFO @ 23:24:33] Energy consumed for all CPUs : 0.020901 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:24:33] 0.053120 kWh of electricity used since the beginning.\n",
            "Epoch 24/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.65it/s]\n",
            "Epoch 24/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 24/30 - Validation:   8%|▊         | 13/157 [00:00<00:08, 16.81it/s][codecarbon INFO @ 23:24:48] Energy consumed for RAM : 0.002355 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:24:48] Energy consumed for all GPUs : 0.030147 kWh. Total GPU Power : 63.09966802526548 W\n",
            "[codecarbon INFO @ 23:24:48] Energy consumed for all CPUs : 0.021078 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:24:48] 0.053580 kWh of electricity used since the beginning.\n",
            "Epoch 24/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Train Loss: 0.3022, Train Accuracy: 90.06%\n",
            "Epoch [24/30], Val Loss: 0.4173, Val Accuracy: 86.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30 - Training:   8%|▊         | 50/625 [00:06<00:52, 10.96it/s][codecarbon INFO @ 23:25:03] Energy consumed for RAM : 0.002375 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:25:03] Energy consumed for all GPUs : 0.030351 kWh. Total GPU Power : 48.85142605547006 W\n",
            "[codecarbon INFO @ 23:25:03] Energy consumed for all CPUs : 0.021256 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:25:03] 0.053981 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:25:03] 0.008002 g.CO2eq/s mean an estimation of 252.3427831076413 kg.CO2eq/year\n",
            "Epoch 25/30 - Training:  32%|███▏      | 200/625 [00:21<00:38, 11.15it/s][codecarbon INFO @ 23:25:18] Energy consumed for RAM : 0.002395 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:25:18] Energy consumed for all GPUs : 0.030617 kWh. Total GPU Power : 64.02802637903761 W\n",
            "[codecarbon INFO @ 23:25:18] Energy consumed for all CPUs : 0.021433 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:25:18] 0.054445 kWh of electricity used since the beginning.\n",
            "Epoch 25/30 - Training:  56%|█████▌    | 348/625 [00:36<00:25, 10.73it/s][codecarbon INFO @ 23:25:33] Energy consumed for RAM : 0.002414 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:25:33] Energy consumed for all GPUs : 0.030887 kWh. Total GPU Power : 64.83497482411842 W\n",
            "[codecarbon INFO @ 23:25:33] Energy consumed for all CPUs : 0.021610 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:25:33] 0.054912 kWh of electricity used since the beginning.\n",
            "Epoch 25/30 - Training:  79%|███████▉  | 496/625 [00:51<00:11, 10.96it/s][codecarbon INFO @ 23:25:48] Energy consumed for RAM : 0.002434 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:25:48] Energy consumed for all GPUs : 0.031157 kWh. Total GPU Power : 64.64058241827462 W\n",
            "[codecarbon INFO @ 23:25:48] Energy consumed for all CPUs : 0.021787 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:25:48] 0.055378 kWh of electricity used since the beginning.\n",
            "Epoch 25/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.63it/s]\n",
            "Epoch 25/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 25/30 - Validation:  16%|█▌        | 25/157 [00:01<00:08, 16.45it/s][codecarbon INFO @ 23:26:03] Energy consumed for RAM : 0.002454 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:26:03] Energy consumed for all GPUs : 0.031418 kWh. Total GPU Power : 62.743113563367665 W\n",
            "[codecarbon INFO @ 23:26:03] Energy consumed for all CPUs : 0.021964 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:26:03] 0.055836 kWh of electricity used since the beginning.\n",
            "Epoch 25/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Train Loss: 0.3031, Train Accuracy: 90.08%\n",
            "Epoch [25/30], Val Loss: 0.4857, Val Accuracy: 84.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30 - Training:   9%|▉         | 58/625 [00:07<00:52, 10.82it/s][codecarbon INFO @ 23:26:18] Energy consumed for RAM : 0.002474 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:26:18] Energy consumed for all GPUs : 0.031624 kWh. Total GPU Power : 49.42794969515908 W\n",
            "[codecarbon INFO @ 23:26:18] Energy consumed for all CPUs : 0.022141 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:26:18] 0.056239 kWh of electricity used since the beginning.\n",
            "Epoch 26/30 - Training:  33%|███▎      | 207/625 [00:22<00:39, 10.51it/s][codecarbon INFO @ 23:26:33] Energy consumed for RAM : 0.002494 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:26:33] Energy consumed for all GPUs : 0.031891 kWh. Total GPU Power : 64.11777274939021 W\n",
            "[codecarbon INFO @ 23:26:33] Energy consumed for all CPUs : 0.022319 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:26:33] 0.056704 kWh of electricity used since the beginning.\n",
            "Epoch 26/30 - Training:  56%|█████▋    | 353/625 [00:37<00:24, 11.03it/s][codecarbon INFO @ 23:26:48] Energy consumed for RAM : 0.002513 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:26:48] Energy consumed for all GPUs : 0.032161 kWh. Total GPU Power : 64.75585293796986 W\n",
            "[codecarbon INFO @ 23:26:48] Energy consumed for all CPUs : 0.022496 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:26:48] 0.057170 kWh of electricity used since the beginning.\n",
            "Epoch 26/30 - Training:  80%|████████  | 501/625 [00:52<00:11, 11.10it/s][codecarbon INFO @ 23:27:03] Energy consumed for RAM : 0.002533 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:27:03] Energy consumed for all GPUs : 0.032429 kWh. Total GPU Power : 64.52104650813318 W\n",
            "[codecarbon INFO @ 23:27:03] Energy consumed for all CPUs : 0.022673 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:27:03] 0.057635 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:27:03] 0.008142 g.CO2eq/s mean an estimation of 256.7566680176931 kg.CO2eq/year\n",
            "Epoch 26/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.63it/s]\n",
            "Epoch 26/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 26/30 - Validation:  22%|██▏       | 34/157 [00:02<00:08, 14.94it/s][codecarbon INFO @ 23:27:18] Energy consumed for RAM : 0.002553 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:27:18] Energy consumed for all GPUs : 0.032686 kWh. Total GPU Power : 61.729270042775156 W\n",
            "[codecarbon INFO @ 23:27:18] Energy consumed for all CPUs : 0.022850 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:27:18] 0.058089 kWh of electricity used since the beginning.\n",
            "Epoch 26/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Train Loss: 0.1755, Train Accuracy: 94.13%\n",
            "Epoch [26/30], Val Loss: 0.3191, Val Accuracy: 90.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30 - Training:  10%|█         | 64/625 [00:07<00:51, 10.97it/s][codecarbon INFO @ 23:27:33] Energy consumed for RAM : 0.002573 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:27:33] Energy consumed for all GPUs : 0.032895 kWh. Total GPU Power : 50.0418815471268 W\n",
            "[codecarbon INFO @ 23:27:33] Energy consumed for all CPUs : 0.023027 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:27:33] 0.058494 kWh of electricity used since the beginning.\n",
            "Epoch 27/30 - Training:  34%|███▍      | 212/625 [00:22<00:38, 10.83it/s][codecarbon INFO @ 23:27:48] Energy consumed for RAM : 0.002592 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:27:48] Energy consumed for all GPUs : 0.033163 kWh. Total GPU Power : 64.33746899332237 W\n",
            "[codecarbon INFO @ 23:27:48] Energy consumed for all CPUs : 0.023204 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:27:48] 0.058959 kWh of electricity used since the beginning.\n",
            "Epoch 27/30 - Training:  58%|█████▊    | 360/625 [00:37<00:24, 10.99it/s][codecarbon INFO @ 23:28:03] Energy consumed for RAM : 0.002612 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:28:03] Energy consumed for all GPUs : 0.033431 kWh. Total GPU Power : 64.64511283625282 W\n",
            "[codecarbon INFO @ 23:28:03] Energy consumed for all CPUs : 0.023381 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:28:03] 0.059424 kWh of electricity used since the beginning.\n",
            "Epoch 27/30 - Training:  81%|████████▏ | 509/625 [00:52<00:10, 10.91it/s][codecarbon INFO @ 23:28:18] Energy consumed for RAM : 0.002632 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:28:18] Energy consumed for all GPUs : 0.033701 kWh. Total GPU Power : 64.66868024462612 W\n",
            "[codecarbon INFO @ 23:28:18] Energy consumed for all CPUs : 0.023558 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:28:18] 0.059891 kWh of electricity used since the beginning.\n",
            "Epoch 27/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.67it/s]\n",
            "Epoch 27/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 27/30 - Validation:  29%|██▊       | 45/157 [00:03<00:07, 14.21it/s][codecarbon INFO @ 23:28:33] Energy consumed for RAM : 0.002652 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:28:33] Energy consumed for all GPUs : 0.033953 kWh. Total GPU Power : 60.564437795360504 W\n",
            "[codecarbon INFO @ 23:28:33] Energy consumed for all CPUs : 0.023735 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:28:33] 0.060340 kWh of electricity used since the beginning.\n",
            "Epoch 27/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 15.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Train Loss: 0.1423, Train Accuracy: 95.22%\n",
            "Epoch [27/30], Val Loss: 0.3147, Val Accuracy: 90.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30 - Training:  11%|█         | 69/625 [00:08<00:50, 10.95it/s][codecarbon INFO @ 23:28:48] Energy consumed for RAM : 0.002672 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:28:48] Energy consumed for all GPUs : 0.034166 kWh. Total GPU Power : 51.05318093513729 W\n",
            "[codecarbon INFO @ 23:28:48] Energy consumed for all CPUs : 0.023912 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:28:48] 0.060750 kWh of electricity used since the beginning.\n",
            "Epoch 28/30 - Training:  35%|███▍      | 217/625 [00:22<00:38, 10.67it/s][codecarbon INFO @ 23:29:03] Energy consumed for RAM : 0.002691 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:29:03] Energy consumed for all GPUs : 0.034433 kWh. Total GPU Power : 64.07151086727002 W\n",
            "[codecarbon INFO @ 23:29:03] Energy consumed for all CPUs : 0.024089 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 28/30 - Training:  35%|███▌      | 219/625 [00:23<00:37, 10.95it/s][codecarbon INFO @ 23:29:03] 0.061213 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:29:03] 0.007972 g.CO2eq/s mean an estimation of 251.41086019625183 kg.CO2eq/year\n",
            "Epoch 28/30 - Training:  59%|█████▊    | 367/625 [00:38<00:23, 10.82it/s][codecarbon INFO @ 23:29:18] Energy consumed for RAM : 0.002711 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:29:18] Energy consumed for all GPUs : 0.034702 kWh. Total GPU Power : 64.75750578416815 W\n",
            "[codecarbon INFO @ 23:29:18] Energy consumed for all CPUs : 0.024266 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:29:18] 0.061679 kWh of electricity used since the beginning.\n",
            "Epoch 28/30 - Training:  83%|████████▎ | 516/625 [00:53<00:09, 11.02it/s][codecarbon INFO @ 23:29:33] Energy consumed for RAM : 0.002731 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:29:33] Energy consumed for all GPUs : 0.034971 kWh. Total GPU Power : 64.70839062497684 W\n",
            "[codecarbon INFO @ 23:29:33] Energy consumed for all CPUs : 0.024443 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:29:33] 0.062145 kWh of electricity used since the beginning.\n",
            "Epoch 28/30 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.69it/s]\n",
            "Epoch 28/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 28/30 - Validation:  35%|███▌      | 55/157 [00:03<00:06, 15.27it/s][codecarbon INFO @ 23:29:48] Energy consumed for RAM : 0.002751 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:29:48] Energy consumed for all GPUs : 0.035219 kWh. Total GPU Power : 59.63673167018126 W\n",
            "[codecarbon INFO @ 23:29:48] Energy consumed for all CPUs : 0.024620 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:29:48] 0.062590 kWh of electricity used since the beginning.\n",
            "Epoch 28/30 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30], Train Loss: 0.1335, Train Accuracy: 95.53%\n",
            "Epoch [28/30], Val Loss: 0.3214, Val Accuracy: 90.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30 - Training:  13%|█▎        | 80/625 [00:09<00:49, 11.00it/s][codecarbon INFO @ 23:30:03] Energy consumed for RAM : 0.002770 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:30:03] Energy consumed for all GPUs : 0.035437 kWh. Total GPU Power : 52.32882678942569 W\n",
            "[codecarbon INFO @ 23:30:03] Energy consumed for all CPUs : 0.024797 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:30:03] 0.063005 kWh of electricity used since the beginning.\n",
            "Epoch 29/30 - Training:  36%|███▌      | 224/625 [00:24<00:37, 10.72it/s][codecarbon INFO @ 23:30:18] Energy consumed for RAM : 0.002790 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:30:18] Energy consumed for all GPUs : 0.035704 kWh. Total GPU Power : 64.04243157473682 W\n",
            "[codecarbon INFO @ 23:30:18] Energy consumed for all CPUs : 0.024975 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:30:18] 0.063469 kWh of electricity used since the beginning.\n",
            "Epoch 29/30 - Training:  59%|█████▊    | 366/625 [00:39<00:24, 10.55it/s][codecarbon INFO @ 23:30:33] Energy consumed for RAM : 0.002810 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:30:33] Energy consumed for all GPUs : 0.035972 kWh. Total GPU Power : 64.19098911169807 W\n",
            "[codecarbon INFO @ 23:30:33] Energy consumed for all CPUs : 0.025152 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:30:33] 0.063934 kWh of electricity used since the beginning.\n",
            "Epoch 29/30 - Training:  82%|████████▏ | 511/625 [00:54<00:10, 10.59it/s][codecarbon INFO @ 23:30:48] Energy consumed for RAM : 0.002830 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:30:48] Energy consumed for all GPUs : 0.036240 kWh. Total GPU Power : 64.2729485779854 W\n",
            "[codecarbon INFO @ 23:30:48] Energy consumed for all CPUs : 0.025329 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:30:48] 0.064398 kWh of electricity used since the beginning.\n",
            "Epoch 29/30 - Training: 100%|██████████| 625/625 [01:06<00:00,  9.42it/s]\n",
            "Epoch 29/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 29/30 - Validation:  26%|██▌       | 41/157 [00:02<00:07, 16.14it/s][codecarbon INFO @ 23:31:03] Energy consumed for RAM : 0.002850 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:31:03] Energy consumed for all GPUs : 0.036493 kWh. Total GPU Power : 60.78010565067934 W\n",
            "[codecarbon INFO @ 23:31:03] Energy consumed for all CPUs : 0.025506 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:31:03] 0.064849 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:31:03] 0.008101 g.CO2eq/s mean an estimation of 255.46005483566495 kg.CO2eq/year\n",
            "Epoch 29/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30], Train Loss: 0.1223, Train Accuracy: 95.89%\n",
            "Epoch [29/30], Val Loss: 0.3175, Val Accuracy: 90.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30 - Training:  10%|█         | 65/625 [00:06<00:52, 10.73it/s][codecarbon INFO @ 23:31:18] Energy consumed for RAM : 0.002869 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:31:18] Energy consumed for all GPUs : 0.036700 kWh. Total GPU Power : 49.75979124072567 W\n",
            "[codecarbon INFO @ 23:31:18] Energy consumed for all CPUs : 0.025683 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:31:18] 0.065252 kWh of electricity used since the beginning.\n",
            "Epoch 30/30 - Training:  34%|███▍      | 215/625 [00:21<00:37, 11.07it/s][codecarbon INFO @ 23:31:33] Energy consumed for RAM : 0.002889 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:31:33] Energy consumed for all GPUs : 0.036967 kWh. Total GPU Power : 64.11063216360256 W\n",
            "[codecarbon INFO @ 23:31:33] Energy consumed for all CPUs : 0.025860 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:31:33] 0.065716 kWh of electricity used since the beginning.\n",
            "Epoch 30/30 - Training:  58%|█████▊    | 361/625 [00:36<00:25, 10.55it/s][codecarbon INFO @ 23:31:48] Energy consumed for RAM : 0.002909 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:31:48] Energy consumed for all GPUs : 0.037235 kWh. Total GPU Power : 64.4733838234232 W\n",
            "[codecarbon INFO @ 23:31:48] Energy consumed for all CPUs : 0.026037 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:31:48] 0.066181 kWh of electricity used since the beginning.\n",
            "Epoch 30/30 - Training:  81%|████████▏ | 508/625 [00:51<00:10, 10.95it/s][codecarbon INFO @ 23:32:03] Energy consumed for RAM : 0.002929 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:32:03] Energy consumed for all GPUs : 0.037506 kWh. Total GPU Power : 65.00628358924044 W\n",
            "[codecarbon INFO @ 23:32:03] Energy consumed for all CPUs : 0.026214 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:32:03] 0.066649 kWh of electricity used since the beginning.\n",
            "Epoch 30/30 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.78it/s]\n",
            "Epoch 30/30 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 30/30 - Validation:  26%|██▌       | 41/157 [00:02<00:07, 16.45it/s][codecarbon INFO @ 23:32:18] Energy consumed for RAM : 0.002949 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:32:18] Energy consumed for all GPUs : 0.037759 kWh. Total GPU Power : 60.83312139945084 W\n",
            "[codecarbon INFO @ 23:32:18] Energy consumed for all CPUs : 0.026391 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:32:18] 0.067099 kWh of electricity used since the beginning.\n",
            "Epoch 30/30 - Validation: 100%|██████████| 157/157 [00:11<00:00, 14.03it/s]\n",
            "[codecarbon INFO @ 23:32:27] Energy consumed for RAM : 0.002959 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:32:27] Energy consumed for all GPUs : 0.037850 kWh. Total GPU Power : 39.80240640621899 W\n",
            "[codecarbon INFO @ 23:32:27] Energy consumed for all CPUs : 0.026488 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:32:27] 0.067297 kWh of electricity used since the beginning.\n",
            "/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Train Loss: 0.1172, Train Accuracy: 96.07%\n",
            "Epoch [30/30], Val Loss: 0.3108, Val Accuracy: 90.54%\n",
            "\n",
            "--- Model Analysis ---\n",
            "Parameter Count: 134301514\n",
            "Model Size: 512.33 MB\n",
            "FLOPs: 0.43 GFLOPs\n",
            "Training Time: 2244.93 seconds\n",
            "Average Inference Time: 0.003616 seconds\n",
            "\n",
            "--- Energy and Emissions Report ---\n",
            "CO2 Emissions (CodeCarbon): 0.018010 kg\n",
            "Average GPU Power Consumption: 63.72 W\n",
            "Total GPU Energy Consumption: 39.733080 kWh\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import codecarbon\n",
        "from thop import profile\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the transforms for the dataset with additional augmentations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split the full training dataset into training and validation sets (80% train, 20% validation)\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Print dataset sizes for debugging\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "# Load the pretrained VGG16 model and modify the classifier\n",
        "model = models.vgg16(pretrained=True)\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),  # Increased dropout for regularization\n",
        "    nn.Linear(input_lastLayer, 10)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer with learning rate scheduler\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Tracking metrics for analysis\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Use CodeCarbon for system-wide energy consumption tracking\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Function to get GPU power consumption using nvidia-smi\n",
        "def get_gpu_power():\n",
        "    try:\n",
        "        # Query power.draw from nvidia-smi\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,nounits,noheader'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        power_draws = result.stdout.strip().split('\\n')\n",
        "        power_draws = [float(p) for p in power_draws]\n",
        "        avg_power_draw = sum(power_draws) / len(power_draws)  # Average power across GPUs\n",
        "        return avg_power_draw  # in watts\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting GPU power: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Tracking GPU-specific energy consumption\n",
        "gpu_power_readings = []\n",
        "\n",
        "# Training loop with GPU power monitoring\n",
        "epochs = 30\n",
        "start_training_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Record GPU power usage\n",
        "        gpu_power_readings.append(get_gpu_power())\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Adjust learning rate based on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "training_time = time.time() - start_training_time\n",
        "\n",
        "# Stop the CodeCarbon tracker after training\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Calculate average GPU power and total GPU energy consumption (in kWh)\n",
        "avg_gpu_power = sum(gpu_power_readings) / len(gpu_power_readings) if gpu_power_readings else 0.0\n",
        "energy_consumption_gpu = (avg_gpu_power * training_time) / 3600  # Convert to kWh\n",
        "\n",
        "# Save the model after training\n",
        "model_path = \"trained_vgg16_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Model analysis metrics\n",
        "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "torch.save(model.state_dict(), \"temp_model.pth\")\n",
        "model_size = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)  # Convert bytes to MB\n",
        "os.remove(\"temp_model.pth\")\n",
        "\n",
        "# FLOPs calculation\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "flops, _ = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "\n",
        "# Inference time calculation\n",
        "model.eval()\n",
        "inference_start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        _ = model(dummy_input)\n",
        "inference_time = (time.time() - inference_start) / 100\n",
        "\n",
        "# Final energy and emissions report\n",
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size:.2f} MB\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Average Inference Time: {inference_time:.6f} seconds\")\n",
        "\n",
        "print(\"\\n--- Energy and Emissions Report ---\")\n",
        "print(f\"CO2 Emissions (CodeCarbon): {emissions:.6f} kg\")\n",
        "print(f\"Average GPU Power Consumption: {avg_gpu_power:.2f} W\")\n",
        "print(f\"Total GPU Energy Consumption: {energy_consumption_gpu:.6f} kWh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EbFQrp5sXlSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5a91fdc-0659-42ac-c2d1-4472c402dab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-4-a051c6344052>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\"))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 66.2MB/s]\n",
            "[codecarbon INFO @ 23:32:43] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 23:32:43] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 23:32:43] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 23:32:43] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 23:32:43] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 23:32:44] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 23:32:44] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 23:32:44] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 23:32:44]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 23:32:44]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 23:32:44]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 23:32:44]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 23:32:44]   CPU count: 2\n",
            "[codecarbon INFO @ 23:32:44]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 23:32:44]   GPU count: 1\n",
            "[codecarbon INFO @ 23:32:44]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 23:32:44] Saving emissions data to file /content/emissions.csv\n",
            "Epoch 1/20 - Training:   0%|          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training:  20%|██        | 125/625 [00:14<00:39, 12.69it/s][codecarbon INFO @ 23:32:59] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:32:59] Energy consumed for all GPUs : 0.000164 kWh. Total GPU Power : 39.34418112448839 W\n",
            "[codecarbon INFO @ 23:32:59] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:32:59] 0.000362 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training:  47%|████▋     | 294/625 [00:29<00:25, 13.18it/s][codecarbon INFO @ 23:33:14] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:33:14] Energy consumed for all GPUs : 0.000351 kWh. Total GPU Power : 44.84451697112497 W\n",
            "[codecarbon INFO @ 23:33:14] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:33:14] 0.000746 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training:  75%|███████▍  | 467/625 [00:44<00:11, 13.35it/s][codecarbon INFO @ 23:33:29] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:33:29] Energy consumed for all GPUs : 0.000544 kWh. Total GPU Power : 46.17577107084282 W\n",
            "[codecarbon INFO @ 23:33:29] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:33:29] 0.001135 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training: 100%|██████████| 625/625 [00:58<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 3.6655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 - Training:   1%|          | 7/625 [00:01<01:10,  8.72it/s][codecarbon INFO @ 23:33:44] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:33:44] Energy consumed for all GPUs : 0.000735 kWh. Total GPU Power : 45.79081895677121 W\n",
            "[codecarbon INFO @ 23:33:44] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:33:44] 0.001523 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  28%|██▊       | 178/625 [00:16<00:31, 14.20it/s][codecarbon INFO @ 23:33:59] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:33:59] Energy consumed for all GPUs : 0.000929 kWh. Total GPU Power : 46.781314114762765 W\n",
            "[codecarbon INFO @ 23:33:59] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:33:59] 0.001915 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  56%|█████▌    | 348/625 [00:31<00:21, 12.84it/s][codecarbon INFO @ 23:34:14] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:34:14] Energy consumed for all GPUs : 0.001119 kWh. Total GPU Power : 45.54928359586331 W\n",
            "[codecarbon INFO @ 23:34:14] Energy consumed for all CPUs : 0.001064 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:34:14] 0.002302 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  81%|████████▏ | 508/625 [00:46<00:10, 11.65it/s][codecarbon INFO @ 23:34:29] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:34:29] Energy consumed for all GPUs : 0.001303 kWh. Total GPU Power : 44.092817194177364 W\n",
            "[codecarbon INFO @ 23:34:29] Energy consumed for all CPUs : 0.001241 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:34:29] 0.002682 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 2.7542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 - Training:   8%|▊         | 52/625 [00:04<00:41, 13.75it/s][codecarbon INFO @ 23:34:44] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:34:44] Energy consumed for all GPUs : 0.001497 kWh. Total GPU Power : 46.679250439923656 W\n",
            "[codecarbon INFO @ 23:34:44] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:34:44] 0.003073 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:34:44] 0.006846 g.CO2eq/s mean an estimation of 215.88099412538736 kg.CO2eq/year\n",
            "Epoch 3/20 - Training:  35%|███▌      | 220/625 [00:19<00:31, 12.84it/s][codecarbon INFO @ 23:34:59] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:34:59] Energy consumed for all GPUs : 0.001690 kWh. Total GPU Power : 46.361128364877345 W\n",
            "[codecarbon INFO @ 23:34:59] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:34:59] 0.003464 kWh of electricity used since the beginning.\n",
            "Epoch 3/20 - Training:  62%|██████▏   | 389/625 [00:34<00:18, 12.83it/s][codecarbon INFO @ 23:35:14] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:35:14] Energy consumed for all GPUs : 0.001881 kWh. Total GPU Power : 45.86439238924115 W\n",
            "[codecarbon INFO @ 23:35:14] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:35:14] 0.003851 kWh of electricity used since the beginning.\n",
            "Epoch 3/20 - Training:  89%|████████▉ | 558/625 [00:49<00:05, 13.31it/s][codecarbon INFO @ 23:35:29] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:35:29] Energy consumed for all GPUs : 0.002073 kWh. Total GPU Power : 46.04748569305037 W\n",
            "[codecarbon INFO @ 23:35:29] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:35:29] 0.004240 kWh of electricity used since the beginning.\n",
            "Epoch 3/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 2.3629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 - Training:  16%|█▌        | 101/625 [00:08<00:49, 10.62it/s][codecarbon INFO @ 23:35:45] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:35:45] Energy consumed for all GPUs : 0.002268 kWh. Total GPU Power : 46.83372460074494 W\n",
            "[codecarbon INFO @ 23:35:45] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:35:45] 0.004632 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  44%|████▎     | 272/625 [00:23<00:32, 10.86it/s][codecarbon INFO @ 23:36:00] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:00] Energy consumed for all GPUs : 0.002465 kWh. Total GPU Power : 47.069207056386546 W\n",
            "[codecarbon INFO @ 23:36:00] Energy consumed for all CPUs : 0.002304 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:00] 0.005026 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  70%|███████   | 440/625 [00:38<00:19,  9.43it/s][codecarbon INFO @ 23:36:15] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:15] Energy consumed for all GPUs : 0.002652 kWh. Total GPU Power : 44.996186212726094 W\n",
            "[codecarbon INFO @ 23:36:15] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:15] 0.005411 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  97%|█████████▋| 608/625 [00:53<00:01,  8.87it/s][codecarbon INFO @ 23:36:30] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:30] Energy consumed for all GPUs : 0.002843 kWh. Total GPU Power : 45.85524085414069 W\n",
            "[codecarbon INFO @ 23:36:30] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:30] 0.005799 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 2.1460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 - Training:  23%|██▎       | 146/625 [00:13<00:57,  8.31it/s][codecarbon INFO @ 23:36:45] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:45] Energy consumed for all GPUs : 0.003042 kWh. Total GPU Power : 47.652890741154074 W\n",
            "[codecarbon INFO @ 23:36:45] Energy consumed for all CPUs : 0.002836 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:45] 0.006195 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:36:45] 0.006951 g.CO2eq/s mean an estimation of 219.20874695789925 kg.CO2eq/year\n",
            "Epoch 5/20 - Training:  49%|████▉     | 309/625 [00:28<00:37,  8.44it/s][codecarbon INFO @ 23:37:00] Energy consumed for RAM : 0.000337 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:00] Energy consumed for all GPUs : 0.003231 kWh. Total GPU Power : 45.29230699122492 W\n",
            "[codecarbon INFO @ 23:37:00] Energy consumed for all CPUs : 0.003013 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:00] 0.006580 kWh of electricity used since the beginning.\n",
            "Epoch 5/20 - Training:  76%|███████▌  | 472/625 [00:43<00:21,  7.01it/s][codecarbon INFO @ 23:37:15] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:15] Energy consumed for all GPUs : 0.003413 kWh. Total GPU Power : 43.63348949693993 W\n",
            "[codecarbon INFO @ 23:37:15] Energy consumed for all CPUs : 0.003190 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:15] 0.006960 kWh of electricity used since the beginning.\n",
            "Epoch 5/20 - Training: 100%|██████████| 625/625 [00:57<00:00, 10.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 1.9470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 - Training:   1%|▏         | 9/625 [00:01<01:04,  9.62it/s][codecarbon INFO @ 23:37:30] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:30] Energy consumed for all GPUs : 0.003599 kWh. Total GPU Power : 44.724579182340364 W\n",
            "[codecarbon INFO @ 23:37:30] Energy consumed for all CPUs : 0.003367 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:30] 0.007343 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  28%|██▊       | 177/625 [00:16<00:33, 13.45it/s][codecarbon INFO @ 23:37:45] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:45] Energy consumed for all GPUs : 0.003791 kWh. Total GPU Power : 45.942302458010104 W\n",
            "[codecarbon INFO @ 23:37:45] Energy consumed for all CPUs : 0.003545 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:45] 0.007732 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  55%|█████▌    | 344/625 [00:31<00:23, 11.95it/s][codecarbon INFO @ 23:38:00] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:00] Energy consumed for all GPUs : 0.003981 kWh. Total GPU Power : 45.61923547194165 W\n",
            "[codecarbon INFO @ 23:38:00] Energy consumed for all CPUs : 0.003722 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:00] 0.008119 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  82%|████████▏ | 510/625 [00:46<00:09, 12.35it/s][codecarbon INFO @ 23:38:15] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:15] Energy consumed for all GPUs : 0.004174 kWh. Total GPU Power : 46.48315152732881 W\n",
            "[codecarbon INFO @ 23:38:15] Energy consumed for all CPUs : 0.003899 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:15] 0.008509 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 1.7895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 - Training:   8%|▊         | 50/625 [00:05<00:44, 12.84it/s][codecarbon INFO @ 23:38:30] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:30] Energy consumed for all GPUs : 0.004374 kWh. Total GPU Power : 47.957519084150526 W\n",
            "[codecarbon INFO @ 23:38:30] Energy consumed for all CPUs : 0.004076 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:30] 0.008905 kWh of electricity used since the beginning.\n",
            "Epoch 7/20 - Training:  35%|███▌      | 220/625 [00:20<00:29, 13.56it/s][codecarbon INFO @ 23:38:45] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:45] Energy consumed for all GPUs : 0.004564 kWh. Total GPU Power : 45.59353631716664 W\n",
            "[codecarbon INFO @ 23:38:45] Energy consumed for all CPUs : 0.004253 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:45] 0.009292 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:38:45] 0.006901 g.CO2eq/s mean an estimation of 217.62263346757288 kg.CO2eq/year\n",
            "Epoch 7/20 - Training:  63%|██████▎   | 391/625 [00:35<00:18, 12.41it/s][codecarbon INFO @ 23:39:00] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:00] Energy consumed for all GPUs : 0.004752 kWh. Total GPU Power : 45.22992160872415 W\n",
            "[codecarbon INFO @ 23:39:00] Energy consumed for all CPUs : 0.004430 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:00] 0.009678 kWh of electricity used since the beginning.\n",
            "Epoch 7/20 - Training:  90%|████████▉ | 560/625 [00:50<00:05, 12.89it/s][codecarbon INFO @ 23:39:15] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:15] Energy consumed for all GPUs : 0.004941 kWh. Total GPU Power : 45.32463710730274 W\n",
            "[codecarbon INFO @ 23:39:15] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:15] 0.010063 kWh of electricity used since the beginning.\n",
            "Epoch 7/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 1.6342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 - Training:  16%|█▌        | 100/625 [00:10<00:37, 13.93it/s][codecarbon INFO @ 23:39:30] Energy consumed for RAM : 0.000535 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:30] Energy consumed for all GPUs : 0.005138 kWh. Total GPU Power : 47.36473991488346 W\n",
            "[codecarbon INFO @ 23:39:30] Energy consumed for all CPUs : 0.004785 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:30] 0.010458 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  43%|████▎     | 269/625 [00:25<00:27, 12.97it/s][codecarbon INFO @ 23:39:45] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:45] Energy consumed for all GPUs : 0.005334 kWh. Total GPU Power : 47.10974902594656 W\n",
            "[codecarbon INFO @ 23:39:45] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:45] 0.010850 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  70%|███████   | 439/625 [00:40<00:13, 13.55it/s][codecarbon INFO @ 23:40:00] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:00] Energy consumed for all GPUs : 0.005534 kWh. Total GPU Power : 47.91833288038758 W\n",
            "[codecarbon INFO @ 23:40:00] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:00] 0.011246 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  98%|█████████▊| 610/625 [00:55<00:01, 12.17it/s][codecarbon INFO @ 23:40:15] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:15] Energy consumed for all GPUs : 0.005722 kWh. Total GPU Power : 45.0869382832753 W\n",
            "[codecarbon INFO @ 23:40:15] Energy consumed for all CPUs : 0.005316 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:15] 0.011632 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 1.5031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 - Training:  24%|██▎       | 148/625 [00:14<00:39, 11.98it/s][codecarbon INFO @ 23:40:30] Energy consumed for RAM : 0.000614 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:30] Energy consumed for all GPUs : 0.005914 kWh. Total GPU Power : 46.00565137667285 W\n",
            "[codecarbon INFO @ 23:40:30] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:30] 0.012021 kWh of electricity used since the beginning.\n",
            "Epoch 9/20 - Training:  51%|█████     | 318/625 [00:29<00:25, 12.17it/s][codecarbon INFO @ 23:40:45] Energy consumed for RAM : 0.000634 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:45] Energy consumed for all GPUs : 0.006102 kWh. Total GPU Power : 45.22571584743723 W\n",
            "[codecarbon INFO @ 23:40:45] Energy consumed for all CPUs : 0.005670 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:45] 0.012406 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:40:45] 0.006936 g.CO2eq/s mean an estimation of 218.74411965850535 kg.CO2eq/year\n",
            "Epoch 9/20 - Training:  78%|███████▊  | 486/625 [00:44<00:10, 13.25it/s][codecarbon INFO @ 23:41:00] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:00] Energy consumed for all GPUs : 0.006296 kWh. Total GPU Power : 46.57420863961769 W\n",
            "[codecarbon INFO @ 23:41:00] Energy consumed for all CPUs : 0.005847 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:00] 0.012796 kWh of electricity used since the beginning.\n",
            "Epoch 9/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 1.4043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 - Training:   5%|▍         | 30/625 [00:02<00:46, 12.92it/s][codecarbon INFO @ 23:41:15] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:15] Energy consumed for all GPUs : 0.006492 kWh. Total GPU Power : 46.9987238176603 W\n",
            "[codecarbon INFO @ 23:41:15] Energy consumed for all CPUs : 0.006024 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:15] 0.013189 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training:  32%|███▏      | 198/625 [00:17<00:31, 13.72it/s][codecarbon INFO @ 23:41:30] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:30] Energy consumed for all GPUs : 0.006687 kWh. Total GPU Power : 46.931704624177904 W\n",
            "[codecarbon INFO @ 23:41:30] Energy consumed for all CPUs : 0.006202 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:30] 0.013582 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training:  59%|█████▊    | 367/625 [00:32<00:20, 12.75it/s][codecarbon INFO @ 23:41:45] Energy consumed for RAM : 0.000713 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:45] Energy consumed for all GPUs : 0.006873 kWh. Total GPU Power : 44.472880776317474 W\n",
            "[codecarbon INFO @ 23:41:45] Energy consumed for all CPUs : 0.006379 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:45] 0.013964 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training:  86%|████████▌ | 539/625 [00:47<00:06, 12.72it/s][codecarbon INFO @ 23:42:00] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:00] Energy consumed for all GPUs : 0.007064 kWh. Total GPU Power : 45.87314205393357 W\n",
            "[codecarbon INFO @ 23:42:00] Energy consumed for all CPUs : 0.006556 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:00] 0.014352 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 1.3845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 - Training:  13%|█▎        | 83/625 [00:07<00:42, 12.80it/s][codecarbon INFO @ 23:42:15] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:15] Energy consumed for all GPUs : 0.007266 kWh. Total GPU Power : 48.6460306171101 W\n",
            "[codecarbon INFO @ 23:42:15] Energy consumed for all CPUs : 0.006733 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:15] 0.014752 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training:  40%|████      | 252/625 [00:22<00:28, 13.09it/s][codecarbon INFO @ 23:42:30] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:30] Energy consumed for all GPUs : 0.007453 kWh. Total GPU Power : 44.76248928881561 W\n",
            "[codecarbon INFO @ 23:42:30] Energy consumed for all CPUs : 0.006910 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:30] 0.015135 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training:  68%|██████▊   | 425/625 [00:37<00:13, 14.60it/s][codecarbon INFO @ 23:42:45] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:45] Energy consumed for all GPUs : 0.007642 kWh. Total GPU Power : 45.54150293151468 W\n",
            "[codecarbon INFO @ 23:42:45] Energy consumed for all CPUs : 0.007087 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:45] 0.015521 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:42:45] 0.006944 g.CO2eq/s mean an estimation of 218.98844837790088 kg.CO2eq/year\n",
            "Epoch 11/20 - Training:  95%|█████████▌| 595/625 [00:52<00:02, 13.76it/s][codecarbon INFO @ 23:43:00] Energy consumed for RAM : 0.000812 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:00] Energy consumed for all GPUs : 0.007835 kWh. Total GPU Power : 46.344835737010996 W\n",
            "[codecarbon INFO @ 23:43:00] Energy consumed for all CPUs : 0.007264 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:00] 0.015911 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 1.3498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 - Training:  22%|██▏       | 138/625 [00:12<00:37, 12.96it/s][codecarbon INFO @ 23:43:15] Energy consumed for RAM : 0.000831 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:15] Energy consumed for all GPUs : 0.008034 kWh. Total GPU Power : 47.59411561262786 W\n",
            "[codecarbon INFO @ 23:43:15] Energy consumed for all CPUs : 0.007442 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:15] 0.016307 kWh of electricity used since the beginning.\n",
            "Epoch 12/20 - Training:  49%|████▉     | 308/625 [00:27<00:23, 13.53it/s][codecarbon INFO @ 23:43:30] Energy consumed for RAM : 0.000851 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:30] Energy consumed for all GPUs : 0.008222 kWh. Total GPU Power : 44.978275474167376 W\n",
            "[codecarbon INFO @ 23:43:30] Energy consumed for all CPUs : 0.007619 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:30] 0.016692 kWh of electricity used since the beginning.\n",
            "Epoch 12/20 - Training:  76%|███████▋  | 477/625 [00:42<00:11, 13.22it/s][codecarbon INFO @ 23:43:45] Energy consumed for RAM : 0.000871 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:45] Energy consumed for all GPUs : 0.008411 kWh. Total GPU Power : 45.44904823719209 W\n",
            "[codecarbon INFO @ 23:43:45] Energy consumed for all CPUs : 0.007796 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:45] 0.017078 kWh of electricity used since the beginning.\n",
            "Epoch 12/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 1.3363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 - Training:   3%|▎         | 18/625 [00:02<00:57, 10.62it/s][codecarbon INFO @ 23:44:00] Energy consumed for RAM : 0.000891 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:00] Energy consumed for all GPUs : 0.008608 kWh. Total GPU Power : 47.32476412525922 W\n",
            "[codecarbon INFO @ 23:44:00] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:00] 0.017472 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  30%|██▉       | 186/625 [00:17<00:44,  9.94it/s][codecarbon INFO @ 23:44:15] Energy consumed for RAM : 0.000911 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:15] Energy consumed for all GPUs : 0.008810 kWh. Total GPU Power : 48.50013705076627 W\n",
            "[codecarbon INFO @ 23:44:15] Energy consumed for all CPUs : 0.008151 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:15] 0.017871 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  57%|█████▋    | 354/625 [00:32<00:31,  8.66it/s][codecarbon INFO @ 23:44:30] Energy consumed for RAM : 0.000930 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:30] Energy consumed for all GPUs : 0.008999 kWh. Total GPU Power : 45.46579223716749 W\n",
            "[codecarbon INFO @ 23:44:30] Energy consumed for all CPUs : 0.008328 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:30] 0.018258 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  83%|████████▎ | 520/625 [00:47<00:12,  8.33it/s][codecarbon INFO @ 23:44:45] Energy consumed for RAM : 0.000950 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:45] Energy consumed for all GPUs : 0.009187 kWh. Total GPU Power : 45.22944988190889 W\n",
            "[codecarbon INFO @ 23:44:45] Energy consumed for all CPUs : 0.008504 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:45] 0.018642 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:44:45] 0.006950 g.CO2eq/s mean an estimation of 219.1896221643228 kg.CO2eq/year\n",
            "Epoch 13/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 1.3467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 - Training:  10%|▉         | 60/625 [00:06<01:14,  7.54it/s][codecarbon INFO @ 23:45:00] Energy consumed for RAM : 0.000970 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:45:00] Energy consumed for all GPUs : 0.009380 kWh. Total GPU Power : 46.191294499638786 W\n",
            "[codecarbon INFO @ 23:45:00] Energy consumed for all CPUs : 0.008682 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:45:00] 0.019031 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training:  37%|███▋      | 229/625 [00:21<00:49,  8.07it/s][codecarbon INFO @ 23:45:15] Energy consumed for RAM : 0.000990 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:45:15] Energy consumed for all GPUs : 0.009574 kWh. Total GPU Power : 46.59115313687614 W\n",
            "[codecarbon INFO @ 23:45:15] Energy consumed for all CPUs : 0.008859 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:45:15] 0.019422 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training:  64%|██████▎   | 397/625 [00:36<00:29,  7.68it/s][codecarbon INFO @ 23:45:30] Energy consumed for RAM : 0.001010 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:45:30] Energy consumed for all GPUs : 0.009762 kWh. Total GPU Power : 45.13834731585305 W\n",
            "[codecarbon INFO @ 23:45:30] Energy consumed for all CPUs : 0.009036 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:45:30] 0.019807 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training:  90%|█████████ | 565/625 [00:50<00:07,  8.26it/s][codecarbon INFO @ 23:45:45] Energy consumed for RAM : 0.001029 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:45:45] Energy consumed for all GPUs : 0.009950 kWh. Total GPU Power : 45.290946120037155 W\n",
            "[codecarbon INFO @ 23:45:45] Energy consumed for all CPUs : 0.009213 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:45:45] 0.020192 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 1.4037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 - Training:  16%|█▋        | 103/625 [00:10<01:10,  7.36it/s][codecarbon INFO @ 23:46:00] Energy consumed for RAM : 0.001049 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:46:00] Energy consumed for all GPUs : 0.010147 kWh. Total GPU Power : 47.085502483794976 W\n",
            "[codecarbon INFO @ 23:46:00] Energy consumed for all CPUs : 0.009390 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:46:00] 0.020586 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  44%|████▍     | 274/625 [00:25<00:51,  6.88it/s][codecarbon INFO @ 23:46:15] Energy consumed for RAM : 0.001069 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:46:15] Energy consumed for all GPUs : 0.010341 kWh. Total GPU Power : 46.62214326863727 W\n",
            "[codecarbon INFO @ 23:46:15] Energy consumed for all CPUs : 0.009567 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:46:15] 0.020977 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  71%|███████   | 441/625 [00:40<00:24,  7.41it/s][codecarbon INFO @ 23:46:30] Energy consumed for RAM : 0.001089 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:46:30] Energy consumed for all GPUs : 0.010529 kWh. Total GPU Power : 45.17256039610094 W\n",
            "Epoch 15/20 - Training:  71%|███████   | 442/625 [00:40<00:24,  7.48it/s][codecarbon INFO @ 23:46:30] Energy consumed for all CPUs : 0.009744 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:46:30] 0.021362 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  98%|█████████▊| 611/625 [00:55<00:02,  6.94it/s][codecarbon INFO @ 23:46:45] Energy consumed for RAM : 0.001108 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:46:45] Energy consumed for all GPUs : 0.010715 kWh. Total GPU Power : 44.80618287924437 W\n",
            "[codecarbon INFO @ 23:46:45] Energy consumed for all CPUs : 0.009921 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:46:45] 0.021745 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:46:45] 0.006915 g.CO2eq/s mean an estimation of 218.08274983497247 kg.CO2eq/year\n",
            "Epoch 15/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 1.4510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 - Training:  24%|██▍       | 151/625 [00:13<00:58,  8.08it/s][codecarbon INFO @ 23:47:00] Energy consumed for RAM : 0.001128 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:47:00] Energy consumed for all GPUs : 0.010910 kWh. Total GPU Power : 46.81646254736509 W\n",
            "[codecarbon INFO @ 23:47:00] Energy consumed for all CPUs : 0.010098 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:47:00] 0.022137 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training:  51%|█████     | 320/625 [00:28<00:36,  8.45it/s][codecarbon INFO @ 23:47:15] Energy consumed for RAM : 0.001148 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:47:15] Energy consumed for all GPUs : 0.011100 kWh. Total GPU Power : 45.532307534414855 W\n",
            "[codecarbon INFO @ 23:47:15] Energy consumed for all CPUs : 0.010276 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:47:15] 0.022524 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training:  79%|███████▊  | 492/625 [00:43<00:12, 11.07it/s][codecarbon INFO @ 23:47:30] Energy consumed for RAM : 0.001168 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:47:30] Energy consumed for all GPUs : 0.011291 kWh. Total GPU Power : 45.94314063410075 W\n",
            "[codecarbon INFO @ 23:47:30] Energy consumed for all CPUs : 0.010453 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:47:30] 0.022912 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 1.5326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 - Training:   5%|▌         | 34/625 [00:04<00:50, 11.72it/s][codecarbon INFO @ 23:47:45] Energy consumed for RAM : 0.001188 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:47:45] Energy consumed for all GPUs : 0.011483 kWh. Total GPU Power : 46.07143037775519 W\n",
            "[codecarbon INFO @ 23:47:45] Energy consumed for all CPUs : 0.010630 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:47:45] 0.023301 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training:  33%|███▎      | 205/625 [00:19<00:37, 11.07it/s][codecarbon INFO @ 23:48:00] Energy consumed for RAM : 0.001207 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:48:00] Energy consumed for all GPUs : 0.011675 kWh. Total GPU Power : 45.94947702899506 W\n",
            "[codecarbon INFO @ 23:48:00] Energy consumed for all CPUs : 0.010807 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:48:00] 0.023689 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training:  60%|██████    | 376/625 [00:34<00:20, 12.04it/s][codecarbon INFO @ 23:48:15] Energy consumed for RAM : 0.001227 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:48:15] Energy consumed for all GPUs : 0.011865 kWh. Total GPU Power : 45.66645499620764 W\n",
            "[codecarbon INFO @ 23:48:15] Energy consumed for all CPUs : 0.010984 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:48:15] 0.024076 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training:  88%|████████▊ | 548/625 [00:49<00:05, 13.61it/s][codecarbon INFO @ 23:48:30] Energy consumed for RAM : 0.001247 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:48:30] Energy consumed for all GPUs : 0.012055 kWh. Total GPU Power : 45.66937398368942 W\n",
            "[codecarbon INFO @ 23:48:30] Energy consumed for all CPUs : 0.011161 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:48:30] 0.024464 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 1.6295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 - Training:  14%|█▍        | 89/625 [00:09<00:42, 12.58it/s][codecarbon INFO @ 23:48:45] Energy consumed for RAM : 0.001267 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:48:45] Energy consumed for all GPUs : 0.012247 kWh. Total GPU Power : 46.00252001605889 W\n",
            "[codecarbon INFO @ 23:48:45] Energy consumed for all CPUs : 0.011338 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:48:45] 0.024852 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:48:45] 0.006925 g.CO2eq/s mean an estimation of 218.37448464981404 kg.CO2eq/year\n",
            "Epoch 18/20 - Training:  42%|████▏     | 261/625 [00:24<00:29, 12.49it/s][codecarbon INFO @ 23:49:00] Energy consumed for RAM : 0.001287 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:49:00] Energy consumed for all GPUs : 0.012436 kWh. Total GPU Power : 45.24467060053045 W\n",
            "[codecarbon INFO @ 23:49:00] Energy consumed for all CPUs : 0.011516 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:49:00] 0.025238 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training:  69%|██████▉   | 430/625 [00:39<00:15, 12.92it/s][codecarbon INFO @ 23:49:15] Energy consumed for RAM : 0.001306 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:49:15] Energy consumed for all GPUs : 0.012626 kWh. Total GPU Power : 45.63424270305722 W\n",
            "[codecarbon INFO @ 23:49:15] Energy consumed for all CPUs : 0.011693 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:49:15] 0.025625 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training:  96%|█████████▌| 600/625 [00:54<00:02, 12.21it/s][codecarbon INFO @ 23:49:30] Energy consumed for RAM : 0.001326 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:49:30] Energy consumed for all GPUs : 0.012816 kWh. Total GPU Power : 45.70901335917457 W\n",
            "[codecarbon INFO @ 23:49:30] Energy consumed for all CPUs : 0.011870 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:49:30] 0.026012 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 1.6786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 - Training:  23%|██▎       | 142/625 [00:13<00:35, 13.72it/s][codecarbon INFO @ 23:49:45] Energy consumed for RAM : 0.001346 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:49:45] Energy consumed for all GPUs : 0.013013 kWh. Total GPU Power : 47.22266973078153 W\n",
            "[codecarbon INFO @ 23:49:45] Energy consumed for all CPUs : 0.012047 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:49:45] 0.026406 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training:  50%|█████     | 314/625 [00:28<00:24, 12.50it/s][codecarbon INFO @ 23:50:00] Energy consumed for RAM : 0.001366 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:50:00] Energy consumed for all GPUs : 0.013212 kWh. Total GPU Power : 47.735671088930154 W\n",
            "[codecarbon INFO @ 23:50:00] Energy consumed for all CPUs : 0.012224 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:50:00] 0.026802 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training:  77%|███████▋  | 483/625 [00:43<00:11, 12.43it/s][codecarbon INFO @ 23:50:15] Energy consumed for RAM : 0.001386 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:50:15] Energy consumed for all GPUs : 0.013401 kWh. Total GPU Power : 45.372046529807754 W\n",
            "[codecarbon INFO @ 23:50:15] Energy consumed for all CPUs : 0.012401 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:50:15] 0.027188 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 1.7053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 - Training:   4%|▍         | 27/625 [00:02<00:48, 12.40it/s][codecarbon INFO @ 23:50:30] Energy consumed for RAM : 0.001405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:50:30] Energy consumed for all GPUs : 0.013594 kWh. Total GPU Power : 46.24896001087872 W\n",
            "[codecarbon INFO @ 23:50:30] Energy consumed for all CPUs : 0.012579 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:50:30] 0.027578 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training:  31%|███       | 195/625 [00:17<00:32, 13.11it/s][codecarbon INFO @ 23:50:45] Energy consumed for RAM : 0.001425 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:50:45] Energy consumed for all GPUs : 0.013785 kWh. Total GPU Power : 45.99967895186482 W\n",
            "[codecarbon INFO @ 23:50:45] Energy consumed for all CPUs : 0.012756 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:50:45] 0.027966 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:50:45] 0.006937 g.CO2eq/s mean an estimation of 218.77745618764516 kg.CO2eq/year\n",
            "Epoch 20/20 - Training:  59%|█████▊    | 367/625 [00:32<00:19, 13.44it/s][codecarbon INFO @ 23:51:00] Energy consumed for RAM : 0.001445 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:51:00] Energy consumed for all GPUs : 0.013977 kWh. Total GPU Power : 46.077369464427555 W\n",
            "[codecarbon INFO @ 23:51:00] Energy consumed for all CPUs : 0.012933 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:51:00] 0.028355 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training:  86%|████████▌ | 538/625 [00:47<00:06, 13.73it/s][codecarbon INFO @ 23:51:15] Energy consumed for RAM : 0.001465 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:51:15] Energy consumed for all GPUs : 0.014166 kWh. Total GPU Power : 45.27537735071433 W\n",
            "[codecarbon INFO @ 23:51:15] Energy consumed for all CPUs : 0.013110 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:51:15] 0.028741 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.18it/s]\n",
            "[codecarbon INFO @ 23:51:24] Energy consumed for RAM : 0.001476 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:51:24] Energy consumed for all GPUs : 0.014267 kWh. Total GPU Power : 44.621642578402025 W\n",
            "[codecarbon INFO @ 23:51:24] Energy consumed for all CPUs : 0.013206 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:51:24] 0.028949 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 1.7211\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 576, 1, 1])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a051c6344052>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProfilerActivity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProfilerActivity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mflops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_time_total\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Cannot use \"squeeze\" as batch-size can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2808\u001b[0m         )\n\u001b[1;32m   2809\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2812\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2774\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2776\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2777\u001b[0m             \u001b[0;34mf\"Expected more than 1 value per channel when training, got input size {size}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 576, 1, 1])"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import codecarbon\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define dataset transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Load teacher model\n",
        "teacher_model = models.vgg16(pretrained=False)\n",
        "teacher_model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 10)\n",
        ")\n",
        "teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\"))\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# Define student model\n",
        "student_model = models.mobilenet_v2(pretrained=True)\n",
        "student_model.classifier[1] = nn.Sequential(\n",
        "    nn.Linear(student_model.last_channel, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 10)\n",
        ")\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "# Define distillation loss\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=3.0, alpha=0.5):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        distillation_loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "            torch.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            torch.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        ) * (self.temperature ** 2)\n",
        "        student_loss = self.criterion_ce(student_logits, labels)\n",
        "        return self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n",
        "\n",
        "criterion = DistillationLoss(temperature=3.0, alpha=0.5)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.AdamW(student_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# Track energy consumption\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "start_time = time.time()\n",
        "gpu_power_readings = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "        student_outputs = student_model(inputs)\n",
        "        loss = criterion(student_outputs, teacher_outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Stop CodeCarbon tracker\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Save model\n",
        "torch.save(student_model.state_dict(), \"student_model_mobilenetv2.pth\")\n",
        "\n",
        "# Model analysis\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp_model.pth\")\n",
        "    size_mb = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)\n",
        "    os.remove(\"temp_model.pth\")\n",
        "    return size_mb\n",
        "\n",
        "def measure_inference_time(model, dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for inputs, _ in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            _ = model(inputs)\n",
        "        end_time = time.time()\n",
        "    return (end_time - start_time) / len(dataloader)\n",
        "\n",
        "# Calculate FLOPs using torch.profiler\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "    student_model(dummy_input)\n",
        "\n",
        "flops = sum(event.cpu_time_total for event in prof.key_averages())\n",
        "\n",
        "# Average GPU power consumption\n",
        "avg_gpu_power = sum(gpu_power_readings) / len(gpu_power_readings) if gpu_power_readings else 0.0\n",
        "total_energy_consumption = (avg_gpu_power * training_time) / 3600  # kWh\n",
        "\n",
        "# Calculate and display final metrics\n",
        "param_count = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "model_size_mb = get_model_size(student_model)\n",
        "inference_time_ms = measure_inference_time(student_model, test_loader) * 1000  # ms\n",
        "\n",
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Average Inference Time: {inference_time_ms:.6f} ms\")\n",
        "\n",
        "print(\"\\n--- Energy and Emissions Report ---\")\n",
        "print(f\"CO2 Emissions (CodeCarbon): {emissions:.6f} kg\")\n",
        "print(f\"Average GPU Power Consumption: {avg_gpu_power:.2f} W\")\n",
        "print(f\"Total GPU Energy Consumption: {total_energy_consumption:.6f} kWh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wp5KQXhLliVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52406afa-9e41-4487-a821-ac1d56abf55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model Test Accuracy: 78.43%\n"
          ]
        }
      ],
      "source": [
        "student_model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = student_model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Student Model Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}