{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oPtyquOgWNKK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentations for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "uRYvC0asW4Cu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install codecarbon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y1SGfc4XXC0g",
        "outputId": "81e5f231-8067-4ad9-a277-40b4a9478a12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.7.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Collecting pynvml (from codecarbon)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Collecting prompt_toolkit<=3.0.36,>=2.0 (from questionary->codecarbon)\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.7.4-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.2/504.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.0.1-py3-none-any.whl (34 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, pynvml, prompt_toolkit, yaspin, questionary, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: prompt_toolkit\n",
            "    Found existing installation: prompt_toolkit 3.0.48\n",
            "    Uninstalling prompt_toolkit-3.0.48:\n",
            "      Successfully uninstalled prompt_toolkit-3.0.48\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.7.4 fief-client-0.20.0 jwcrypto-1.5.6 prompt_toolkit-3.0.36 pynvml-11.5.3 questionary-2.0.1 rapidfuzz-3.10.1 termcolor-2.3.0 types-python-dateutil-2.9.0.20241003 yaspin-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              },
              "id": "68fc5179ed534401a338c3d9074eb4af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRHBuPRQXG7U",
        "outputId": "f093c532-f29f-49d5-8112-578f7a553bb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YOGO4GQXeMD",
        "outputId": "997e24f9-2351-400b-c48d-bd342bc8eb3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset with train/test splits\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split the training data into training and validation sets (80% train, 20% validation)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3FwQMlFXpSx",
        "outputId": "114cac9b-409b-4357-f145-8135141fd329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ResNet-50 model and modify the final layer\n",
        "teacher_model = models.resnet50(pretrained=True)\n",
        "teacher_model.fc = nn.Linear(2048, 10)  # CIFAR-10 has 10 classes\n",
        "\n",
        "# Check if GPU is available and move model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher_model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and learning rate scheduler\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer_teacher, step_size=10, gamma=0.1)  # Reduce LR by 0.1 every 10 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if-gn7LrXxtn",
        "outputId": "8151f732-c59b-44ec-bdd6-c43563c9ec14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 177MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs_teacher = 30  # Increased epochs for better accuracy\n",
        "best_val_accuracy = 0.0\n",
        "start_time_teacher = time.time()\n",
        "\n",
        "for epoch in range(num_epochs_teacher):\n",
        "    # Training phase\n",
        "    teacher_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Teacher Model Training Epoch [{epoch + 1}/{num_epochs_teacher}]'):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_teacher.zero_grad()\n",
        "        outputs = teacher_model(inputs)\n",
        "        loss = criterion_ce(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_teacher.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation phase\n",
        "    teacher_model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = teacher_model(inputs)\n",
        "            loss = criterion_ce(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(teacher_model.state_dict(), 'teacher_model_best.pth')\n",
        "        print(f\"Best model saved with accuracy: {best_val_accuracy:.2f}%\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs_teacher}], \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "end_time_teacher = time.time()\n",
        "print(f\"Teacher training completed in {(end_time_teacher - start_time_teacher) / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSbD2EUcX2um",
        "outputId": "e3bc8213-cfbd-410c-def1-83c1a660a0aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [1/30]: 100%|██████████| 313/313 [00:45<00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 53.67%\n",
            "Epoch [1/30], Train Loss: 1.3850, Train Accuracy: 51.98%, Val Loss: 1.3176, Val Accuracy: 53.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [2/30]: 100%|██████████| 313/313 [00:42<00:00,  7.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 64.93%\n",
            "Epoch [2/30], Train Loss: 1.0584, Train Accuracy: 63.58%, Val Loss: 1.0253, Val Accuracy: 64.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [3/30]: 100%|██████████| 313/313 [00:42<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 65.46%\n",
            "Epoch [3/30], Train Loss: 0.9553, Train Accuracy: 67.41%, Val Loss: 0.9959, Val Accuracy: 65.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [4/30]: 100%|██████████| 313/313 [00:42<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 71.03%\n",
            "Epoch [4/30], Train Loss: 0.9145, Train Accuracy: 68.76%, Val Loss: 0.8512, Val Accuracy: 71.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [5/30]: 100%|██████████| 313/313 [00:42<00:00,  7.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Train Loss: 0.8272, Train Accuracy: 72.11%, Val Loss: 1.3268, Val Accuracy: 59.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [6/30]: 100%|██████████| 313/313 [00:44<00:00,  7.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 73.43%\n",
            "Epoch [6/30], Train Loss: 0.8165, Train Accuracy: 72.28%, Val Loss: 0.7673, Val Accuracy: 73.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [7/30]: 100%|██████████| 313/313 [00:44<00:00,  6.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Train Loss: 0.7642, Train Accuracy: 74.10%, Val Loss: 0.7968, Val Accuracy: 72.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [8/30]: 100%|██████████| 313/313 [00:44<00:00,  6.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 74.69%\n",
            "Epoch [8/30], Train Loss: 0.7126, Train Accuracy: 75.70%, Val Loss: 0.7381, Val Accuracy: 74.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [9/30]: 100%|██████████| 313/313 [00:43<00:00,  7.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Train Loss: 0.7126, Train Accuracy: 75.47%, Val Loss: 0.7504, Val Accuracy: 74.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [10/30]: 100%|██████████| 313/313 [00:42<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 75.19%\n",
            "Epoch [10/30], Train Loss: 0.6851, Train Accuracy: 76.65%, Val Loss: 0.7288, Val Accuracy: 75.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [11/30]: 100%|██████████| 313/313 [00:42<00:00,  7.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 79.51%\n",
            "Epoch [11/30], Train Loss: 0.5609, Train Accuracy: 80.78%, Val Loss: 0.5796, Val Accuracy: 79.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [12/30]: 100%|██████████| 313/313 [00:42<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 80.31%\n",
            "Epoch [12/30], Train Loss: 0.5174, Train Accuracy: 82.19%, Val Loss: 0.5633, Val Accuracy: 80.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [13/30]: 100%|██████████| 313/313 [00:42<00:00,  7.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 81.40%\n",
            "Epoch [13/30], Train Loss: 0.4981, Train Accuracy: 82.88%, Val Loss: 0.5452, Val Accuracy: 81.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [14/30]: 100%|██████████| 313/313 [00:42<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Train Loss: 0.4796, Train Accuracy: 83.47%, Val Loss: 0.5373, Val Accuracy: 81.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [15/30]: 100%|██████████| 313/313 [00:42<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Train Loss: 0.4697, Train Accuracy: 83.83%, Val Loss: 0.5378, Val Accuracy: 81.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [16/30]: 100%|██████████| 313/313 [00:42<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 81.52%\n",
            "Epoch [16/30], Train Loss: 0.4585, Train Accuracy: 84.03%, Val Loss: 0.5288, Val Accuracy: 81.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [17/30]: 100%|██████████| 313/313 [00:43<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 82.14%\n",
            "Epoch [17/30], Train Loss: 0.4525, Train Accuracy: 84.36%, Val Loss: 0.5214, Val Accuracy: 82.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [18/30]: 100%|██████████| 313/313 [00:43<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Train Loss: 0.4431, Train Accuracy: 84.60%, Val Loss: 0.5177, Val Accuracy: 81.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [19/30]: 100%|██████████| 313/313 [00:42<00:00,  7.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Train Loss: 0.4325, Train Accuracy: 84.93%, Val Loss: 0.5226, Val Accuracy: 81.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [20/30]: 100%|██████████| 313/313 [00:42<00:00,  7.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 82.26%\n",
            "Epoch [20/30], Train Loss: 0.4257, Train Accuracy: 85.07%, Val Loss: 0.5096, Val Accuracy: 82.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [21/30]: 100%|██████████| 313/313 [00:42<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 82.96%\n",
            "Epoch [21/30], Train Loss: 0.4110, Train Accuracy: 85.49%, Val Loss: 0.5054, Val Accuracy: 82.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [22/30]: 100%|██████████| 313/313 [00:42<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Train Loss: 0.4050, Train Accuracy: 85.89%, Val Loss: 0.4964, Val Accuracy: 82.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [23/30]: 100%|██████████| 313/313 [00:42<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Train Loss: 0.3994, Train Accuracy: 86.06%, Val Loss: 0.4991, Val Accuracy: 82.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [24/30]: 100%|██████████| 313/313 [00:42<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Train Loss: 0.3993, Train Accuracy: 86.05%, Val Loss: 0.4957, Val Accuracy: 82.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [25/30]: 100%|██████████| 313/313 [00:43<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Train Loss: 0.3989, Train Accuracy: 86.05%, Val Loss: 0.4945, Val Accuracy: 82.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [26/30]: 100%|██████████| 313/313 [00:41<00:00,  7.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Train Loss: 0.3953, Train Accuracy: 86.24%, Val Loss: 0.4989, Val Accuracy: 82.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [27/30]: 100%|██████████| 313/313 [00:42<00:00,  7.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Train Loss: 0.3890, Train Accuracy: 86.42%, Val Loss: 0.5034, Val Accuracy: 82.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [28/30]: 100%|██████████| 313/313 [00:42<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 83.04%\n",
            "Epoch [28/30], Train Loss: 0.3882, Train Accuracy: 86.56%, Val Loss: 0.4920, Val Accuracy: 83.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [29/30]: 100%|██████████| 313/313 [00:42<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved with accuracy: 83.39%\n",
            "Epoch [29/30], Train Loss: 0.3914, Train Accuracy: 86.47%, Val Loss: 0.4913, Val Accuracy: 83.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Teacher Model Training Epoch [30/30]: 100%|██████████| 313/313 [00:42<00:00,  7.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Train Loss: 0.3914, Train Accuracy: 86.41%, Val Loss: 0.5053, Val Accuracy: 82.83%\n",
            "Teacher training completed in 26.09 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the best model for testing\n",
        "# teacher_model.load_state_dict(torch.load('teacher_model_best_pruned.pth'))\n",
        "\n",
        "# Testing phase\n",
        "teacher_model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = teacher_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f'Accuracy of the teacher model on the test set: {test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu7TLpf-YQgG",
        "outputId": "e8047372-a2e1-400d-eba6-464a86e104b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the teacher model on the test set: 86.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "from ptflops import get_model_complexity_info\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import codecarbon\n",
        "\n",
        "# Initialize CodeCarbon tracker\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define dataset transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Load ResNet-50 model and modify for CIFAR-10\n",
        "teacher_model = models.resnet50(pretrained=True)\n",
        "teacher_model.fc = nn.Linear(2048, 10)  # CIFAR-10 has 10 classes\n",
        "teacher_model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Function to apply L1 unstructured pruning\n",
        "def apply_pruning(model, amount=0.85):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "\n",
        "# Apply pruning\n",
        "apply_pruning(teacher_model, amount=0.85)\n",
        "\n",
        "# Function to remove pruning and make it permanent\n",
        "def remove_pruning(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'weight_mask'):\n",
        "                prune.remove(module, 'weight')\n",
        "    return model\n",
        "\n",
        "# Measure FLOPs\n",
        "def calculate_flops(model):\n",
        "    model.eval()\n",
        "    flops, params = get_model_complexity_info(model, (3, 32, 32), as_strings=False, print_per_layer_stat=False)\n",
        "    return flops, params\n",
        "\n",
        "# Count non-zero parameters\n",
        "def count_non_zero_params(model):\n",
        "    non_zero_count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            non_zero_count += torch.count_nonzero(param).item()\n",
        "    return non_zero_count\n",
        "\n",
        "# Measure inference time\n",
        "def measure_inference_time(model, dataloader):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = images.to(device)\n",
        "            _ = model(images)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / len(dataloader)\n",
        "\n",
        "# Training loop\n",
        "num_epochs_teacher = 10\n",
        "best_val_accuracy = 0.0\n",
        "start_time_teacher = time.time()\n",
        "\n",
        "for epoch in range(num_epochs_teacher):\n",
        "    # Training phase\n",
        "    teacher_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs_teacher} - Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = teacher_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation phase\n",
        "    teacher_model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = teacher_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Save the best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(teacher_model.state_dict(), 'resnet50_pruned_best.pth')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs_teacher}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "end_time_teacher = time.time()\n",
        "training_time = (end_time_teacher - start_time_teacher) / 60\n",
        "\n",
        "# Remove pruning and make it permanent\n",
        "teacher_model = remove_pruning(teacher_model)\n",
        "\n",
        "# Calculate metrics after pruning\n",
        "flops, params = calculate_flops(teacher_model)\n",
        "param_count = count_non_zero_params(teacher_model)\n",
        "inference_time = measure_inference_time(teacher_model, test_loader)\n",
        "\n",
        "# Stop CodeCarbon tracker\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Save the pruned model\n",
        "torch.save(teacher_model.state_dict(), 'resnet50_pruned_final.pth')\n",
        "model_size = os.path.getsize('resnet50_pruned_final.pth') / (1024 * 1024)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\n--- Model Analysis After Pruning ---\")\n",
        "print(f\"Non-zero Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size:.2f} MB\")\n",
        "print(f\"Training Time: {training_time:.2f} minutes\")\n",
        "print(f\"Inference Time per Image: {inference_time:.6f} seconds\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"CO2 Emissions: {emissions:.6f} kg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ionJJYzeeWLS",
        "outputId": "d0574945-0abe-48dc-887c-cbf47a004b8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 23:36:17] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 23:36:17] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 23:36:17] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 23:36:17] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 23:36:17] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 23:36:18] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 23:36:18] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 23:36:18] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 23:36:18]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 23:36:18]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 23:36:18]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 23:36:18]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 23:36:18]   CPU count: 2\n",
            "[codecarbon INFO @ 23:36:18]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 23:36:18]   GPU count: 1\n",
            "[codecarbon INFO @ 23:36:18]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 23:36:18] Saving emissions data to file /content/emissions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/10 - Training:  29%|██▉       | 182/625 [00:12<00:27, 15.97it/s][codecarbon INFO @ 23:36:33] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:33] Energy consumed for all GPUs : 0.000260 kWh. Total GPU Power : 62.377670194014826 W\n",
            "[codecarbon INFO @ 23:36:33] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:33] 0.000457 kWh of electricity used since the beginning.\n",
            "Epoch 1/10 - Training:  65%|██████▍   | 404/625 [00:27<00:14, 14.78it/s][codecarbon INFO @ 23:36:48] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:36:48] Energy consumed for all GPUs : 0.000546 kWh. Total GPU Power : 68.6398104945438 W\n",
            "[codecarbon INFO @ 23:36:48] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:36:48] 0.000940 kWh of electricity used since the beginning.\n",
            "Epoch 1/10 - Training: 100%|██████████| 625/625 [00:42<00:00, 14.73it/s]\n",
            "[codecarbon INFO @ 23:37:03] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:03] Energy consumed for all GPUs : 0.000829 kWh. Total GPU Power : 67.80380469049776 W\n",
            "[codecarbon INFO @ 23:37:03] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:03] 0.001420 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.9772, Train Accuracy: 66.13%, Val Loss: 0.6473, Val Accuracy: 78.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Training:  23%|██▎       | 145/625 [00:09<00:34, 13.77it/s][codecarbon INFO @ 23:37:18] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:18] Energy consumed for all GPUs : 0.001093 kWh. Total GPU Power : 63.544426387382124 W\n",
            "[codecarbon INFO @ 23:37:18] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:18] 0.001881 kWh of electricity used since the beginning.\n",
            "Epoch 2/10 - Training:  59%|█████▉    | 371/625 [00:24<00:19, 13.19it/s][codecarbon INFO @ 23:37:33] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:33] Energy consumed for all GPUs : 0.001378 kWh. Total GPU Power : 68.39738994311855 W\n",
            "[codecarbon INFO @ 23:37:33] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:33] 0.002363 kWh of electricity used since the beginning.\n",
            "Epoch 2/10 - Training:  96%|█████████▌| 599/625 [00:39<00:01, 15.33it/s][codecarbon INFO @ 23:37:48] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:37:48] Energy consumed for all GPUs : 0.001665 kWh. Total GPU Power : 68.84076331659935 W\n",
            "[codecarbon INFO @ 23:37:48] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:37:48] 0.002846 kWh of electricity used since the beginning.\n",
            "Epoch 2/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Train Loss: 0.5814, Train Accuracy: 80.09%, Val Loss: 0.5817, Val Accuracy: 80.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 - Training:  21%|██        | 131/625 [00:09<00:30, 16.07it/s][codecarbon INFO @ 23:38:03] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:03] Energy consumed for all GPUs : 0.001944 kWh. Total GPU Power : 67.04870023055763 W\n",
            "[codecarbon INFO @ 23:38:03] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:03] 0.003322 kWh of electricity used since the beginning.\n",
            "Epoch 3/10 - Training:  57%|█████▋    | 359/625 [00:24<00:16, 16.09it/s][codecarbon INFO @ 23:38:18] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:18] Energy consumed for all GPUs : 0.002230 kWh. Total GPU Power : 68.71343435083594 W\n",
            "[codecarbon INFO @ 23:38:18] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:18] 0.003806 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:38:18] 0.004397 g.CO2eq/s mean an estimation of 138.6697132082111 kg.CO2eq/year\n",
            "Epoch 3/10 - Training:  94%|█████████▍| 587/625 [00:39<00:02, 16.02it/s][codecarbon INFO @ 23:38:33] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:33] Energy consumed for all GPUs : 0.002515 kWh. Total GPU Power : 68.38555363300166 W\n",
            "[codecarbon INFO @ 23:38:33] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:33] 0.004287 kWh of electricity used since the beginning.\n",
            "Epoch 3/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Train Loss: 0.4476, Train Accuracy: 84.65%, Val Loss: 0.5686, Val Accuracy: 80.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 - Training:  19%|█▊        | 117/625 [00:07<00:31, 16.20it/s][codecarbon INFO @ 23:38:48] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:38:48] Energy consumed for all GPUs : 0.002787 kWh. Total GPU Power : 65.39924490701816 W\n",
            "[codecarbon INFO @ 23:38:48] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:38:48] 0.004756 kWh of electricity used since the beginning.\n",
            "Epoch 4/10 - Training:  55%|█████▌    | 345/625 [00:22<00:17, 16.30it/s][codecarbon INFO @ 23:39:03] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:03] Energy consumed for all GPUs : 0.003073 kWh. Total GPU Power : 68.61285237039878 W\n",
            "[codecarbon INFO @ 23:39:03] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:03] 0.005239 kWh of electricity used since the beginning.\n",
            "Epoch 4/10 - Training:  92%|█████████▏| 577/625 [00:37<00:02, 16.14it/s][codecarbon INFO @ 23:39:18] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:18] Energy consumed for all GPUs : 0.003361 kWh. Total GPU Power : 69.28493183441682 W\n",
            "[codecarbon INFO @ 23:39:18] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:18] 0.005724 kWh of electricity used since the beginning.\n",
            "Epoch 4/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Train Loss: 0.3665, Train Accuracy: 87.46%, Val Loss: 0.6034, Val Accuracy: 79.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 - Training:  18%|█▊        | 113/625 [00:07<00:39, 12.90it/s][codecarbon INFO @ 23:39:33] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:33] Energy consumed for all GPUs : 0.003639 kWh. Total GPU Power : 66.56555318501088 W\n",
            "[codecarbon INFO @ 23:39:33] Energy consumed for all CPUs : 0.002302 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:33] 0.006198 kWh of electricity used since the beginning.\n",
            "Epoch 5/10 - Training:  54%|█████▍    | 339/625 [00:22<00:22, 12.72it/s][codecarbon INFO @ 23:39:48] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:39:48] Energy consumed for all GPUs : 0.003923 kWh. Total GPU Power : 68.27647078176503 W\n",
            "[codecarbon INFO @ 23:39:48] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:39:48] 0.006680 kWh of electricity used since the beginning.\n",
            "Epoch 5/10 - Training:  90%|█████████ | 563/625 [00:37<00:04, 13.38it/s][codecarbon INFO @ 23:40:03] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:03] Energy consumed for all GPUs : 0.004208 kWh. Total GPU Power : 68.27815935012246 W\n",
            "[codecarbon INFO @ 23:40:04] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:04] 0.007161 kWh of electricity used since the beginning.\n",
            "Epoch 5/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Train Loss: 0.3010, Train Accuracy: 89.51%, Val Loss: 0.5612, Val Accuracy: 81.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 - Training:  15%|█▍        | 91/625 [00:06<00:33, 15.90it/s][codecarbon INFO @ 23:40:18] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:19] Energy consumed for all GPUs : 0.004484 kWh. Total GPU Power : 66.32363247129513 W\n",
            "[codecarbon INFO @ 23:40:19] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:19] 0.007634 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:40:19] 0.004425 g.CO2eq/s mean an estimation of 139.55251484520227 kg.CO2eq/year\n",
            "Epoch 6/10 - Training:  51%|█████     | 319/625 [00:21<00:19, 15.86it/s][codecarbon INFO @ 23:40:33] Energy consumed for RAM : 0.000336 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:34] Energy consumed for all GPUs : 0.004770 kWh. Total GPU Power : 68.71185583309664 W\n",
            "[codecarbon INFO @ 23:40:34] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:34] 0.008117 kWh of electricity used since the beginning.\n",
            "Epoch 6/10 - Training:  88%|████████▊ | 549/625 [00:36<00:04, 15.93it/s][codecarbon INFO @ 23:40:48] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:40:49] Energy consumed for all GPUs : 0.005056 kWh. Total GPU Power : 68.7822871891103 W\n",
            "[codecarbon INFO @ 23:40:49] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:40:49] 0.008600 kWh of electricity used since the beginning.\n",
            "Epoch 6/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Train Loss: 0.2551, Train Accuracy: 91.33%, Val Loss: 0.5737, Val Accuracy: 82.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 - Training:  11%|█         | 69/625 [00:04<00:36, 15.24it/s][codecarbon INFO @ 23:41:04] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:04] Energy consumed for all GPUs : 0.005321 kWh. Total GPU Power : 63.557338717164804 W\n",
            "[codecarbon INFO @ 23:41:04] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:04] 0.009062 kWh of electricity used since the beginning.\n",
            "Epoch 7/10 - Training:  48%|████▊     | 299/625 [00:19<00:20, 16.05it/s][codecarbon INFO @ 23:41:19] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:19] Energy consumed for all GPUs : 0.005606 kWh. Total GPU Power : 68.56824281190634 W\n",
            "[codecarbon INFO @ 23:41:19] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:19] 0.009544 kWh of electricity used since the beginning.\n",
            "Epoch 7/10 - Training:  84%|████████▍ | 527/625 [00:34<00:06, 15.41it/s][codecarbon INFO @ 23:41:34] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:34] Energy consumed for all GPUs : 0.005893 kWh. Total GPU Power : 68.82200471702521 W\n",
            "[codecarbon INFO @ 23:41:34] Energy consumed for all CPUs : 0.003719 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:34] 0.010027 kWh of electricity used since the beginning.\n",
            "Epoch 7/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Train Loss: 0.2154, Train Accuracy: 92.43%, Val Loss: 0.6045, Val Accuracy: 81.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 - Training:  11%|█         | 67/625 [00:04<00:35, 15.70it/s][codecarbon INFO @ 23:41:49] Energy consumed for RAM : 0.000435 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:41:49] Energy consumed for all GPUs : 0.006171 kWh. Total GPU Power : 66.87695779403104 W\n",
            "[codecarbon INFO @ 23:41:49] Energy consumed for all CPUs : 0.003896 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:41:49] 0.010503 kWh of electricity used since the beginning.\n",
            "Epoch 8/10 - Training:  48%|████▊     | 297/625 [00:19<00:21, 15.47it/s][codecarbon INFO @ 23:42:04] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:04] Energy consumed for all GPUs : 0.006457 kWh. Total GPU Power : 68.64970165831767 W\n",
            "[codecarbon INFO @ 23:42:04] Energy consumed for all CPUs : 0.004073 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:04] 0.010986 kWh of electricity used since the beginning.\n",
            "Epoch 8/10 - Training:  83%|████████▎ | 521/625 [00:34<00:07, 13.86it/s][codecarbon INFO @ 23:42:19] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:19] Energy consumed for all GPUs : 0.006744 kWh. Total GPU Power : 68.74822240244795 W\n",
            "[codecarbon INFO @ 23:42:19] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:19] 0.011469 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:42:19] 0.004432 g.CO2eq/s mean an estimation of 139.7778176554655 kg.CO2eq/year\n",
            "Epoch 8/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Train Loss: 0.1938, Train Accuracy: 93.18%, Val Loss: 0.6370, Val Accuracy: 81.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 - Training:   9%|▉         | 57/625 [00:04<00:43, 12.95it/s][codecarbon INFO @ 23:42:34] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:34] Energy consumed for all GPUs : 0.007022 kWh. Total GPU Power : 66.86350645628731 W\n",
            "[codecarbon INFO @ 23:42:34] Energy consumed for all CPUs : 0.004428 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:34] 0.011944 kWh of electricity used since the beginning.\n",
            "Epoch 9/10 - Training:  45%|████▌     | 283/625 [00:19<00:23, 14.45it/s][codecarbon INFO @ 23:42:49] Energy consumed for RAM : 0.000514 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:42:49] Energy consumed for all GPUs : 0.007307 kWh. Total GPU Power : 68.51878437119873 W\n",
            "[codecarbon INFO @ 23:42:49] Energy consumed for all CPUs : 0.004604 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:42:49] 0.012426 kWh of electricity used since the beginning.\n",
            "Epoch 9/10 - Training:  82%|████████▏ | 513/625 [00:34<00:06, 16.18it/s][codecarbon INFO @ 23:43:04] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:04] Energy consumed for all GPUs : 0.007593 kWh. Total GPU Power : 68.70788967445333 W\n",
            "[codecarbon INFO @ 23:43:04] Energy consumed for all CPUs : 0.004782 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:04] 0.012909 kWh of electricity used since the beginning.\n",
            "Epoch 9/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 15.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Train Loss: 0.1735, Train Accuracy: 93.87%, Val Loss: 0.6168, Val Accuracy: 82.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 - Training:   7%|▋         | 41/625 [00:02<00:35, 16.24it/s][codecarbon INFO @ 23:43:19] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:19] Energy consumed for all GPUs : 0.007860 kWh. Total GPU Power : 64.16542672963475 W\n",
            "[codecarbon INFO @ 23:43:19] Energy consumed for all CPUs : 0.004959 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:19] 0.013373 kWh of electricity used since the beginning.\n",
            "Epoch 10/10 - Training:  43%|████▎     | 271/625 [00:17<00:22, 15.54it/s][codecarbon INFO @ 23:43:34] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:34] Energy consumed for all GPUs : 0.008146 kWh. Total GPU Power : 68.83379672054959 W\n",
            "[codecarbon INFO @ 23:43:34] Energy consumed for all CPUs : 0.005136 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:34] 0.013856 kWh of electricity used since the beginning.\n",
            "Epoch 10/10 - Training:  79%|███████▉  | 495/625 [00:32<00:08, 15.86it/s][codecarbon INFO @ 23:43:49] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:43:49] Energy consumed for all GPUs : 0.008432 kWh. Total GPU Power : 68.47853913905173 W\n",
            "[codecarbon INFO @ 23:43:49] Energy consumed for all CPUs : 0.005313 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:43:49] 0.014338 kWh of electricity used since the beginning.\n",
            "Epoch 10/10 - Training: 100%|██████████| 625/625 [00:41<00:00, 14.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Train Loss: 0.1609, Train Accuracy: 94.25%, Val Loss: 0.6651, Val Accuracy: 81.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 23:44:04] Energy consumed for RAM : 0.000613 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:04] Energy consumed for all GPUs : 0.008710 kWh. Total GPU Power : 66.70825356055292 W\n",
            "[codecarbon INFO @ 23:44:04] Energy consumed for all CPUs : 0.005490 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:04] 0.014813 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:44:05] Energy consumed for RAM : 0.000615 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 23:44:05] Energy consumed for all GPUs : 0.008734 kWh. Total GPU Power : 64.46371701192484 W\n",
            "[codecarbon INFO @ 23:44:05] Energy consumed for all CPUs : 0.005506 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 23:44:05] 0.014855 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 23:44:05] 0.004416 g.CO2eq/s mean an estimation of 139.2680192326752 kg.CO2eq/year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Analysis After Pruning ---\n",
            "Non-zero Parameter Count: 3574441\n",
            "Model Size: 90.06 MB\n",
            "Training Time: 7.67 minutes\n",
            "Inference Time per Image: 0.022866 seconds\n",
            "FLOPs: 0.08 GFLOPs\n",
            "CO2 Emissions: 0.002061 kg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbdhTsabe-s0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}