{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iQIzSyG6zgBq",
        "outputId": "d0a43db2-ddc9-4a18-ed7f-c75db539416a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.7.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Collecting pynvml (from codecarbon)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Collecting prompt_toolkit<=3.0.36,>=2.0 (from questionary->codecarbon)\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.7.4-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.2/504.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.0.1-py3-none-any.whl (34 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, pynvml, prompt_toolkit, yaspin, questionary, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: prompt_toolkit\n",
            "    Found existing installation: prompt_toolkit 3.0.48\n",
            "    Uninstalling prompt_toolkit-3.0.48:\n",
            "      Successfully uninstalled prompt_toolkit-3.0.48\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.7.4 fief-client-0.20.0 jwcrypto-1.5.6 prompt_toolkit-3.0.36 pynvml-11.5.3 questionary-2.0.1 rapidfuzz-3.10.1 termcolor-2.3.0 types-python-dateutil-2.9.0.20241003 yaspin-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              },
              "id": "568c300e90c347958e5256a5348c3cbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install codecarbon\n",
        "!pip install thop\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import codecarbon\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define dataset transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Load the pre-trained teacher model (VGG-16)\n",
        "teacher_model = models.vgg16(pretrained=False)\n",
        "teacher_model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 10)\n",
        ")\n",
        "teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\", map_location=device))\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# Define the student model (MobileNetV2)\n",
        "student_model = models.mobilenet_v2(pretrained=True)\n",
        "student_model.classifier[1] = nn.Sequential(\n",
        "    nn.Linear(student_model.last_channel, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 10)\n",
        ")\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "# Define Dynamic Distillation Loss\n",
        "class DynamicDistillationLoss(nn.Module):\n",
        "    def __init__(self, initial_T=5.0, final_T=2.0, alpha=0.7, total_epochs=20):\n",
        "        super(DynamicDistillationLoss, self).__init__()\n",
        "        self.initial_T = initial_T\n",
        "        self.final_T = final_T\n",
        "        self.alpha = alpha\n",
        "        self.total_epochs = total_epochs\n",
        "        self.criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels, current_epoch):\n",
        "        T = self.initial_T - (self.initial_T - self.final_T) * (current_epoch / self.total_epochs)\n",
        "        distillation_loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "            torch.log_softmax(student_logits / T, dim=1),\n",
        "            torch.softmax(teacher_logits / T, dim=1)\n",
        "        ) * (T ** 2)\n",
        "        student_loss = self.criterion_ce(student_logits, labels)\n",
        "        return self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n",
        "\n",
        "criterion = DynamicDistillationLoss(initial_T=5.0, final_T=2.0, alpha=0.7, total_epochs=20)\n",
        "optimizer = optim.AdamW(student_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# Initialize CodeCarbon tracker\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "start_time = time.time()\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "        student_outputs = student_model(inputs)\n",
        "        loss = criterion(student_outputs, teacher_outputs, labels, current_epoch=epoch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(student_outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# Stop CodeCarbon tracker\n",
        "emissions = tracker.stop()\n",
        "end_time = time.time()\n",
        "training_time = (end_time - start_time) / 60  # Convert to minutes\n",
        "\n",
        "# Model Analysis\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp_model.pth\")\n",
        "    size_mb = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)\n",
        "    os.remove(\"temp_model.pth\")\n",
        "    return size_mb\n",
        "\n",
        "def measure_inference_time(model, dataloader):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            _ = model(inputs)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / len(dataloader)\n",
        "\n",
        "# Calculate FLOPs\n",
        "flops, _ = get_model_complexity_info(student_model, (3, 32, 32), as_strings=False, print_per_layer_stat=False)\n",
        "param_count = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "model_size = get_model_size(student_model)\n",
        "inference_time = measure_inference_time(student_model, test_loader)\n",
        "\n",
        "# Print model analysis\n",
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size:.2f} MB\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"Training Time: {training_time:.2f} minutes\")\n",
        "print(f\"Average Inference Time: {inference_time:.6f} seconds\")\n",
        "print(\"\\n--- Energy and Emissions Report ---\")\n",
        "print(f\"CO2 Emissions: {emissions:.6f} kg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9zNyrNs0ASh",
        "outputId": "bfc49a4b-52db-4590-b670-4c6a3d955900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-4-34310d872b55>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\", map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "[codecarbon INFO @ 10:41:42] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 10:41:42] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 10:41:42] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 10:41:42] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 10:41:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 10:41:43] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 10:41:43] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 10:41:43] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 10:41:43]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 10:41:43]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 10:41:43]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 10:41:43]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 10:41:43]   CPU count: 2\n",
            "[codecarbon INFO @ 10:41:43]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 10:41:43]   GPU count: 1\n",
            "[codecarbon INFO @ 10:41:43]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 10:41:43] Saving emissions data to file /content/emissions.csv\n",
            "Epoch 1/20 - Training:   0%|          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training:  27%|██▋       | 170/625 [00:14<00:35, 12.99it/s][codecarbon INFO @ 10:41:58] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:41:58] Energy consumed for all GPUs : 0.000169 kWh. Total GPU Power : 40.405401285013866 W\n",
            "[codecarbon INFO @ 10:41:58] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:41:58] 0.000366 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training:  56%|█████▌    | 350/625 [00:29<00:22, 11.97it/s][codecarbon INFO @ 10:42:13] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:42:13] Energy consumed for all GPUs : 0.000354 kWh. Total GPU Power : 44.39611042962135 W\n",
            "[codecarbon INFO @ 10:42:13] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:42:13] 0.000748 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training:  85%|████████▍ | 531/625 [00:45<00:07, 13.19it/s][codecarbon INFO @ 10:42:28] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:42:28] Energy consumed for all GPUs : 0.000532 kWh. Total GPU Power : 42.78823707008795 W\n",
            "[codecarbon INFO @ 10:42:28] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:42:28] 0.001124 kWh of electricity used since the beginning.\n",
            "Epoch 1/20 - Training: 100%|██████████| 625/625 [00:51<00:00, 12.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 5.9725, Training Accuracy: 55.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 - Training:  13%|█▎        | 82/625 [00:08<00:43, 12.41it/s][codecarbon INFO @ 10:42:43] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:42:43] Energy consumed for all GPUs : 0.000728 kWh. Total GPU Power : 47.001471643617414 W\n",
            "[codecarbon INFO @ 10:42:43] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:42:43] 0.001517 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  42%|████▏     | 260/625 [00:23<00:28, 12.62it/s][codecarbon INFO @ 10:42:58] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:42:58] Energy consumed for all GPUs : 0.000914 kWh. Total GPU Power : 44.618649439633806 W\n",
            "[codecarbon INFO @ 10:42:58] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:42:58] 0.001899 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  70%|███████   | 438/625 [00:38<00:13, 13.72it/s][codecarbon INFO @ 10:43:13] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:43:13] Energy consumed for all GPUs : 0.001104 kWh. Total GPU Power : 45.51179691507609 W\n",
            "[codecarbon INFO @ 10:43:13] Energy consumed for all CPUs : 0.001064 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:43:13] 0.002286 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training:  98%|█████████▊| 611/625 [00:53<00:01, 12.03it/s][codecarbon INFO @ 10:43:28] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:43:29] Energy consumed for all GPUs : 0.001292 kWh. Total GPU Power : 45.2023426856341 W\n",
            "[codecarbon INFO @ 10:43:29] Energy consumed for all CPUs : 0.001241 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:43:29] 0.002671 kWh of electricity used since the beginning.\n",
            "Epoch 2/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 4.2548, Training Accuracy: 66.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 - Training:  22%|██▏       | 138/625 [00:14<00:38, 12.63it/s][codecarbon INFO @ 10:43:44] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:43:44] Energy consumed for all GPUs : 0.001467 kWh. Total GPU Power : 41.95604948584178 W\n",
            "[codecarbon INFO @ 10:43:44] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:43:44] 0.003043 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:43:44] 0.011922 g.CO2eq/s mean an estimation of 375.982121169834 kg.CO2eq/year\n",
            "Epoch 3/20 - Training:  49%|████▊     | 304/625 [00:29<00:23, 13.49it/s][codecarbon INFO @ 10:43:59] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:43:59] Energy consumed for all GPUs : 0.001647 kWh. Total GPU Power : 43.32306414561045 W\n",
            "[codecarbon INFO @ 10:43:59] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:43:59] 0.003420 kWh of electricity used since the beginning.\n",
            "Epoch 3/20 - Training:  77%|███████▋  | 482/625 [00:44<00:11, 12.76it/s][codecarbon INFO @ 10:44:14] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:44:14] Energy consumed for all GPUs : 0.001828 kWh. Total GPU Power : 43.51574315329609 W\n",
            "[codecarbon INFO @ 10:44:14] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:44:14] 0.003798 kWh of electricity used since the beginning.\n",
            "Epoch 3/20 - Training: 100%|██████████| 625/625 [00:56<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 3.6541, Training Accuracy: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 - Training:   5%|▍         | 31/625 [00:02<00:45, 13.20it/s][codecarbon INFO @ 10:44:29] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:44:29] Energy consumed for all GPUs : 0.002022 kWh. Total GPU Power : 46.52830081887072 W\n",
            "[codecarbon INFO @ 10:44:29] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:44:29] 0.004189 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  34%|███▍      | 213/625 [00:17<00:30, 13.39it/s][codecarbon INFO @ 10:44:44] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:44:44] Energy consumed for all GPUs : 0.002221 kWh. Total GPU Power : 47.787433080773326 W\n",
            "[codecarbon INFO @ 10:44:44] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:44:44] 0.004585 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  63%|██████▎   | 392/625 [00:32<00:16, 14.43it/s][codecarbon INFO @ 10:44:59] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:44:59] Energy consumed for all GPUs : 0.002418 kWh. Total GPU Power : 47.149055935732164 W\n",
            "[codecarbon INFO @ 10:44:59] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:44:59] 0.004978 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training:  92%|█████████▏| 572/625 [00:47<00:04, 12.87it/s][codecarbon INFO @ 10:45:14] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:45:14] Energy consumed for all GPUs : 0.002610 kWh. Total GPU Power : 46.036687158885414 W\n",
            "[codecarbon INFO @ 10:45:14] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:45:14] 0.005367 kWh of electricity used since the beginning.\n",
            "Epoch 4/20 - Training: 100%|██████████| 625/625 [00:52<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 3.2867, Training Accuracy: 72.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 - Training:  21%|██        | 129/625 [00:10<00:34, 14.35it/s][codecarbon INFO @ 10:45:29] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:45:29] Energy consumed for all GPUs : 0.002819 kWh. Total GPU Power : 50.20308632871738 W\n",
            "[codecarbon INFO @ 10:45:29] Energy consumed for all CPUs : 0.002658 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:45:29] 0.005774 kWh of electricity used since the beginning.\n",
            "Epoch 5/20 - Training:  49%|████▉     | 307/625 [00:24<00:27, 11.43it/s][codecarbon INFO @ 10:45:44] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:45:44] Energy consumed for all GPUs : 0.003020 kWh. Total GPU Power : 48.26470915391257 W\n",
            "[codecarbon INFO @ 10:45:44] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:45:44] 0.006171 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:45:44] 0.012264 g.CO2eq/s mean an estimation of 386.74650289082666 kg.CO2eq/year\n",
            "Epoch 5/20 - Training:  78%|███████▊  | 488/625 [00:40<00:14,  9.73it/s][codecarbon INFO @ 10:45:59] Energy consumed for RAM : 0.000337 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:45:59] Energy consumed for all GPUs : 0.003221 kWh. Total GPU Power : 48.37561385585447 W\n",
            "[codecarbon INFO @ 10:45:59] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:45:59] 0.006570 kWh of electricity used since the beginning.\n",
            "Epoch 5/20 - Training: 100%|██████████| 625/625 [00:51<00:00, 12.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 2.8963, Training Accuracy: 74.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 - Training:   6%|▌         | 38/625 [00:03<01:06,  8.80it/s][codecarbon INFO @ 10:46:14] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:46:14] Energy consumed for all GPUs : 0.003427 kWh. Total GPU Power : 49.364848611371784 W\n",
            "[codecarbon INFO @ 10:46:14] Energy consumed for all CPUs : 0.003190 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:46:14] 0.006973 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  34%|███▍      | 213/625 [00:18<00:46,  8.78it/s][codecarbon INFO @ 10:46:29] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:46:29] Energy consumed for all GPUs : 0.003619 kWh. Total GPU Power : 46.402496981151806 W\n",
            "[codecarbon INFO @ 10:46:29] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:46:29] 0.007362 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  62%|██████▏   | 389/625 [00:33<00:27,  8.46it/s][codecarbon INFO @ 10:46:44] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:46:44] Energy consumed for all GPUs : 0.003821 kWh. Total GPU Power : 48.236169038689106 W\n",
            "[codecarbon INFO @ 10:46:44] Energy consumed for all CPUs : 0.003544 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:46:44] 0.007760 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training:  90%|█████████ | 565/625 [00:48<00:07,  7.79it/s][codecarbon INFO @ 10:46:59] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:46:59] Energy consumed for all GPUs : 0.004021 kWh. Total GPU Power : 48.10235363449455 W\n",
            "[codecarbon INFO @ 10:46:59] Energy consumed for all CPUs : 0.003721 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:46:59] 0.008158 kWh of electricity used since the beginning.\n",
            "Epoch 6/20 - Training: 100%|██████████| 625/625 [00:53<00:00, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 2.6296, Training Accuracy: 75.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 - Training:  18%|█▊        | 114/625 [00:10<01:05,  7.82it/s][codecarbon INFO @ 10:47:14] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:47:14] Energy consumed for all GPUs : 0.004230 kWh. Total GPU Power : 50.03618055190949 W\n",
            "[codecarbon INFO @ 10:47:14] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:47:14] 0.008564 kWh of electricity used since the beginning.\n",
            "Epoch 7/20 - Training:  46%|████▋     | 290/625 [00:25<00:46,  7.24it/s][codecarbon INFO @ 10:47:29] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:47:29] Energy consumed for all GPUs : 0.004426 kWh. Total GPU Power : 47.22859031991993 W\n",
            "[codecarbon INFO @ 10:47:29] Energy consumed for all CPUs : 0.004076 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:47:29] 0.008957 kWh of electricity used since the beginning.\n",
            "Epoch 7/20 - Training:  75%|███████▍  | 466/625 [00:40<00:13, 11.53it/s][codecarbon INFO @ 10:47:44] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:47:44] Energy consumed for all GPUs : 0.004630 kWh. Total GPU Power : 48.83569725872046 W\n",
            "[codecarbon INFO @ 10:47:44] Energy consumed for all CPUs : 0.004253 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:47:44] 0.009357 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:47:44] 0.012485 g.CO2eq/s mean an estimation of 393.7151541248415 kg.CO2eq/year\n",
            "Epoch 7/20 - Training: 100%|██████████| 625/625 [00:53<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 2.3556, Training Accuracy: 77.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 - Training:   3%|▎         | 21/625 [00:02<00:51, 11.79it/s][codecarbon INFO @ 10:47:59] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:47:59] Energy consumed for all GPUs : 0.004839 kWh. Total GPU Power : 50.37622220534473 W\n",
            "[codecarbon INFO @ 10:47:59] Energy consumed for all CPUs : 0.004430 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:47:59] 0.009764 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  32%|███▏      | 198/625 [00:17<00:31, 13.51it/s][codecarbon INFO @ 10:48:14] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:48:14] Energy consumed for all GPUs : 0.005042 kWh. Total GPU Power : 48.689703766853775 W\n",
            "[codecarbon INFO @ 10:48:14] Energy consumed for all CPUs : 0.004607 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:48:14] 0.010163 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  60%|█████▉    | 372/625 [00:32<00:18, 13.96it/s][codecarbon INFO @ 10:48:29] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:48:29] Energy consumed for all GPUs : 0.005242 kWh. Total GPU Power : 48.007870200974764 W\n",
            "[codecarbon INFO @ 10:48:29] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:48:29] 0.010560 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training:  88%|████████▊ | 550/625 [00:47<00:05, 14.28it/s][codecarbon INFO @ 10:48:44] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:48:44] Energy consumed for all GPUs : 0.005445 kWh. Total GPU Power : 48.65531104347383 W\n",
            "[codecarbon INFO @ 10:48:44] Energy consumed for all CPUs : 0.004961 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:48:44] 0.010960 kWh of electricity used since the beginning.\n",
            "Epoch 8/20 - Training: 100%|██████████| 625/625 [00:52<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 2.1295, Training Accuracy: 78.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 - Training:  15%|█▌        | 94/625 [00:09<00:42, 12.43it/s][codecarbon INFO @ 10:48:59] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:48:59] Energy consumed for all GPUs : 0.005644 kWh. Total GPU Power : 47.69577009593921 W\n",
            "[codecarbon INFO @ 10:48:59] Energy consumed for all CPUs : 0.005138 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:48:59] 0.011356 kWh of electricity used since the beginning.\n",
            "Epoch 9/20 - Training:  44%|████▎     | 273/625 [00:24<00:25, 13.74it/s][codecarbon INFO @ 10:49:14] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:49:14] Energy consumed for all GPUs : 0.005845 kWh. Total GPU Power : 48.226931171561915 W\n",
            "[codecarbon INFO @ 10:49:14] Energy consumed for all CPUs : 0.005315 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:49:14] 0.011754 kWh of electricity used since the beginning.\n",
            "Epoch 9/20 - Training:  72%|███████▏  | 450/625 [00:39<00:12, 13.84it/s][codecarbon INFO @ 10:49:29] Energy consumed for RAM : 0.000614 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:49:29] Energy consumed for all GPUs : 0.006040 kWh. Total GPU Power : 46.920449466691466 W\n",
            "[codecarbon INFO @ 10:49:29] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:49:29] 0.012146 kWh of electricity used since the beginning.\n",
            "Epoch 9/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 1.9663, Training Accuracy: 79.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 - Training:   0%|          | 2/625 [00:00<02:40,  3.88it/s][codecarbon INFO @ 10:49:44] Energy consumed for RAM : 0.000633 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:49:44] Energy consumed for all GPUs : 0.006248 kWh. Total GPU Power : 49.85717922978543 W\n",
            "[codecarbon INFO @ 10:49:44] Energy consumed for all CPUs : 0.005670 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:49:44] 0.012551 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:49:44] 0.012520 g.CO2eq/s mean an estimation of 394.82700561855495 kg.CO2eq/year\n",
            "Epoch 10/20 - Training:  28%|██▊       | 175/625 [00:15<00:33, 13.34it/s][codecarbon INFO @ 10:49:59] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:49:59] Energy consumed for all GPUs : 0.006448 kWh. Total GPU Power : 47.99216118939571 W\n",
            "[codecarbon INFO @ 10:49:59] Energy consumed for all CPUs : 0.005847 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:49:59] 0.012948 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training:  56%|█████▋    | 352/625 [00:30<00:20, 13.25it/s][codecarbon INFO @ 10:50:14] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:50:14] Energy consumed for all GPUs : 0.006650 kWh. Total GPU Power : 48.40838898588697 W\n",
            "[codecarbon INFO @ 10:50:14] Energy consumed for all CPUs : 0.006024 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:50:14] 0.013347 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training:  85%|████████▍ | 530/625 [00:45<00:07, 13.44it/s][codecarbon INFO @ 10:50:29] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:50:29] Energy consumed for all GPUs : 0.006852 kWh. Total GPU Power : 48.67690897595046 W\n",
            "[codecarbon INFO @ 10:50:29] Energy consumed for all CPUs : 0.006201 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:50:29] 0.013746 kWh of electricity used since the beginning.\n",
            "Epoch 10/20 - Training: 100%|██████████| 625/625 [00:53<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 1.8669, Training Accuracy: 79.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 - Training:  13%|█▎        | 84/625 [00:06<00:38, 14.01it/s][codecarbon INFO @ 10:50:44] Energy consumed for RAM : 0.000713 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:50:44] Energy consumed for all GPUs : 0.007062 kWh. Total GPU Power : 50.20475858361381 W\n",
            "[codecarbon INFO @ 10:50:44] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:50:44] 0.014152 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training:  42%|████▏     | 263/625 [00:22<00:26, 13.43it/s][codecarbon INFO @ 10:50:59] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:50:59] Energy consumed for all GPUs : 0.007263 kWh. Total GPU Power : 48.51234130057 W\n",
            "[codecarbon INFO @ 10:50:59] Energy consumed for all CPUs : 0.006555 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:50:59] 0.014551 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training:  71%|███████   | 441/625 [00:37<00:12, 14.53it/s][codecarbon INFO @ 10:51:14] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:51:14] Energy consumed for all GPUs : 0.007477 kWh. Total GPU Power : 51.36544480759493 W\n",
            "[codecarbon INFO @ 10:51:14] Energy consumed for all CPUs : 0.006732 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:51:14] 0.014962 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training: 100%|█████████▉| 623/625 [00:52<00:00, 20.05it/s][codecarbon INFO @ 10:51:29] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:51:29] Energy consumed for all GPUs : 0.007690 kWh. Total GPU Power : 50.956726647723826 W\n",
            "[codecarbon INFO @ 10:51:29] Energy consumed for all CPUs : 0.006909 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:51:29] 0.015371 kWh of electricity used since the beginning.\n",
            "Epoch 11/20 - Training: 100%|██████████| 625/625 [00:52<00:00, 11.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 1.7916, Training Accuracy: 80.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 - Training:  27%|██▋       | 171/625 [00:14<00:35, 12.65it/s][codecarbon INFO @ 10:51:44] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:51:44] Energy consumed for all GPUs : 0.007904 kWh. Total GPU Power : 51.62209931109979 W\n",
            "[codecarbon INFO @ 10:51:44] Energy consumed for all CPUs : 0.007086 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:51:44] 0.015782 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:51:44] 0.012667 g.CO2eq/s mean an estimation of 399.45717532250626 kg.CO2eq/year\n",
            "Epoch 12/20 - Training:  55%|█████▍    | 343/625 [00:29<00:21, 12.87it/s][codecarbon INFO @ 10:51:59] Energy consumed for RAM : 0.000811 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:51:59] Energy consumed for all GPUs : 0.008102 kWh. Total GPU Power : 47.598588309541114 W\n",
            "[codecarbon INFO @ 10:51:59] Energy consumed for all CPUs : 0.007263 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:51:59] 0.016177 kWh of electricity used since the beginning.\n",
            "Epoch 12/20 - Training:  83%|████████▎ | 519/625 [00:44<00:07, 13.61it/s][codecarbon INFO @ 10:52:14] Energy consumed for RAM : 0.000831 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:52:14] Energy consumed for all GPUs : 0.008306 kWh. Total GPU Power : 48.77685343029551 W\n",
            "[codecarbon INFO @ 10:52:14] Energy consumed for all CPUs : 0.007440 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:52:14] 0.016577 kWh of electricity used since the beginning.\n",
            "Epoch 12/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 1.7640, Training Accuracy: 79.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 - Training:   9%|▉         | 58/625 [00:05<00:43, 12.93it/s][codecarbon INFO @ 10:52:29] Energy consumed for RAM : 0.000851 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:52:29] Energy consumed for all GPUs : 0.008510 kWh. Total GPU Power : 49.04596588266318 W\n",
            "[codecarbon INFO @ 10:52:29] Energy consumed for all CPUs : 0.007618 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:52:29] 0.016979 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  37%|███▋      | 232/625 [00:20<00:28, 13.61it/s][codecarbon INFO @ 10:52:44] Energy consumed for RAM : 0.000871 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:52:44] Energy consumed for all GPUs : 0.008709 kWh. Total GPU Power : 47.631850258843606 W\n",
            "[codecarbon INFO @ 10:52:44] Energy consumed for all CPUs : 0.007795 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:52:44] 0.017375 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  65%|██████▍   | 405/625 [00:35<00:17, 12.46it/s][codecarbon INFO @ 10:52:59] Energy consumed for RAM : 0.000891 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:52:59] Energy consumed for all GPUs : 0.008909 kWh. Total GPU Power : 48.127081079383814 W\n",
            "[codecarbon INFO @ 10:52:59] Energy consumed for all CPUs : 0.007972 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:52:59] 0.017772 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training:  92%|█████████▏| 577/625 [00:50<00:04, 11.71it/s][codecarbon INFO @ 10:53:14] Energy consumed for RAM : 0.000911 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:53:14] Energy consumed for all GPUs : 0.009118 kWh. Total GPU Power : 49.91913766705602 W\n",
            "[codecarbon INFO @ 10:53:14] Energy consumed for all CPUs : 0.008149 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:53:14] 0.018177 kWh of electricity used since the beginning.\n",
            "Epoch 13/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 1.6991, Training Accuracy: 80.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 - Training:  19%|█▉        | 120/625 [00:10<00:50, 10.10it/s][codecarbon INFO @ 10:53:29] Energy consumed for RAM : 0.000930 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:53:29] Energy consumed for all GPUs : 0.009326 kWh. Total GPU Power : 50.09125092909457 W\n",
            "[codecarbon INFO @ 10:53:29] Energy consumed for all CPUs : 0.008326 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:53:29] 0.018582 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training:  47%|████▋     | 293/625 [00:25<00:34,  9.58it/s][codecarbon INFO @ 10:53:44] Energy consumed for RAM : 0.000950 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:53:44] Energy consumed for all GPUs : 0.009525 kWh. Total GPU Power : 47.73322354228782 W\n",
            "[codecarbon INFO @ 10:53:44] Energy consumed for all CPUs : 0.008503 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:53:44] 0.018978 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:53:44] 0.012527 g.CO2eq/s mean an estimation of 395.0610190213535 kg.CO2eq/year\n",
            "Epoch 14/20 - Training:  74%|███████▍  | 465/625 [00:40<00:17,  9.30it/s][codecarbon INFO @ 10:53:59] Energy consumed for RAM : 0.000970 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:53:59] Energy consumed for all GPUs : 0.009721 kWh. Total GPU Power : 46.95454655292516 W\n",
            "[codecarbon INFO @ 10:53:59] Energy consumed for all CPUs : 0.008681 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:53:59] 0.019371 kWh of electricity used since the beginning.\n",
            "Epoch 14/20 - Training: 100%|██████████| 625/625 [00:53<00:00, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 1.7062, Training Accuracy: 79.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 - Training:   1%|▏         | 8/625 [00:01<01:27,  7.05it/s][codecarbon INFO @ 10:54:14] Energy consumed for RAM : 0.000990 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:54:14] Energy consumed for all GPUs : 0.009932 kWh. Total GPU Power : 50.80241571874605 W\n",
            "[codecarbon INFO @ 10:54:14] Energy consumed for all CPUs : 0.008858 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:54:14] 0.019780 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  27%|██▋       | 168/625 [00:16<00:59,  7.70it/s][codecarbon INFO @ 10:54:29] Energy consumed for RAM : 0.001009 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:54:29] Energy consumed for all GPUs : 0.010133 kWh. Total GPU Power : 48.252080156732895 W\n",
            "[codecarbon INFO @ 10:54:29] Energy consumed for all CPUs : 0.009035 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:54:29] 0.020177 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  55%|█████▍    | 341/625 [00:31<00:32,  8.80it/s][codecarbon INFO @ 10:54:44] Energy consumed for RAM : 0.001029 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:54:44] Energy consumed for all GPUs : 0.010336 kWh. Total GPU Power : 48.83613600569094 W\n",
            "[codecarbon INFO @ 10:54:44] Energy consumed for all CPUs : 0.009212 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:54:44] 0.020577 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training:  82%|████████▏ | 514/625 [00:46<00:13,  8.33it/s][codecarbon INFO @ 10:54:59] Energy consumed for RAM : 0.001049 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:54:59] Energy consumed for all GPUs : 0.010544 kWh. Total GPU Power : 49.92002074713044 W\n",
            "[codecarbon INFO @ 10:54:59] Energy consumed for all CPUs : 0.009389 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:54:59] 0.020983 kWh of electricity used since the beginning.\n",
            "Epoch 15/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 1.7028, Training Accuracy: 79.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 - Training:   7%|▋         | 45/625 [00:05<01:04,  9.04it/s][codecarbon INFO @ 10:55:14] Energy consumed for RAM : 0.001069 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:55:14] Energy consumed for all GPUs : 0.010747 kWh. Total GPU Power : 48.53227790512943 W\n",
            "[codecarbon INFO @ 10:55:14] Energy consumed for all CPUs : 0.009567 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:55:14] 0.021382 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training:  35%|███▍      | 217/625 [00:20<00:48,  8.42it/s][codecarbon INFO @ 10:55:29] Energy consumed for RAM : 0.001089 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:55:29] Energy consumed for all GPUs : 0.010945 kWh. Total GPU Power : 47.62021880100179 W\n",
            "[codecarbon INFO @ 10:55:29] Energy consumed for all CPUs : 0.009744 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:55:29] 0.021777 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training:  63%|██████▎   | 391/625 [00:35<00:30,  7.62it/s][codecarbon INFO @ 10:55:44] Energy consumed for RAM : 0.001108 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:55:44] Energy consumed for all GPUs : 0.011147 kWh. Total GPU Power : 48.69851908894295 W\n",
            "[codecarbon INFO @ 10:55:44] Energy consumed for all CPUs : 0.009920 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:55:44] 0.022176 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:55:44] 0.012532 g.CO2eq/s mean an estimation of 395.2226023012252 kg.CO2eq/year\n",
            "Epoch 16/20 - Training:  90%|█████████ | 565/625 [00:50<00:07,  7.79it/s][codecarbon INFO @ 10:55:59] Energy consumed for RAM : 0.001128 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:55:59] Energy consumed for all GPUs : 0.011349 kWh. Total GPU Power : 48.48101121826821 W\n",
            "[codecarbon INFO @ 10:55:59] Energy consumed for all CPUs : 0.010098 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:55:59] 0.022575 kWh of electricity used since the beginning.\n",
            "Epoch 16/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 1.7321, Training Accuracy: 78.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 - Training:  17%|█▋        | 108/625 [00:10<01:10,  7.29it/s][codecarbon INFO @ 10:56:14] Energy consumed for RAM : 0.001148 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:56:14] Energy consumed for all GPUs : 0.011550 kWh. Total GPU Power : 48.28229585141849 W\n",
            "[codecarbon INFO @ 10:56:14] Energy consumed for all CPUs : 0.010275 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:56:14] 0.022972 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training:  45%|████▍     | 279/625 [00:25<00:38,  9.09it/s][codecarbon INFO @ 10:56:29] Energy consumed for RAM : 0.001168 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:56:29] Energy consumed for all GPUs : 0.011746 kWh. Total GPU Power : 46.990218809616394 W\n",
            "[codecarbon INFO @ 10:56:29] Energy consumed for all CPUs : 0.010452 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:56:29] 0.023366 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training:  72%|███████▏  | 453/625 [00:40<00:17,  9.69it/s][codecarbon INFO @ 10:56:44] Energy consumed for RAM : 0.001188 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:56:44] Energy consumed for all GPUs : 0.011945 kWh. Total GPU Power : 47.63045928192932 W\n",
            "[codecarbon INFO @ 10:56:44] Energy consumed for all CPUs : 0.010630 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:56:44] 0.023762 kWh of electricity used since the beginning.\n",
            "Epoch 17/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 1.7307, Training Accuracy: 77.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 - Training:   0%|          | 3/625 [00:00<02:18,  4.49it/s][codecarbon INFO @ 10:56:59] Energy consumed for RAM : 0.001207 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:56:59] Energy consumed for all GPUs : 0.012149 kWh. Total GPU Power : 48.98064648570396 W\n",
            "[codecarbon INFO @ 10:56:59] Energy consumed for all CPUs : 0.010807 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:56:59] 0.024163 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training:  28%|██▊       | 176/625 [00:15<00:34, 12.97it/s][codecarbon INFO @ 10:57:14] Energy consumed for RAM : 0.001227 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:57:14] Energy consumed for all GPUs : 0.012351 kWh. Total GPU Power : 48.65304616218441 W\n",
            "[codecarbon INFO @ 10:57:14] Energy consumed for all CPUs : 0.010984 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:57:14] 0.024562 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training:  56%|█████▌    | 347/625 [00:30<00:23, 11.90it/s][codecarbon INFO @ 10:57:29] Energy consumed for RAM : 0.001247 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:57:29] Energy consumed for all GPUs : 0.012552 kWh. Total GPU Power : 48.36055728667963 W\n",
            "[codecarbon INFO @ 10:57:29] Energy consumed for all CPUs : 0.011161 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:57:29] 0.024960 kWh of electricity used since the beginning.\n",
            "Epoch 18/20 - Training:  84%|████████▎ | 522/625 [00:45<00:07, 13.01it/s][codecarbon INFO @ 10:57:44] Energy consumed for RAM : 0.001267 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:57:44] Energy consumed for all GPUs : 0.012755 kWh. Total GPU Power : 48.56464292596391 W\n",
            "[codecarbon INFO @ 10:57:44] Energy consumed for all CPUs : 0.011338 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:57:44] 0.025360 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:57:44] 0.012474 g.CO2eq/s mean an estimation of 393.3766186095923 kg.CO2eq/year\n",
            "Epoch 18/20 - Training: 100%|██████████| 625/625 [00:53<00:00, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 1.6872, Training Accuracy: 77.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 - Training:  10%|█         | 64/625 [00:07<00:47, 11.88it/s][codecarbon INFO @ 10:57:59] Energy consumed for RAM : 0.001286 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:57:59] Energy consumed for all GPUs : 0.012970 kWh. Total GPU Power : 51.60729495642105 W\n",
            "[codecarbon INFO @ 10:57:59] Energy consumed for all CPUs : 0.011516 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:57:59] 0.025772 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training:  38%|███▊      | 237/625 [00:22<00:29, 13.27it/s][codecarbon INFO @ 10:58:14] Energy consumed for RAM : 0.001306 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:58:14] Energy consumed for all GPUs : 0.013163 kWh. Total GPU Power : 46.526768244231704 W\n",
            "[codecarbon INFO @ 10:58:14] Energy consumed for all CPUs : 0.011692 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:58:14] 0.026162 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training:  66%|██████▌   | 411/625 [00:37<00:17, 12.54it/s][codecarbon INFO @ 10:58:29] Energy consumed for RAM : 0.001326 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:58:29] Energy consumed for all GPUs : 0.013363 kWh. Total GPU Power : 47.97954654338202 W\n",
            "[codecarbon INFO @ 10:58:29] Energy consumed for all CPUs : 0.011870 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:58:29] 0.026559 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training:  94%|█████████▍| 587/625 [00:52<00:02, 14.21it/s][codecarbon INFO @ 10:58:44] Energy consumed for RAM : 0.001346 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:58:44] Energy consumed for all GPUs : 0.013566 kWh. Total GPU Power : 48.542794492854505 W\n",
            "[codecarbon INFO @ 10:58:44] Energy consumed for all CPUs : 0.012047 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:58:44] 0.026958 kWh of electricity used since the beginning.\n",
            "Epoch 19/20 - Training: 100%|██████████| 625/625 [00:55<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 1.6403, Training Accuracy: 77.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 - Training:  21%|██        | 129/625 [00:12<00:42, 11.57it/s][codecarbon INFO @ 10:58:59] Energy consumed for RAM : 0.001366 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:58:59] Energy consumed for all GPUs : 0.013771 kWh. Total GPU Power : 49.276042864736674 W\n",
            "[codecarbon INFO @ 10:58:59] Energy consumed for all CPUs : 0.012224 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:58:59] 0.027361 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training:  48%|████▊     | 301/625 [00:27<00:25, 12.51it/s][codecarbon INFO @ 10:59:14] Energy consumed for RAM : 0.001385 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:59:14] Energy consumed for all GPUs : 0.013972 kWh. Total GPU Power : 48.13494322496946 W\n",
            "[codecarbon INFO @ 10:59:14] Energy consumed for all CPUs : 0.012401 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:59:14] 0.027758 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training:  76%|███████▌  | 476/625 [00:42<00:11, 12.81it/s][codecarbon INFO @ 10:59:29] Energy consumed for RAM : 0.001405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:59:29] Energy consumed for all GPUs : 0.014180 kWh. Total GPU Power : 50.11315406382429 W\n",
            "[codecarbon INFO @ 10:59:29] Energy consumed for all CPUs : 0.012579 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:59:29] 0.028164 kWh of electricity used since the beginning.\n",
            "Epoch 20/20 - Training: 100%|██████████| 625/625 [00:54<00:00, 11.38it/s]\n",
            "[codecarbon INFO @ 10:59:42] Energy consumed for RAM : 0.001422 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 10:59:42] Energy consumed for all GPUs : 0.014352 kWh. Total GPU Power : 49.35006326030559 W\n",
            "[codecarbon INFO @ 10:59:42] Energy consumed for all CPUs : 0.012727 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:59:42] 0.028501 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 10:59:42] 0.012568 g.CO2eq/s mean an estimation of 396.33392046543497 kg.CO2eq/year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 1.4894, Training Accuracy: 77.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Analysis ---\n",
            "Parameter Count: 2554378\n",
            "Model Size: 9.98 MB\n",
            "FLOPs: 0.01 GFLOPs\n",
            "Training Time: 17.98 minutes\n",
            "Average Inference Time: 0.021753 seconds\n",
            "\n",
            "--- Energy and Emissions Report ---\n",
            "CO2 Emissions: 0.013418 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgC4NuHg2SV7",
        "outputId": "6aa0c6b7-7bf7-48ba-990b-5ae295d5c6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test dataset\n",
        "student_model.eval()\n",
        "correct_test, total_test = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = student_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbBKIsy32Z94",
        "outputId": "d7b7d749-21de-4317-e29f-a34c827bf0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gq8QzwJF7lVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}