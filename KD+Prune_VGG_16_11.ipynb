{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLG5r_qQFkIp"
      },
      "source": [
        "## Medium Article Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYDqB3PwFGBV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYDDJiRtFjqQ",
        "outputId": "e008082b-d518-4fc6-fec9-9cf4d2795f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "num_epochs = 5\n",
        "batch_size = 40\n",
        "learning_rate = 0.001\n",
        "classes = ('plane', 'car' , 'bird',\n",
        "    'cat', 'deer', 'dog',\n",
        "    'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL5L1bAVFn_A",
        "outputId": "d254b086-2b61-4bbb-ff56-ab96309496dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "       (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    )\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = True,\n",
        "    download =True, transform = transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root= './data', train = False,\n",
        "    download =True, transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BVx4Ap2HiPo",
        "outputId": "0e6ab198-8ea9-470b-e68e-2cfaf463c867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1250\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset\n",
        "    , batch_size = batch_size\n",
        "    , shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset\n",
        "    , batch_size = batch_size\n",
        "    , shuffle = True)\n",
        "n_total_step = len(train_loader)\n",
        "print(n_total_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFG7Ns3tHile",
        "outputId": "505f49a4-bb6b-4107-b062-913b159e779f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 128MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg16(pretrained = True)\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(input_lastLayer,10)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9,weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n4CS1x7uHfwp",
        "outputId": "2d704621-1b8e-4b70-9bfe-5db02701f7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 1/5, step: 250/1250: loss = 0.28140, acc = 87.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 1/5, step: 500/1250: loss = 0.27473, acc = 85.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 1/5, step: 750/1250: loss = 0.23134, acc = 92.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 1/5, step: 1000/1250: loss = 0.24825, acc = 90.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 1/5, step: 1250/1250: loss = 0.17533, acc = 92.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 2/5, step: 250/1250: loss = 0.02754, acc = 100.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 2/5, step: 500/1250: loss = 0.15064, acc = 95.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 2/5, step: 750/1250: loss = 0.19934, acc = 90.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 2/5, step: 1000/1250: loss = 0.33348, acc = 92.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 2/5, step: 1250/1250: loss = 0.08297, acc = 97.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 3/5, step: 250/1250: loss = 0.16366, acc = 95.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 3/5, step: 500/1250: loss = 0.09611, acc = 97.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 3/5, step: 750/1250: loss = 0.14750, acc = 92.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 3/5, step: 1000/1250: loss = 0.05519, acc = 97.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 3/5, step: 1250/1250: loss = 0.08164, acc = 97.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 4/5, step: 250/1250: loss = 0.08666, acc = 95.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 4/5, step: 500/1250: loss = 0.03823, acc = 100.00%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epoch 4/5, step: 750/1250: loss = 0.07935, acc = 97.50%\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c32ca6db0ffc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (imgs , labels) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        labels_hat = model(imgs)\n",
        "        n_corrects = (labels_hat.argmax(axis=1)==labels).sum().item()\n",
        "        loss_value = criterion(labels_hat, labels)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if (i+1) % 250 == 0:\n",
        "          print(f\"epoch {epoch+1}/{num_epochs}, step: {i+1}/{n_total_step}: loss = {loss_value:.5f}, acc = {100*(n_corrects/labels.size(0)):.2f}%\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVGF-nJMH0oA",
        "outputId": "bcd8b712-ece9-4291-eece-1407e6d2fad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall accuracy 91.33%\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    number_corrects = 0\n",
        "    number_samples = 0\n",
        "    for i, (test_images_set , test_labels_set) in enumerate(test_loader):\n",
        "        test_images_set = test_images_set.to(device)\n",
        "        test_labels_set = test_labels_set.to(device)\n",
        "        y_predicted = model(test_images_set)\n",
        "        labels_predicted = y_predicted.argmax(axis = 1)\n",
        "        number_corrects += (labels_predicted==test_labels_set).sum().item()\n",
        "        number_samples += test_labels_set.size(0)\n",
        "    print(f'Overall accuracy {(number_corrects / number_samples)*100}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfi7n0WlH0kW"
      },
      "outputs": [],
      "source": [
        "heatmap = pd.DataFrame(data=0,index=classes,columns=classes)\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(batch_size):\n",
        "            true_label = labels[i].item()\n",
        "            predicted_label = predicted[i].item()\n",
        "            heatmap.iloc[true_label,predicted_label] += 1\n",
        "_, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.heatmap(heatmap, annot=True, fmt=”d”,cmap=”YlGnBu”)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CLBuRIbH0fX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9Cv9pADIKUt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycj1TyIikSLI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHbQyEhHkSI-",
        "outputId": "90df5fb8-c512-4434-abeb-684e78a42cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.7.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Collecting pynvml (from codecarbon)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Collecting prompt_toolkit<=3.0.36,>=2.0 (from questionary->codecarbon)\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.7.4-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.2/504.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.0.1-py3-none-any.whl (34 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, pynvml, prompt_toolkit, yaspin, questionary, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: prompt_toolkit\n",
            "    Found existing installation: prompt_toolkit 3.0.48\n",
            "    Uninstalling prompt_toolkit-3.0.48:\n",
            "      Successfully uninstalled prompt_toolkit-3.0.48\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.7.4 fief-client-0.20.0 jwcrypto-1.5.6 prompt_toolkit-3.0.36 pynvml-11.5.3 questionary-2.0.1 rapidfuzz-3.10.1 termcolor-2.3.0 types-python-dateutil-2.9.0.20241003 yaspin-3.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0ddfe864448842aebc4b706223d03b9a",
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "askZFP8SkSGb",
        "outputId": "c58b5c69-2d5c-4fe2-82fd-eec1ebcd5255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ],
      "source": [
        "!pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve5VZl8Kk1JI",
        "outputId": "73d40cae-da39-44d7-80cb-e305954bee32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import codecarbon\n",
        "from thop import profile\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuQoUisRk9ye",
        "outputId": "d8306cfa-8192-4725-9496-4ca08132f834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training set size: 40000\n",
            "Validation set size: 10000\n",
            "Test set size: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the transforms for the dataset with additional augmentations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split the full training dataset into training and validation sets (80% train, 20% validation)\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Print dataset sizes for debugging\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiE54aXWkSDq",
        "outputId": "25a515f7-44f5-4fab-8788-d523802a3d1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 86.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:24:40] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 03:24:40] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 03:24:40] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 03:24:40] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 03:24:40] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 03:24:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 03:24:41] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 03:24:41] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 03:24:41]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 03:24:41]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 03:24:41]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 03:24:41]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 03:24:41]   CPU count: 2\n",
            "[codecarbon INFO @ 03:24:41]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 03:24:41]   GPU count: 1\n",
            "[codecarbon INFO @ 03:24:41]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 03:24:41] Saving emissions data to file /content/emissions.csv\n",
            "Epoch 1/25 - Training:   0%|          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/25 - Training:  19%|█▉        | 121/625 [00:14<00:45, 10.99it/s][codecarbon INFO @ 03:24:56] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:24:56] Energy consumed for all GPUs : 0.000223 kWh. Total GPU Power : 53.44238898507237 W\n",
            "[codecarbon INFO @ 03:24:56] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:24:56] 0.000420 kWh of electricity used since the beginning.\n",
            "Epoch 1/25 - Training:  44%|████▎     | 273/625 [00:29<00:31, 11.25it/s][codecarbon INFO @ 03:25:11] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:25:11] Energy consumed for all GPUs : 0.000476 kWh. Total GPU Power : 60.96460019287156 W\n",
            "[codecarbon INFO @ 03:25:11] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:25:11] 0.000870 kWh of electricity used since the beginning.\n",
            "Epoch 1/25 - Training:  68%|██████▊   | 426/625 [00:44<00:17, 11.14it/s][codecarbon INFO @ 03:25:26] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:25:26] Energy consumed for all GPUs : 0.000730 kWh. Total GPU Power : 60.87444749848632 W\n",
            "[codecarbon INFO @ 03:25:26] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:25:26] 0.001321 kWh of electricity used since the beginning.\n",
            "Epoch 1/25 - Training:  93%|█████████▎| 580/625 [00:59<00:04, 11.24it/s][codecarbon INFO @ 03:25:41] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:25:41] Energy consumed for all GPUs : 0.000987 kWh. Total GPU Power : 61.84430172517769 W\n",
            "[codecarbon INFO @ 03:25:41] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:25:41] 0.001775 kWh of electricity used since the beginning.\n",
            "Epoch 1/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.77it/s]\n",
            "Epoch 1/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Train Loss: 1.2619, Train Accuracy: 57.04%\n",
            "Epoch [1/25], Val Loss: 0.9031, Val Accuracy: 69.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2/25 - Training:   0%|          | 0/625 [00:00<?, ?it/s][codecarbon INFO @ 03:25:56] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:25:56] Energy consumed for all GPUs : 0.001179 kWh. Total GPU Power : 45.910643096133235 W\n",
            "[codecarbon INFO @ 03:25:56] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:25:56] 0.002163 kWh of electricity used since the beginning.\n",
            "Epoch 2/25 - Training:  23%|██▎       | 143/625 [00:15<00:44, 10.86it/s][codecarbon INFO @ 03:26:11] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:26:11] Energy consumed for all GPUs : 0.001431 kWh. Total GPU Power : 60.52333402443282 W\n",
            "[codecarbon INFO @ 03:26:11] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:26:11] 0.002612 kWh of electricity used since the beginning.\n",
            "Epoch 2/25 - Training:  46%|████▌     | 285/625 [00:29<00:30, 10.97it/s][codecarbon INFO @ 03:26:26] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:26:26] Energy consumed for all GPUs : 0.001683 kWh. Total GPU Power : 60.679759473460145 W\n",
            "[codecarbon INFO @ 03:26:26] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:26:26] 0.003062 kWh of electricity used since the beginning.\n",
            "Epoch 2/25 - Training:  70%|███████   | 438/625 [00:45<00:16, 11.33it/s][codecarbon INFO @ 03:26:41] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:26:41] Energy consumed for all GPUs : 0.001944 kWh. Total GPU Power : 62.62518078025344 W\n",
            "[codecarbon INFO @ 03:26:41] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:26:41] 0.003520 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:26:41] 0.010235 g.CO2eq/s mean an estimation of 322.77442002591613 kg.CO2eq/year\n",
            "Epoch 2/25 - Training:  95%|█████████▍| 592/625 [01:00<00:02, 11.19it/s][codecarbon INFO @ 03:26:56] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:26:56] Energy consumed for all GPUs : 0.002206 kWh. Total GPU Power : 62.83069780867359 W\n",
            "[codecarbon INFO @ 03:26:56] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:26:56] 0.003978 kWh of electricity used since the beginning.\n",
            "Epoch 2/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.91it/s]\n",
            "Epoch 2/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 2/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/25], Train Loss: 0.8249, Train Accuracy: 73.43%\n",
            "Epoch [2/25], Val Loss: 0.8351, Val Accuracy: 72.66%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/25 - Training:   1%|▏         | 9/625 [00:01<01:06,  9.33it/s][codecarbon INFO @ 03:27:11] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:27:11] Energy consumed for all GPUs : 0.002399 kWh. Total GPU Power : 46.33225168392685 W\n",
            "[codecarbon INFO @ 03:27:11] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:27:11] 0.004368 kWh of electricity used since the beginning.\n",
            "Epoch 3/25 - Training:  26%|██▌       | 160/625 [00:16<00:40, 11.34it/s][codecarbon INFO @ 03:27:26] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:27:26] Energy consumed for all GPUs : 0.002660 kWh. Total GPU Power : 62.68205276766801 W\n",
            "[codecarbon INFO @ 03:27:26] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:27:26] 0.004826 kWh of electricity used since the beginning.\n",
            "Epoch 3/25 - Training:  50%|█████     | 313/625 [00:31<00:28, 11.08it/s][codecarbon INFO @ 03:27:41] Energy consumed for RAM : 0.000237 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:27:41] Energy consumed for all GPUs : 0.002925 kWh. Total GPU Power : 63.55726167786459 W\n",
            "[codecarbon INFO @ 03:27:41] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:27:41] 0.005288 kWh of electricity used since the beginning.\n",
            "Epoch 3/25 - Training:  74%|███████▍  | 465/625 [00:46<00:14, 11.35it/s][codecarbon INFO @ 03:27:56] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:27:56] Energy consumed for all GPUs : 0.003189 kWh. Total GPU Power : 63.36440570834628 W\n",
            "[codecarbon INFO @ 03:27:56] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:27:56] 0.005749 kWh of electricity used since the beginning.\n",
            "Epoch 3/25 - Training:  99%|█████████▉| 620/625 [01:01<00:00, 11.39it/s][codecarbon INFO @ 03:28:11] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:28:11] Energy consumed for all GPUs : 0.003454 kWh. Total GPU Power : 63.71891069149403 W\n",
            "[codecarbon INFO @ 03:28:11] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:28:11] 0.006211 kWh of electricity used since the beginning.\n",
            "Epoch 3/25 - Training: 100%|██████████| 625/625 [01:01<00:00, 10.12it/s]\n",
            "Epoch 3/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 3/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/25], Train Loss: 0.7086, Train Accuracy: 77.26%\n",
            "Epoch [3/25], Val Loss: 0.7034, Val Accuracy: 76.66%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/25 - Training:   6%|▌         | 35/625 [00:03<00:52, 11.28it/s][codecarbon INFO @ 03:28:26] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:28:26] Energy consumed for all GPUs : 0.003648 kWh. Total GPU Power : 46.56021388548481 W\n",
            "[codecarbon INFO @ 03:28:26] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:28:26] 0.006603 kWh of electricity used since the beginning.\n",
            "Epoch 4/25 - Training:  30%|██▉       | 186/625 [00:18<00:42, 10.33it/s][codecarbon INFO @ 03:28:41] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:28:41] Energy consumed for all GPUs : 0.003912 kWh. Total GPU Power : 63.32185351297714 W\n",
            "[codecarbon INFO @ 03:28:41] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:28:41] 0.007063 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:28:41] 0.010303 g.CO2eq/s mean an estimation of 324.92330848640927 kg.CO2eq/year\n",
            "Epoch 4/25 - Training:  54%|█████▎    | 335/625 [00:33<00:30,  9.65it/s][codecarbon INFO @ 03:28:56] Energy consumed for RAM : 0.000336 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:28:56] Energy consumed for all GPUs : 0.004177 kWh. Total GPU Power : 63.68429191617699 W\n",
            "[codecarbon INFO @ 03:28:56] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:28:56] 0.007525 kWh of electricity used since the beginning.\n",
            "Epoch 4/25 - Training:  77%|███████▋  | 484/625 [00:48<00:16,  8.81it/s][codecarbon INFO @ 03:29:11] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:29:11] Energy consumed for all GPUs : 0.004441 kWh. Total GPU Power : 63.40842596908897 W\n",
            "[codecarbon INFO @ 03:29:11] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:29:11] 0.007986 kWh of electricity used since the beginning.\n",
            "Epoch 4/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.97it/s]\n",
            "Epoch 4/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 4/25 - Validation:   3%|▎         | 5/157 [00:00<00:23,  6.52it/s][codecarbon INFO @ 03:29:26] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:29:26] Energy consumed for all GPUs : 0.004702 kWh. Total GPU Power : 62.58633563114675 W\n",
            "[codecarbon INFO @ 03:29:26] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:29:26] 0.008443 kWh of electricity used since the beginning.\n",
            "Epoch 4/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/25], Train Loss: 0.6310, Train Accuracy: 79.56%\n",
            "Epoch [4/25], Val Loss: 0.5849, Val Accuracy: 80.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/25 - Training:   8%|▊         | 48/625 [00:05<01:11,  8.05it/s][codecarbon INFO @ 03:29:41] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:29:41] Energy consumed for all GPUs : 0.004905 kWh. Total GPU Power : 48.644529056470645 W\n",
            "[codecarbon INFO @ 03:29:41] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:29:41] 0.008843 kWh of electricity used since the beginning.\n",
            "Epoch 5/25 - Training:  32%|███▏      | 198/625 [00:20<00:52,  8.14it/s][codecarbon INFO @ 03:29:56] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:29:56] Energy consumed for all GPUs : 0.005169 kWh. Total GPU Power : 63.4227118834812 W\n",
            "[codecarbon INFO @ 03:29:56] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:29:56] 0.009304 kWh of electricity used since the beginning.\n",
            "Epoch 5/25 - Training:  56%|█████▌    | 348/625 [00:35<00:31,  8.68it/s][codecarbon INFO @ 03:30:11] Energy consumed for RAM : 0.000435 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 5/25 - Training:  56%|█████▌    | 349/625 [00:35<00:34,  8.06it/s][codecarbon INFO @ 03:30:11] Energy consumed for all GPUs : 0.005434 kWh. Total GPU Power : 63.74834170193395 W\n",
            "[codecarbon INFO @ 03:30:11] Energy consumed for all CPUs : 0.003897 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:30:11] 0.009766 kWh of electricity used since the beginning.\n",
            "Epoch 5/25 - Training:  80%|████████  | 500/625 [00:50<00:15,  8.16it/s][codecarbon INFO @ 03:30:26] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:30:26] Energy consumed for all GPUs : 0.005700 kWh. Total GPU Power : 64.13020631338424 W\n",
            "[codecarbon INFO @ 03:30:26] Energy consumed for all CPUs : 0.004074 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:30:26] 0.010229 kWh of electricity used since the beginning.\n",
            "Epoch 5/25 - Training: 100%|██████████| 625/625 [01:02<00:00, 10.06it/s]\n",
            "Epoch 5/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 5/25 - Validation:  19%|█▉        | 30/157 [00:03<00:12, 10.45it/s][codecarbon INFO @ 03:30:41] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:30:41] Energy consumed for all GPUs : 0.005952 kWh. Total GPU Power : 60.47265760418888 W\n",
            "[codecarbon INFO @ 03:30:41] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:30:41] 0.010678 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:30:41] 0.010515 g.CO2eq/s mean an estimation of 331.5922045318682 kg.CO2eq/year\n",
            "Epoch 5/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/25], Train Loss: 0.5777, Train Accuracy: 81.25%\n",
            "Epoch [5/25], Val Loss: 0.6102, Val Accuracy: 79.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/25 - Training:  10%|█         | 65/625 [00:07<01:13,  7.61it/s][codecarbon INFO @ 03:30:56] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:30:56] Energy consumed for all GPUs : 0.006165 kWh. Total GPU Power : 51.27155929314384 W\n",
            "[codecarbon INFO @ 03:30:56] Energy consumed for all CPUs : 0.004428 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:30:56] 0.011088 kWh of electricity used since the beginning.\n",
            "Epoch 6/25 - Training:  35%|███▍      | 217/625 [00:22<00:39, 10.43it/s][codecarbon INFO @ 03:31:11] Energy consumed for RAM : 0.000514 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:31:11] Energy consumed for all GPUs : 0.006432 kWh. Total GPU Power : 63.999818179342846 W\n",
            "[codecarbon INFO @ 03:31:11] Energy consumed for all CPUs : 0.004605 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:31:11] 0.011552 kWh of electricity used since the beginning.\n",
            "Epoch 6/25 - Training:  59%|█████▉    | 368/625 [00:37<00:24, 10.59it/s][codecarbon INFO @ 03:31:26] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:31:26] Energy consumed for all GPUs : 0.006699 kWh. Total GPU Power : 64.08614254538043 W\n",
            "[codecarbon INFO @ 03:31:26] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:31:26] 0.012016 kWh of electricity used since the beginning.\n",
            "Epoch 6/25 - Training:  83%|████████▎ | 520/625 [00:52<00:09, 10.87it/s][codecarbon INFO @ 03:31:41] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:31:41] Energy consumed for all GPUs : 0.006966 kWh. Total GPU Power : 64.24068923675392 W\n",
            "[codecarbon INFO @ 03:31:41] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:31:41] 0.012480 kWh of electricity used since the beginning.\n",
            "Epoch 6/25 - Training: 100%|██████████| 625/625 [01:02<00:00, 10.00it/s]\n",
            "Epoch 6/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 6/25 - Validation:  41%|████▏     | 65/157 [00:05<00:05, 16.85it/s][codecarbon INFO @ 03:31:56] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:31:56] Energy consumed for all GPUs : 0.007206 kWh. Total GPU Power : 57.542477171619545 W\n",
            "[codecarbon INFO @ 03:31:56] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:31:56] 0.012917 kWh of electricity used since the beginning.\n",
            "Epoch 6/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/25], Train Loss: 0.5402, Train Accuracy: 82.44%\n",
            "Epoch [6/25], Val Loss: 0.5698, Val Accuracy: 81.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/25 - Training:  15%|█▍        | 91/625 [00:09<00:49, 10.73it/s][codecarbon INFO @ 03:32:11] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:32:11] Energy consumed for all GPUs : 0.007439 kWh. Total GPU Power : 55.84007817767733 W\n",
            "[codecarbon INFO @ 03:32:11] Energy consumed for all CPUs : 0.005314 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:32:11] 0.013346 kWh of electricity used since the beginning.\n",
            "Epoch 7/25 - Training:  39%|███▊      | 241/625 [00:25<00:35, 10.90it/s][codecarbon INFO @ 03:32:26] Energy consumed for RAM : 0.000613 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:32:26] Energy consumed for all GPUs : 0.007705 kWh. Total GPU Power : 63.873553333034195 W\n",
            "[codecarbon INFO @ 03:32:26] Energy consumed for all CPUs : 0.005491 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:32:26] 0.013809 kWh of electricity used since the beginning.\n",
            "Epoch 7/25 - Training:  63%|██████▎   | 391/625 [00:39<00:21, 11.02it/s][codecarbon INFO @ 03:32:41] Energy consumed for RAM : 0.000633 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:32:41] Energy consumed for all GPUs : 0.007974 kWh. Total GPU Power : 64.64643508827503 W\n",
            "[codecarbon INFO @ 03:32:41] Energy consumed for all CPUs : 0.005668 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:32:41] 0.014275 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:32:41] 0.010462 g.CO2eq/s mean an estimation of 329.92323842433996 kg.CO2eq/year\n",
            "Epoch 7/25 - Training:  87%|████████▋ | 541/625 [00:54<00:07, 11.19it/s][codecarbon INFO @ 03:32:56] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:32:56] Energy consumed for all GPUs : 0.008240 kWh. Total GPU Power : 63.87982821085793 W\n",
            "[codecarbon INFO @ 03:32:56] Energy consumed for all CPUs : 0.005845 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:32:56] 0.014738 kWh of electricity used since the beginning.\n",
            "Epoch 7/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.92it/s]\n",
            "Epoch 7/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 7/25 - Validation:  75%|███████▍  | 117/157 [00:07<00:02, 19.35it/s][codecarbon INFO @ 03:33:11] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:33:11] Energy consumed for all GPUs : 0.008472 kWh. Total GPU Power : 55.74048865987639 W\n",
            "[codecarbon INFO @ 03:33:11] Energy consumed for all CPUs : 0.006022 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:33:11] 0.015168 kWh of electricity used since the beginning.\n",
            "Epoch 7/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/25], Train Loss: 0.4976, Train Accuracy: 83.83%\n",
            "Epoch [7/25], Val Loss: 0.5572, Val Accuracy: 81.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/25 - Training:  19%|█▉        | 121/625 [00:12<00:45, 11.12it/s][codecarbon INFO @ 03:33:26] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:33:26] Energy consumed for all GPUs : 0.008722 kWh. Total GPU Power : 59.83135530256911 W\n",
            "[codecarbon INFO @ 03:33:26] Energy consumed for all CPUs : 0.006199 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:33:26] 0.015614 kWh of electricity used since the beginning.\n",
            "Epoch 8/25 - Training:  44%|████▍     | 274/625 [00:27<00:32, 10.96it/s][codecarbon INFO @ 03:33:41] Energy consumed for RAM : 0.000712 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:33:41] Energy consumed for all GPUs : 0.008990 kWh. Total GPU Power : 64.29645933869843 W\n",
            "[codecarbon INFO @ 03:33:41] Energy consumed for all CPUs : 0.006377 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:33:41] 0.016079 kWh of electricity used since the beginning.\n",
            "Epoch 8/25 - Training:  68%|██████▊   | 427/625 [00:42<00:18, 10.85it/s][codecarbon INFO @ 03:33:56] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:33:56] Energy consumed for all GPUs : 0.009257 kWh. Total GPU Power : 64.31210523068346 W\n",
            "[codecarbon INFO @ 03:33:56] Energy consumed for all CPUs : 0.006554 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:33:56] 0.016543 kWh of electricity used since the beginning.\n",
            "Epoch 8/25 - Training:  93%|█████████▎| 579/625 [00:57<00:04, 11.03it/s][codecarbon INFO @ 03:34:11] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:34:11] Energy consumed for all GPUs : 0.009524 kWh. Total GPU Power : 64.03047788071927 W\n",
            "[codecarbon INFO @ 03:34:11] Energy consumed for all CPUs : 0.006731 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:34:11] 0.017007 kWh of electricity used since the beginning.\n",
            "Epoch 8/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.94it/s]\n",
            "Epoch 8/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 8/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/25], Train Loss: 0.4730, Train Accuracy: 84.34%\n",
            "Epoch [8/25], Val Loss: 0.5222, Val Accuracy: 82.78%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/25 - Training:   0%|          | 3/625 [00:00<02:07,  4.87it/s][codecarbon INFO @ 03:34:26] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:34:26] Energy consumed for all GPUs : 0.009739 kWh. Total GPU Power : 51.47966841292392 W\n",
            "[codecarbon INFO @ 03:34:26] Energy consumed for all CPUs : 0.006908 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:34:26] 0.017419 kWh of electricity used since the beginning.\n",
            "Epoch 9/25 - Training:  24%|██▍       | 153/625 [00:15<00:47,  9.95it/s][codecarbon INFO @ 03:34:41] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:34:41] Energy consumed for all GPUs : 0.010005 kWh. Total GPU Power : 63.79991888990163 W\n",
            "[codecarbon INFO @ 03:34:41] Energy consumed for all CPUs : 0.007086 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:34:41] 0.017882 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:34:41] 0.010485 g.CO2eq/s mean an estimation of 330.6422419661948 kg.CO2eq/year\n",
            "Epoch 9/25 - Training:  48%|████▊     | 302/625 [00:30<00:34,  9.25it/s][codecarbon INFO @ 03:34:56] Energy consumed for RAM : 0.000811 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:34:56] Energy consumed for all GPUs : 0.010272 kWh. Total GPU Power : 64.17678715701905 W\n",
            "[codecarbon INFO @ 03:34:56] Energy consumed for all CPUs : 0.007263 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:34:56] 0.018346 kWh of electricity used since the beginning.\n",
            "Epoch 9/25 - Training:  72%|███████▏  | 453/625 [00:45<00:20,  8.49it/s][codecarbon INFO @ 03:35:11] Energy consumed for RAM : 0.000831 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:35:11] Energy consumed for all GPUs : 0.010540 kWh. Total GPU Power : 64.3718550457346 W\n",
            "[codecarbon INFO @ 03:35:11] Energy consumed for all CPUs : 0.007439 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:35:11] 0.018810 kWh of electricity used since the beginning.\n",
            "Epoch 9/25 - Training:  97%|█████████▋| 604/625 [01:00<00:02,  7.92it/s][codecarbon INFO @ 03:35:26] Energy consumed for RAM : 0.000851 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:35:26] Energy consumed for all GPUs : 0.010807 kWh. Total GPU Power : 64.20585609235515 W\n",
            "[codecarbon INFO @ 03:35:26] Energy consumed for all CPUs : 0.007617 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:35:26] 0.019275 kWh of electricity used since the beginning.\n",
            "Epoch 9/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.89it/s]\n",
            "Epoch 9/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 9/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/25], Train Loss: 0.4518, Train Accuracy: 85.23%\n",
            "Epoch [9/25], Val Loss: 0.5255, Val Accuracy: 82.85%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/25 - Training:   4%|▍         | 26/625 [00:03<01:17,  7.71it/s][codecarbon INFO @ 03:35:41] Energy consumed for RAM : 0.000871 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:35:41] Energy consumed for all GPUs : 0.011021 kWh. Total GPU Power : 51.17895903410742 W\n",
            "[codecarbon INFO @ 03:35:41] Energy consumed for all CPUs : 0.007794 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:35:41] 0.019686 kWh of electricity used since the beginning.\n",
            "Epoch 10/25 - Training:  28%|██▊       | 175/625 [00:18<00:53,  8.41it/s][codecarbon INFO @ 03:35:56] Energy consumed for RAM : 0.000891 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:35:56] Energy consumed for all GPUs : 0.011287 kWh. Total GPU Power : 63.83150507779245 W\n",
            "[codecarbon INFO @ 03:35:56] Energy consumed for all CPUs : 0.007971 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:35:56] 0.020149 kWh of electricity used since the beginning.\n",
            "Epoch 10/25 - Training:  52%|█████▏    | 325/625 [00:33<00:38,  7.69it/s][codecarbon INFO @ 03:36:11] Energy consumed for RAM : 0.000910 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:36:11] Energy consumed for all GPUs : 0.011554 kWh. Total GPU Power : 64.2514550123985 W\n",
            "[codecarbon INFO @ 03:36:11] Energy consumed for all CPUs : 0.008148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:36:11] 0.020612 kWh of electricity used since the beginning.\n",
            "Epoch 10/25 - Training:  76%|███████▌  | 475/625 [00:48<00:19,  7.58it/s][codecarbon INFO @ 03:36:26] Energy consumed for RAM : 0.000930 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:36:26] Energy consumed for all GPUs : 0.011820 kWh. Total GPU Power : 64.03863776652992 W\n",
            "[codecarbon INFO @ 03:36:26] Energy consumed for all CPUs : 0.008326 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:36:26] 0.021076 kWh of electricity used since the beginning.\n",
            "Epoch 10/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.90it/s]\n",
            "Epoch 10/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 10/25 - Validation:   1%|          | 1/157 [00:00<00:37,  4.20it/s][codecarbon INFO @ 03:36:41] Energy consumed for RAM : 0.000950 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:36:41] Energy consumed for all GPUs : 0.012088 kWh. Total GPU Power : 64.46367981293841 W\n",
            "[codecarbon INFO @ 03:36:41] Energy consumed for all CPUs : 0.008502 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:36:41] 0.021540 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:36:41] 0.010641 g.CO2eq/s mean an estimation of 335.5632106607693 kg.CO2eq/year\n",
            "Epoch 10/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/25], Train Loss: 0.4348, Train Accuracy: 85.86%\n",
            "Epoch [10/25], Val Loss: 0.5010, Val Accuracy: 83.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/25 - Training:   8%|▊         | 49/625 [00:06<00:56, 10.18it/s][codecarbon INFO @ 03:36:56] Energy consumed for RAM : 0.000970 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 11/25 - Training:   8%|▊         | 50/625 [00:06<00:57,  9.96it/s][codecarbon INFO @ 03:36:56] Energy consumed for all GPUs : 0.012302 kWh. Total GPU Power : 51.354166832827374 W\n",
            "[codecarbon INFO @ 03:36:56] Energy consumed for all CPUs : 0.008680 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:36:56] 0.021952 kWh of electricity used since the beginning.\n",
            "Epoch 11/25 - Training:  32%|███▏      | 201/625 [00:21<00:38, 10.93it/s][codecarbon INFO @ 03:37:11] Energy consumed for RAM : 0.000989 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:37:11] Energy consumed for all GPUs : 0.012571 kWh. Total GPU Power : 64.62979783724934 W\n",
            "[codecarbon INFO @ 03:37:11] Energy consumed for all CPUs : 0.008857 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:37:11] 0.022417 kWh of electricity used since the beginning.\n",
            "Epoch 11/25 - Training:  56%|█████▋    | 353/625 [00:36<00:25, 10.85it/s][codecarbon INFO @ 03:37:26] Energy consumed for RAM : 0.001009 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:37:26] Energy consumed for all GPUs : 0.012839 kWh. Total GPU Power : 64.36282670474357 W\n",
            "[codecarbon INFO @ 03:37:26] Energy consumed for all CPUs : 0.009033 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:37:26] 0.022881 kWh of electricity used since the beginning.\n",
            "Epoch 11/25 - Training:  81%|████████  | 504/625 [00:51<00:10, 11.11it/s][codecarbon INFO @ 03:37:41] Energy consumed for RAM : 0.001029 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:37:41] Energy consumed for all GPUs : 0.013107 kWh. Total GPU Power : 64.42125782146175 W\n",
            "[codecarbon INFO @ 03:37:41] Energy consumed for all CPUs : 0.009211 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:37:41] 0.023346 kWh of electricity used since the beginning.\n",
            "Epoch 11/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.90it/s]\n",
            "Epoch 11/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 11/25 - Validation:  29%|██▉       | 46/157 [00:03<00:06, 16.96it/s][codecarbon INFO @ 03:37:56] Energy consumed for RAM : 0.001049 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:37:56] Energy consumed for all GPUs : 0.013362 kWh. Total GPU Power : 61.266993437762665 W\n",
            "[codecarbon INFO @ 03:37:56] Energy consumed for all CPUs : 0.009388 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:37:56] 0.023799 kWh of electricity used since the beginning.\n",
            "Epoch 11/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/25], Train Loss: 0.4190, Train Accuracy: 86.45%\n",
            "Epoch [11/25], Val Loss: 0.5320, Val Accuracy: 82.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/25 - Training:  13%|█▎        | 79/625 [00:08<00:49, 11.09it/s][codecarbon INFO @ 03:38:11] Energy consumed for RAM : 0.001069 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:38:11] Energy consumed for all GPUs : 0.013589 kWh. Total GPU Power : 54.5034770565602 W\n",
            "[codecarbon INFO @ 03:38:11] Energy consumed for all CPUs : 0.009565 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:38:11] 0.024223 kWh of electricity used since the beginning.\n",
            "Epoch 12/25 - Training:  37%|███▋      | 230/625 [00:23<00:36, 10.95it/s][codecarbon INFO @ 03:38:26] Energy consumed for RAM : 0.001088 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:38:26] Energy consumed for all GPUs : 0.013858 kWh. Total GPU Power : 64.50249692542862 W\n",
            "[codecarbon INFO @ 03:38:26] Energy consumed for all CPUs : 0.009742 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:38:26] 0.024689 kWh of electricity used since the beginning.\n",
            "Epoch 12/25 - Training:  61%|██████    | 382/625 [00:38<00:22, 10.99it/s][codecarbon INFO @ 03:38:41] Energy consumed for RAM : 0.001108 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:38:41] Energy consumed for all GPUs : 0.014126 kWh. Total GPU Power : 64.28151185829212 W\n",
            "[codecarbon INFO @ 03:38:41] Energy consumed for all CPUs : 0.009919 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:38:41] 0.025153 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:38:41] 0.010508 g.CO2eq/s mean an estimation of 331.3953067764649 kg.CO2eq/year\n",
            "Epoch 12/25 - Training:  86%|████████▌ | 536/625 [00:54<00:08, 10.74it/s][codecarbon INFO @ 03:38:56] Energy consumed for RAM : 0.001128 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:38:56] Energy consumed for all GPUs : 0.014394 kWh. Total GPU Power : 64.28540279929537 W\n",
            "[codecarbon INFO @ 03:38:56] Energy consumed for all CPUs : 0.010096 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:38:56] 0.025618 kWh of electricity used since the beginning.\n",
            "Epoch 12/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.91it/s]\n",
            "Epoch 12/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 12/25 - Validation:  62%|██████▏   | 98/157 [00:05<00:03, 17.09it/s][codecarbon INFO @ 03:39:11] Energy consumed for RAM : 0.001148 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:39:11] Energy consumed for all GPUs : 0.014633 kWh. Total GPU Power : 57.33727587121792 W\n",
            "[codecarbon INFO @ 03:39:12] Energy consumed for all CPUs : 0.010274 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:39:12] 0.026054 kWh of electricity used since the beginning.\n",
            "Epoch 12/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/25], Train Loss: 0.3970, Train Accuracy: 87.06%\n",
            "Epoch [12/25], Val Loss: 0.4832, Val Accuracy: 84.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/25 - Training:  18%|█▊        | 111/625 [00:11<00:46, 11.01it/s][codecarbon INFO @ 03:39:26] Energy consumed for RAM : 0.001168 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:39:27] Energy consumed for all GPUs : 0.014877 kWh. Total GPU Power : 58.65909264432293 W\n",
            "[codecarbon INFO @ 03:39:27] Energy consumed for all CPUs : 0.010451 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:39:27] 0.026495 kWh of electricity used since the beginning.\n",
            "Epoch 13/25 - Training:  42%|████▏     | 262/625 [00:26<00:32, 11.00it/s][codecarbon INFO @ 03:39:42] Energy consumed for RAM : 0.001187 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:39:42] Energy consumed for all GPUs : 0.015145 kWh. Total GPU Power : 64.1900619408972 W\n",
            "[codecarbon INFO @ 03:39:42] Energy consumed for all CPUs : 0.010628 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:39:42] 0.026960 kWh of electricity used since the beginning.\n",
            "Epoch 13/25 - Training:  66%|██████▌   | 413/625 [00:41<00:19, 10.87it/s][codecarbon INFO @ 03:39:57] Energy consumed for RAM : 0.001207 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:39:57] Energy consumed for all GPUs : 0.015415 kWh. Total GPU Power : 65.00176728188951 W\n",
            "[codecarbon INFO @ 03:39:57] Energy consumed for all CPUs : 0.010805 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:39:57] 0.027427 kWh of electricity used since the beginning.\n",
            "Epoch 13/25 - Training:  91%|█████████ | 567/625 [00:56<00:05, 11.29it/s][codecarbon INFO @ 03:40:12] Energy consumed for RAM : 0.001227 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:40:12] Energy consumed for all GPUs : 0.015685 kWh. Total GPU Power : 64.75971350640663 W\n",
            "[codecarbon INFO @ 03:40:12] Energy consumed for all CPUs : 0.010982 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:40:12] 0.027894 kWh of electricity used since the beginning.\n",
            "Epoch 13/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.90it/s]\n",
            "Epoch 13/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 13/25 - Validation:  96%|█████████▌| 150/157 [00:08<00:00, 16.50it/s][codecarbon INFO @ 03:40:27] Energy consumed for RAM : 0.001247 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:40:27] Energy consumed for all GPUs : 0.015910 kWh. Total GPU Power : 54.03981134671976 W\n",
            "[codecarbon INFO @ 03:40:27] Energy consumed for all CPUs : 0.011159 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:40:27] 0.028316 kWh of electricity used since the beginning.\n",
            "Epoch 13/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/25], Train Loss: 0.3980, Train Accuracy: 86.88%\n",
            "Epoch [13/25], Val Loss: 0.5598, Val Accuracy: 81.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/25 - Training:  23%|██▎       | 143/625 [00:14<00:43, 11.06it/s][codecarbon INFO @ 03:40:42] Energy consumed for RAM : 0.001266 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:40:42] Energy consumed for all GPUs : 0.016176 kWh. Total GPU Power : 63.82619467246095 W\n",
            "[codecarbon INFO @ 03:40:42] Energy consumed for all CPUs : 0.011336 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:40:42] 0.028779 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:40:42] 0.010542 g.CO2eq/s mean an estimation of 332.46635496222893 kg.CO2eq/year\n",
            "Epoch 14/25 - Training:  47%|████▋     | 295/625 [00:29<00:31, 10.53it/s][codecarbon INFO @ 03:40:57] Energy consumed for RAM : 0.001286 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:40:57] Energy consumed for all GPUs : 0.016449 kWh. Total GPU Power : 65.4364597703328 W\n",
            "[codecarbon INFO @ 03:40:57] Energy consumed for all CPUs : 0.011514 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:40:57] 0.029249 kWh of electricity used since the beginning.\n",
            "Epoch 14/25 - Training:  72%|███████▏  | 448/625 [00:44<00:16, 10.77it/s][codecarbon INFO @ 03:41:12] Energy consumed for RAM : 0.001306 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:41:12] Energy consumed for all GPUs : 0.016721 kWh. Total GPU Power : 65.31001264220168 W\n",
            "[codecarbon INFO @ 03:41:12] Energy consumed for all CPUs : 0.011691 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:41:12] 0.029718 kWh of electricity used since the beginning.\n",
            "Epoch 14/25 - Training:  96%|█████████▌| 599/625 [00:59<00:02,  8.80it/s][codecarbon INFO @ 03:41:27] Energy consumed for RAM : 0.001326 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:41:27] Energy consumed for all GPUs : 0.016990 kWh. Total GPU Power : 64.87958562628482 W\n",
            "[codecarbon INFO @ 03:41:27] Energy consumed for all CPUs : 0.011868 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:41:27] 0.030184 kWh of electricity used since the beginning.\n",
            "Epoch 14/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.96it/s]\n",
            "Epoch 14/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 14/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/25], Train Loss: 0.3910, Train Accuracy: 87.23%\n",
            "Epoch [14/25], Val Loss: 0.4500, Val Accuracy: 85.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/25 - Training:   4%|▎         | 22/625 [00:02<01:13,  8.21it/s][codecarbon INFO @ 03:41:42] Energy consumed for RAM : 0.001346 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:41:42] Energy consumed for all GPUs : 0.017206 kWh. Total GPU Power : 51.740349500619985 W\n",
            "[codecarbon INFO @ 03:41:42] Energy consumed for all CPUs : 0.012045 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:41:42] 0.030597 kWh of electricity used since the beginning.\n",
            "Epoch 15/25 - Training:  28%|██▊       | 172/625 [00:17<00:56,  8.02it/s][codecarbon INFO @ 03:41:57] Energy consumed for RAM : 0.001365 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:41:57] Energy consumed for all GPUs : 0.017474 kWh. Total GPU Power : 64.57802132131674 W\n",
            "[codecarbon INFO @ 03:41:57] Energy consumed for all CPUs : 0.012222 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:41:57] 0.031062 kWh of electricity used since the beginning.\n",
            "Epoch 15/25 - Training:  51%|█████▏    | 321/625 [00:32<00:35,  8.45it/s][codecarbon INFO @ 03:42:12] Energy consumed for RAM : 0.001385 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:42:12] Energy consumed for all GPUs : 0.017746 kWh. Total GPU Power : 65.34056625239259 W\n",
            "[codecarbon INFO @ 03:42:12] Energy consumed for all CPUs : 0.012399 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:42:12] 0.031531 kWh of electricity used since the beginning.\n",
            "Epoch 15/25 - Training:  76%|███████▌  | 473/625 [00:47<00:19,  7.69it/s][codecarbon INFO @ 03:42:27] Energy consumed for RAM : 0.001405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:42:27] Energy consumed for all GPUs : 0.018018 kWh. Total GPU Power : 65.21304057957752 W\n",
            "[codecarbon INFO @ 03:42:27] Energy consumed for all CPUs : 0.012576 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:42:27] 0.031999 kWh of electricity used since the beginning.\n",
            "Epoch 15/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.94it/s]\n",
            "Epoch 15/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:42:42] Energy consumed for RAM : 0.001425 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:42:42] Energy consumed for all GPUs : 0.018289 kWh. Total GPU Power : 64.76688736178261 W\n",
            "[codecarbon INFO @ 03:42:42] Energy consumed for all CPUs : 0.012754 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:42:42] 0.032467 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:42:42] 0.010719 g.CO2eq/s mean an estimation of 338.03541168542824 kg.CO2eq/year\n",
            "Epoch 15/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/25], Train Loss: 0.3645, Train Accuracy: 87.95%\n",
            "Epoch [15/25], Val Loss: 0.5027, Val Accuracy: 83.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/25 - Training:   8%|▊         | 47/625 [00:05<00:58,  9.84it/s][codecarbon INFO @ 03:42:57] Energy consumed for RAM : 0.001445 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:42:57] Energy consumed for all GPUs : 0.018506 kWh. Total GPU Power : 52.147527797487875 W\n",
            "[codecarbon INFO @ 03:42:57] Energy consumed for all CPUs : 0.012931 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 16/25 - Training:   8%|▊         | 49/625 [00:06<00:57,  9.99it/s][codecarbon INFO @ 03:42:57] 0.032881 kWh of electricity used since the beginning.\n",
            "Epoch 16/25 - Training:  32%|███▏      | 200/625 [00:21<00:39, 10.83it/s][codecarbon INFO @ 03:43:12] Energy consumed for RAM : 0.001464 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:43:12] Energy consumed for all GPUs : 0.018775 kWh. Total GPU Power : 64.83513591873836 W\n",
            "[codecarbon INFO @ 03:43:12] Energy consumed for all CPUs : 0.013108 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:43:12] 0.033347 kWh of electricity used since the beginning.\n",
            "Epoch 16/25 - Training:  56%|█████▋    | 352/625 [00:36<00:25, 10.75it/s][codecarbon INFO @ 03:43:27] Energy consumed for RAM : 0.001484 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:43:27] Energy consumed for all GPUs : 0.019048 kWh. Total GPU Power : 65.52088346324517 W\n",
            "[codecarbon INFO @ 03:43:27] Energy consumed for all CPUs : 0.013285 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:43:27] 0.033817 kWh of electricity used since the beginning.\n",
            "Epoch 16/25 - Training:  81%|████████  | 504/625 [00:50<00:10, 11.13it/s][codecarbon INFO @ 03:43:42] Energy consumed for RAM : 0.001504 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:43:42] Energy consumed for all GPUs : 0.019321 kWh. Total GPU Power : 65.34924560370452 W\n",
            "[codecarbon INFO @ 03:43:42] Energy consumed for all CPUs : 0.013462 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 16/25 - Training:  81%|████████  | 506/625 [00:51<00:10, 11.11it/s][codecarbon INFO @ 03:43:42] 0.034286 kWh of electricity used since the beginning.\n",
            "Epoch 16/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.97it/s]\n",
            "Epoch 16/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 16/25 - Validation:  32%|███▏      | 50/157 [00:03<00:06, 16.85it/s][codecarbon INFO @ 03:43:57] Energy consumed for RAM : 0.001524 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:43:57] Energy consumed for all GPUs : 0.019579 kWh. Total GPU Power : 62.13436045734197 W\n",
            "[codecarbon INFO @ 03:43:57] Energy consumed for all CPUs : 0.013639 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:43:57] 0.034742 kWh of electricity used since the beginning.\n",
            "Epoch 16/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/25], Train Loss: 0.3609, Train Accuracy: 88.24%\n",
            "Epoch [16/25], Val Loss: 0.4580, Val Accuracy: 84.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/25 - Training:  13%|█▎        | 82/625 [00:09<00:48, 11.22it/s][codecarbon INFO @ 03:44:12] Energy consumed for RAM : 0.001543 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:44:12] Energy consumed for all GPUs : 0.019812 kWh. Total GPU Power : 55.76287795199609 W\n",
            "[codecarbon INFO @ 03:44:12] Energy consumed for all CPUs : 0.013816 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:44:12] 0.035171 kWh of electricity used since the beginning.\n",
            "Epoch 17/25 - Training:  37%|███▋      | 234/625 [00:24<00:35, 10.99it/s][codecarbon INFO @ 03:44:27] Energy consumed for RAM : 0.001563 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:44:27] Energy consumed for all GPUs : 0.020083 kWh. Total GPU Power : 65.21834723447327 W\n",
            "[codecarbon INFO @ 03:44:27] Energy consumed for all CPUs : 0.013993 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:44:27] 0.035640 kWh of electricity used since the beginning.\n",
            "Epoch 17/25 - Training:  61%|██████▏   | 383/625 [00:39<00:22, 10.69it/s][codecarbon INFO @ 03:44:42] Energy consumed for RAM : 0.001583 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:44:42] Energy consumed for all GPUs : 0.020355 kWh. Total GPU Power : 65.20633573336083 W\n",
            "[codecarbon INFO @ 03:44:42] Energy consumed for all CPUs : 0.014170 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:44:42] 0.036108 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:44:42] 0.010593 g.CO2eq/s mean an estimation of 334.06498589610777 kg.CO2eq/year\n",
            "Epoch 17/25 - Training:  85%|████████▌ | 533/625 [00:54<00:08, 10.94it/s][codecarbon INFO @ 03:44:57] Energy consumed for RAM : 0.001603 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:44:57] Energy consumed for all GPUs : 0.020627 kWh. Total GPU Power : 65.29267615735566 W\n",
            "[codecarbon INFO @ 03:44:57] Energy consumed for all CPUs : 0.014348 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:44:57] 0.036578 kWh of electricity used since the beginning.\n",
            "Epoch 17/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.85it/s]\n",
            "Epoch 17/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 17/25 - Validation:  62%|██████▏   | 97/157 [00:05<00:02, 20.36it/s][codecarbon INFO @ 03:45:12] Energy consumed for RAM : 0.001623 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:45:12] Energy consumed for all GPUs : 0.020872 kWh. Total GPU Power : 58.64250277327011 W\n",
            "[codecarbon INFO @ 03:45:12] Energy consumed for all CPUs : 0.014525 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:45:12] 0.037019 kWh of electricity used since the beginning.\n",
            "Epoch 17/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/25], Train Loss: 0.3510, Train Accuracy: 88.42%\n",
            "Epoch [17/25], Val Loss: 0.4937, Val Accuracy: 84.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/25 - Training:  17%|█▋        | 105/625 [00:11<00:48, 10.67it/s][codecarbon INFO @ 03:45:27] Energy consumed for RAM : 0.001642 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:45:27] Energy consumed for all GPUs : 0.021114 kWh. Total GPU Power : 58.26828983812964 W\n",
            "[codecarbon INFO @ 03:45:27] Energy consumed for all CPUs : 0.014702 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:45:27] 0.037458 kWh of electricity used since the beginning.\n",
            "Epoch 18/25 - Training:  40%|████      | 252/625 [00:26<00:34, 10.86it/s][codecarbon INFO @ 03:45:42] Energy consumed for RAM : 0.001662 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:45:42] Energy consumed for all GPUs : 0.021384 kWh. Total GPU Power : 64.8587916288121 W\n",
            "[codecarbon INFO @ 03:45:42] Energy consumed for all CPUs : 0.014879 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:45:42] 0.037925 kWh of electricity used since the beginning.\n",
            "Epoch 18/25 - Training:  64%|██████▍   | 403/625 [00:41<00:20, 10.84it/s][codecarbon INFO @ 03:45:57] Energy consumed for RAM : 0.001682 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:45:57] Energy consumed for all GPUs : 0.021657 kWh. Total GPU Power : 65.47211359012185 W\n",
            "[codecarbon INFO @ 03:45:57] Energy consumed for all CPUs : 0.015056 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:45:57] 0.038395 kWh of electricity used since the beginning.\n",
            "Epoch 18/25 - Training:  89%|████████▉ | 555/625 [00:56<00:06, 10.93it/s][codecarbon INFO @ 03:46:12] Energy consumed for RAM : 0.001702 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:46:12] Energy consumed for all GPUs : 0.021930 kWh. Total GPU Power : 65.46782819818246 W\n",
            "[codecarbon INFO @ 03:46:12] Energy consumed for all CPUs : 0.015233 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:46:12] 0.038865 kWh of electricity used since the beginning.\n",
            "Epoch 18/25 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.73it/s]\n",
            "Epoch 18/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 18/25 - Validation:  78%|███████▊  | 123/157 [00:07<00:01, 18.51it/s][codecarbon INFO @ 03:46:27] Energy consumed for RAM : 0.001722 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:46:27] Energy consumed for all GPUs : 0.022165 kWh. Total GPU Power : 56.50384104322057 W\n",
            "[codecarbon INFO @ 03:46:27] Energy consumed for all CPUs : 0.015410 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:46:27] 0.039297 kWh of electricity used since the beginning.\n",
            "Epoch 18/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/25], Train Loss: 0.3349, Train Accuracy: 88.99%\n",
            "Epoch [18/25], Val Loss: 0.4448, Val Accuracy: 85.23%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/25 - Training:  20%|██        | 125/625 [00:13<00:46, 10.75it/s][codecarbon INFO @ 03:46:42] Energy consumed for RAM : 0.001741 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:46:42] Energy consumed for all GPUs : 0.022416 kWh. Total GPU Power : 60.20462060585039 W\n",
            "[codecarbon INFO @ 03:46:42] Energy consumed for all CPUs : 0.015587 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:46:42] 0.039744 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:46:42] 0.010575 g.CO2eq/s mean an estimation of 333.49991125889443 kg.CO2eq/year\n",
            "Epoch 19/25 - Training:  44%|████▍     | 277/625 [00:28<00:31, 11.05it/s][codecarbon INFO @ 03:46:57] Energy consumed for RAM : 0.001761 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:46:57] Energy consumed for all GPUs : 0.022688 kWh. Total GPU Power : 65.28348567775937 W\n",
            "[codecarbon INFO @ 03:46:57] Energy consumed for all CPUs : 0.015764 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:46:57] 0.040213 kWh of electricity used since the beginning.\n",
            "Epoch 19/25 - Training:  68%|██████▊   | 426/625 [00:43<00:18, 10.94it/s][codecarbon INFO @ 03:47:12] Energy consumed for RAM : 0.001781 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:47:12] Energy consumed for all GPUs : 0.022959 kWh. Total GPU Power : 65.13321109088683 W\n",
            "[codecarbon INFO @ 03:47:12] Energy consumed for all CPUs : 0.015941 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:47:12] 0.040682 kWh of electricity used since the beginning.\n",
            "Epoch 19/25 - Training:  92%|█████████▏| 577/625 [00:58<00:04, 10.96it/s][codecarbon INFO @ 03:47:27] Energy consumed for RAM : 0.001801 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:47:27] Energy consumed for all GPUs : 0.023230 kWh. Total GPU Power : 65.19056964598695 W\n",
            "[codecarbon INFO @ 03:47:27] Energy consumed for all CPUs : 0.016118 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:47:27] 0.041150 kWh of electricity used since the beginning.\n",
            "Epoch 19/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.80it/s]\n",
            "Epoch 19/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 19/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 17.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/25], Train Loss: 0.3404, Train Accuracy: 88.87%\n",
            "Epoch [19/25], Val Loss: 0.4714, Val Accuracy: 84.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 20/25 - Training:   0%|          | 0/625 [00:00<?, ?it/s][codecarbon INFO @ 03:47:42] Energy consumed for RAM : 0.001821 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:47:42] Energy consumed for all GPUs : 0.023452 kWh. Total GPU Power : 53.18147936622101 W\n",
            "[codecarbon INFO @ 03:47:42] Energy consumed for all CPUs : 0.016295 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:47:42] 0.041568 kWh of electricity used since the beginning.\n",
            "Epoch 20/25 - Training:  23%|██▎       | 144/625 [00:15<00:57,  8.37it/s][codecarbon INFO @ 03:47:57] Energy consumed for RAM : 0.001840 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:47:57] Energy consumed for all GPUs : 0.023713 kWh. Total GPU Power : 62.76456623519963 W\n",
            "[codecarbon INFO @ 03:47:57] Energy consumed for all CPUs : 0.016473 kWh. Total CPU Power : 42.5 W\n",
            "Epoch 20/25 - Training:  23%|██▎       | 145/625 [00:15<01:02,  7.70it/s][codecarbon INFO @ 03:47:57] 0.042027 kWh of electricity used since the beginning.\n",
            "Epoch 20/25 - Training:  47%|████▋     | 293/625 [00:30<00:40,  8.12it/s][codecarbon INFO @ 03:48:12] Energy consumed for RAM : 0.001860 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:48:12] Energy consumed for all GPUs : 0.023984 kWh. Total GPU Power : 65.07957762483338 W\n",
            "[codecarbon INFO @ 03:48:12] Energy consumed for all CPUs : 0.016650 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:48:12] 0.042494 kWh of electricity used since the beginning.\n",
            "Epoch 20/25 - Training:  71%|███████   | 441/625 [00:45<00:22,  8.25it/s][codecarbon INFO @ 03:48:27] Energy consumed for RAM : 0.001880 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 20/25 - Training:  71%|███████   | 442/625 [00:45<00:21,  8.64it/s][codecarbon INFO @ 03:48:27] Energy consumed for all GPUs : 0.024255 kWh. Total GPU Power : 65.15779993660958 W\n",
            "[codecarbon INFO @ 03:48:27] Energy consumed for all CPUs : 0.016827 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:48:27] 0.042962 kWh of electricity used since the beginning.\n",
            "Epoch 20/25 - Training:  94%|█████████▍| 586/625 [01:00<00:04,  8.15it/s][codecarbon INFO @ 03:48:42] Energy consumed for RAM : 0.001900 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:48:42] Energy consumed for all GPUs : 0.024525 kWh. Total GPU Power : 64.69904949442241 W\n",
            "[codecarbon INFO @ 03:48:42] Energy consumed for all CPUs : 0.017004 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:48:42] 0.043428 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:48:42] 0.010715 g.CO2eq/s mean an estimation of 337.90200249155623 kg.CO2eq/year\n",
            "Epoch 20/25 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.67it/s]\n",
            "Epoch 20/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 20/25 - Validation: 100%|██████████| 157/157 [00:09<00:00, 16.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/25], Train Loss: 0.3371, Train Accuracy: 89.01%\n",
            "Epoch [20/25], Val Loss: 0.4596, Val Accuracy: 84.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/25 - Training:   0%|          | 2/625 [00:00<03:56,  2.64it/s][codecarbon INFO @ 03:48:57] Energy consumed for RAM : 0.001919 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:48:57] Energy consumed for all GPUs : 0.024735 kWh. Total GPU Power : 50.564203230693956 W\n",
            "[codecarbon INFO @ 03:48:57] Energy consumed for all CPUs : 0.017181 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:48:57] 0.043835 kWh of electricity used since the beginning.\n",
            "Epoch 21/25 - Training:  24%|██▍       | 149/625 [00:15<00:58,  8.14it/s][codecarbon INFO @ 03:49:12] Energy consumed for RAM : 0.001939 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:49:12] Energy consumed for all GPUs : 0.025003 kWh. Total GPU Power : 64.17805315014077 W\n",
            "[codecarbon INFO @ 03:49:12] Energy consumed for all CPUs : 0.017358 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:49:12] 0.044301 kWh of electricity used since the beginning.\n",
            "Epoch 21/25 - Training:  48%|████▊     | 297/625 [00:30<00:40,  8.01it/s][codecarbon INFO @ 03:49:27] Energy consumed for RAM : 0.001959 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:49:27] Energy consumed for all GPUs : 0.025274 kWh. Total GPU Power : 65.2269412336386 W\n",
            "[codecarbon INFO @ 03:49:27] Energy consumed for all CPUs : 0.017535 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:49:27] 0.044769 kWh of electricity used since the beginning.\n",
            "Epoch 21/25 - Training:  71%|███████   | 443/625 [00:45<00:24,  7.40it/s][codecarbon INFO @ 03:49:42] Energy consumed for RAM : 0.001979 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:49:42] Energy consumed for all GPUs : 0.025546 kWh. Total GPU Power : 65.2061886662645 W\n",
            "[codecarbon INFO @ 03:49:42] Energy consumed for all CPUs : 0.017712 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:49:42] 0.045238 kWh of electricity used since the beginning.\n",
            "Epoch 21/25 - Training:  94%|█████████▍| 590/625 [01:01<00:05,  6.99it/s][codecarbon INFO @ 03:49:57] Energy consumed for RAM : 0.001999 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:49:57] Energy consumed for all GPUs : 0.025816 kWh. Total GPU Power : 64.70775572499852 W\n",
            "[codecarbon INFO @ 03:49:57] Energy consumed for all CPUs : 0.017890 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:49:57] 0.045705 kWh of electricity used since the beginning.\n",
            "Epoch 21/25 - Training: 100%|██████████| 625/625 [01:04<00:00,  9.68it/s]\n",
            "Epoch 21/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 21/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 15.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [21/25], Train Loss: 0.3243, Train Accuracy: 89.43%\n",
            "Epoch [21/25], Val Loss: 0.4610, Val Accuracy: 85.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/25 - Training:   1%|          | 5/625 [00:01<01:39,  6.21it/s][codecarbon INFO @ 03:50:12] Energy consumed for RAM : 0.002019 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:50:12] Energy consumed for all GPUs : 0.026024 kWh. Total GPU Power : 49.73574855606552 W\n",
            "[codecarbon INFO @ 03:50:12] Energy consumed for all CPUs : 0.018067 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:50:12] 0.046110 kWh of electricity used since the beginning.\n",
            "Epoch 22/25 - Training:  25%|██▍       | 154/625 [00:16<00:44, 10.51it/s][codecarbon INFO @ 03:50:27] Energy consumed for RAM : 0.002038 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:50:27] Energy consumed for all GPUs : 0.026296 kWh. Total GPU Power : 65.3321060105756 W\n",
            "[codecarbon INFO @ 03:50:27] Energy consumed for all CPUs : 0.018244 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:50:27] 0.046579 kWh of electricity used since the beginning.\n",
            "Epoch 22/25 - Training:  49%|████▊     | 304/625 [00:31<00:30, 10.62it/s][codecarbon INFO @ 03:50:42] Energy consumed for RAM : 0.002058 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:50:42] Energy consumed for all GPUs : 0.026567 kWh. Total GPU Power : 65.04405452376197 W\n",
            "[codecarbon INFO @ 03:50:42] Energy consumed for all CPUs : 0.018422 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:50:42] 0.047047 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:50:42] 0.010517 g.CO2eq/s mean an estimation of 331.6593734604223 kg.CO2eq/year\n",
            "Epoch 22/25 - Training:  73%|███████▎  | 454/625 [00:46<00:15, 10.73it/s][codecarbon INFO @ 03:50:57] Energy consumed for RAM : 0.002078 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:50:57] Energy consumed for all GPUs : 0.026839 kWh. Total GPU Power : 65.36485131343551 W\n",
            "[codecarbon INFO @ 03:50:57] Energy consumed for all CPUs : 0.018599 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:50:57] 0.047516 kWh of electricity used since the beginning.\n",
            "Epoch 22/25 - Training:  97%|█████████▋| 604/625 [01:01<00:01, 10.96it/s][codecarbon INFO @ 03:51:12] Energy consumed for RAM : 0.002098 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:51:12] Energy consumed for all GPUs : 0.027110 kWh. Total GPU Power : 64.97630662575645 W\n",
            "[codecarbon INFO @ 03:51:12] Energy consumed for all CPUs : 0.018776 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:51:12] 0.047984 kWh of electricity used since the beginning.\n",
            "Epoch 22/25 - Training: 100%|██████████| 625/625 [01:03<00:00,  9.90it/s]\n",
            "Epoch 22/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 22/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [22/25], Train Loss: 0.3169, Train Accuracy: 89.58%\n",
            "Epoch [22/25], Val Loss: 0.4740, Val Accuracy: 85.17%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/25 - Training:   3%|▎         | 20/625 [00:02<00:55, 10.83it/s][codecarbon INFO @ 03:51:27] Energy consumed for RAM : 0.002117 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:51:27] Energy consumed for all GPUs : 0.027318 kWh. Total GPU Power : 50.0284504721611 W\n",
            "[codecarbon INFO @ 03:51:27] Energy consumed for all CPUs : 0.018953 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:51:27] 0.048389 kWh of electricity used since the beginning.\n",
            "Epoch 23/25 - Training:  27%|██▋       | 170/625 [00:17<00:42, 10.63it/s][codecarbon INFO @ 03:51:42] Energy consumed for RAM : 0.002137 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:51:42] Energy consumed for all GPUs : 0.027588 kWh. Total GPU Power : 64.71770907041378 W\n",
            "[codecarbon INFO @ 03:51:42] Energy consumed for all CPUs : 0.019130 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:51:42] 0.048855 kWh of electricity used since the beginning.\n",
            "Epoch 23/25 - Training:  51%|█████     | 320/625 [00:32<00:27, 11.03it/s][codecarbon INFO @ 03:51:57] Energy consumed for RAM : 0.002157 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:51:57] Energy consumed for all GPUs : 0.027861 kWh. Total GPU Power : 65.42233348087393 W\n",
            "[codecarbon INFO @ 03:51:57] Energy consumed for all CPUs : 0.019307 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:51:57] 0.049325 kWh of electricity used since the beginning.\n",
            "Epoch 23/25 - Training:  75%|███████▌  | 470/625 [00:47<00:14, 10.96it/s][codecarbon INFO @ 03:52:12] Energy consumed for RAM : 0.002177 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:52:12] Energy consumed for all GPUs : 0.028132 kWh. Total GPU Power : 65.19868601685319 W\n",
            "[codecarbon INFO @ 03:52:12] Energy consumed for all CPUs : 0.019484 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:52:12] 0.049793 kWh of electricity used since the beginning.\n",
            "Epoch 23/25 - Training:  99%|█████████▉| 621/625 [01:02<00:00, 11.35it/s][codecarbon INFO @ 03:52:27] Energy consumed for RAM : 0.002197 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 23/25 - Training: 100%|█████████▉| 623/625 [01:02<00:00, 11.66it/s][codecarbon INFO @ 03:52:27] Energy consumed for all GPUs : 0.028404 kWh. Total GPU Power : 65.38948666765425 W\n",
            "[codecarbon INFO @ 03:52:27] Energy consumed for all CPUs : 0.019661 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:52:27] 0.050262 kWh of electricity used since the beginning.\n",
            "Epoch 23/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.99it/s]\n",
            "Epoch 23/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 23/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [23/25], Train Loss: 0.1905, Train Accuracy: 93.61%\n",
            "Epoch [23/25], Val Loss: 0.3435, Val Accuracy: 89.07%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/25 - Training:   6%|▌         | 36/625 [00:03<00:53, 11.05it/s][codecarbon INFO @ 03:52:42] Energy consumed for RAM : 0.002216 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:52:42] Energy consumed for all GPUs : 0.028608 kWh. Total GPU Power : 49.008633671762794 W\n",
            "[codecarbon INFO @ 03:52:42] Energy consumed for all CPUs : 0.019838 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:52:42] 0.050663 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:52:42] 0.010518 g.CO2eq/s mean an estimation of 331.68648771941275 kg.CO2eq/year\n",
            "Epoch 24/25 - Training:  30%|██▉       | 185/625 [00:18<00:41, 10.55it/s][codecarbon INFO @ 03:52:57] Energy consumed for RAM : 0.002236 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:52:57] Energy consumed for all GPUs : 0.028878 kWh. Total GPU Power : 64.74954774069738 W\n",
            "[codecarbon INFO @ 03:52:57] Energy consumed for all CPUs : 0.020015 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:52:57] 0.051129 kWh of electricity used since the beginning.\n",
            "Epoch 24/25 - Training:  53%|█████▎    | 334/625 [00:33<00:26, 10.81it/s][codecarbon INFO @ 03:53:12] Energy consumed for RAM : 0.002256 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:53:12] Energy consumed for all GPUs : 0.029149 kWh. Total GPU Power : 65.30542425800876 W\n",
            "[codecarbon INFO @ 03:53:12] Energy consumed for all CPUs : 0.020192 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:53:12] 0.051598 kWh of electricity used since the beginning.\n",
            "Epoch 24/25 - Training:  78%|███████▊  | 485/625 [00:48<00:12, 11.02it/s][codecarbon INFO @ 03:53:27] Energy consumed for RAM : 0.002276 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:53:27] Energy consumed for all GPUs : 0.029422 kWh. Total GPU Power : 65.22749086279455 W\n",
            "[codecarbon INFO @ 03:53:27] Energy consumed for all CPUs : 0.020370 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:53:27] 0.052067 kWh of electricity used since the beginning.\n",
            "Epoch 24/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.96it/s]\n",
            "Epoch 24/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 24/25 - Validation:   7%|▋         | 11/157 [00:00<00:09, 14.66it/s][codecarbon INFO @ 03:53:42] Energy consumed for RAM : 0.002296 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:53:42] Energy consumed for all GPUs : 0.029687 kWh. Total GPU Power : 63.710414157248266 W\n",
            "[codecarbon INFO @ 03:53:42] Energy consumed for all CPUs : 0.020547 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:53:42] 0.052530 kWh of electricity used since the beginning.\n",
            "Epoch 24/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [24/25], Train Loss: 0.1556, Train Accuracy: 94.83%\n",
            "Epoch [24/25], Val Loss: 0.3365, Val Accuracy: 89.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/25 - Training:   8%|▊         | 51/625 [00:05<00:52, 11.01it/s][codecarbon INFO @ 03:53:57] Energy consumed for RAM : 0.002315 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:53:57] Energy consumed for all GPUs : 0.029894 kWh. Total GPU Power : 49.79539689484897 W\n",
            "[codecarbon INFO @ 03:53:57] Energy consumed for all CPUs : 0.020724 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:53:57] 0.052934 kWh of electricity used since the beginning.\n",
            "Epoch 25/25 - Training:  32%|███▏      | 200/625 [00:20<00:37, 11.27it/s][codecarbon INFO @ 03:54:12] Energy consumed for RAM : 0.002335 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:54:12] Energy consumed for all GPUs : 0.030165 kWh. Total GPU Power : 64.99333825412349 W\n",
            "[codecarbon INFO @ 03:54:12] Energy consumed for all CPUs : 0.020901 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:54:12] 0.053402 kWh of electricity used since the beginning.\n",
            "Epoch 25/25 - Training:  56%|█████▌    | 351/625 [00:35<00:24, 10.99it/s][codecarbon INFO @ 03:54:27] Energy consumed for RAM : 0.002355 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:54:27] Energy consumed for all GPUs : 0.030436 kWh. Total GPU Power : 65.14598078108088 W\n",
            "[codecarbon INFO @ 03:54:27] Energy consumed for all CPUs : 0.021078 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:54:27] 0.053870 kWh of electricity used since the beginning.\n",
            "Epoch 25/25 - Training:  80%|████████  | 502/625 [00:50<00:11, 10.87it/s][codecarbon INFO @ 03:54:42] Energy consumed for RAM : 0.002375 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:54:42] Energy consumed for all GPUs : 0.030709 kWh. Total GPU Power : 65.35489240095278 W\n",
            "[codecarbon INFO @ 03:54:42] Energy consumed for all CPUs : 0.021256 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:54:42] 0.054339 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:54:42] 0.010690 g.CO2eq/s mean an estimation of 337.1111420195946 kg.CO2eq/year\n",
            "Epoch 25/25 - Training: 100%|██████████| 625/625 [01:02<00:00,  9.99it/s]\n",
            "Epoch 25/25 - Validation:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 25/25 - Validation:  24%|██▎       | 37/157 [00:02<00:07, 16.61it/s][codecarbon INFO @ 03:54:57] Energy consumed for RAM : 0.002395 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 25/25 - Validation:  26%|██▌       | 41/157 [00:02<00:06, 17.84it/s][codecarbon INFO @ 03:54:57] Energy consumed for all GPUs : 0.030970 kWh. Total GPU Power : 62.67189726544253 W\n",
            "[codecarbon INFO @ 03:54:57] Energy consumed for all CPUs : 0.021433 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:54:57] 0.054797 kWh of electricity used since the beginning.\n",
            "Epoch 25/25 - Validation: 100%|██████████| 157/157 [00:10<00:00, 14.46it/s]\n",
            "[codecarbon INFO @ 03:55:05] Energy consumed for RAM : 0.002405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:55:05] Energy consumed for all GPUs : 0.031065 kWh. Total GPU Power : 41.98339440956246 W\n",
            "[codecarbon INFO @ 03:55:05] Energy consumed for all CPUs : 0.021529 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:55:05] 0.054999 kWh of electricity used since the beginning.\n",
            "/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [25/25], Train Loss: 0.1442, Train Accuracy: 95.14%\n",
            "Epoch [25/25], Val Loss: 0.3299, Val Accuracy: 89.41%\n",
            "\n",
            "--- Model Analysis ---\n",
            "Parameter Count: 134301514\n",
            "Model Size: 512.33 MB\n",
            "FLOPs: 0.43 GFLOPs\n",
            "Training Time: 1824.55 seconds\n",
            "Average Inference Time: 0.003761 seconds\n",
            "\n",
            "--- Energy and Emissions Report ---\n",
            "CO2 Emissions (CodeCarbon): 0.019207 kg\n",
            "Average GPU Power Consumption: 63.74 W\n",
            "Total GPU Energy Consumption: 32.306925 kWh\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained VGG16 model and modify the classifier\n",
        "model = models.vgg16(pretrained=True)\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),  # Increased dropout for regularization\n",
        "    nn.Linear(input_lastLayer, 10)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer with learning rate scheduler\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Tracking metrics for analysis\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Use CodeCarbon for system-wide energy consumption tracking\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Function to get GPU power consumption using nvidia-smi\n",
        "def get_gpu_power():\n",
        "    try:\n",
        "        # Query power.draw from nvidia-smi\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,nounits,noheader'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        power_draws = result.stdout.strip().split('\\n')\n",
        "        power_draws = [float(p) for p in power_draws]\n",
        "        avg_power_draw = sum(power_draws) / len(power_draws)  # Average power across GPUs\n",
        "        return avg_power_draw  # in watts\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting GPU power: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Tracking GPU-specific energy consumption\n",
        "gpu_power_readings = []\n",
        "\n",
        "# Training loop with GPU power monitoring\n",
        "epochs = 25\n",
        "start_training_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Record GPU power usage\n",
        "        gpu_power_readings.append(get_gpu_power())\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Adjust learning rate based on validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "training_time = time.time() - start_training_time\n",
        "\n",
        "# Stop the CodeCarbon tracker after training\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Calculate average GPU power and total GPU energy consumption (in kWh)\n",
        "avg_gpu_power = sum(gpu_power_readings) / len(gpu_power_readings) if gpu_power_readings else 0.0\n",
        "energy_consumption_gpu = (avg_gpu_power * training_time) / 3600  # Convert to kWh\n",
        "\n",
        "# Save the model after training\n",
        "model_path = \"trained_vgg16_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Model analysis metrics\n",
        "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "torch.save(model.state_dict(), \"temp_model.pth\")\n",
        "model_size = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)  # Convert bytes to MB\n",
        "os.remove(\"temp_model.pth\")\n",
        "\n",
        "# FLOPs calculation\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "flops, _ = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "\n",
        "# Inference time calculation\n",
        "model.eval()\n",
        "inference_start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        _ = model(dummy_input)\n",
        "inference_time = (time.time() - inference_start) / 100\n",
        "\n",
        "# Final energy and emissions report\n",
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size:.2f} MB\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Average Inference Time: {inference_time:.6f} seconds\")\n",
        "\n",
        "print(\"\\n--- Energy and Emissions Report ---\")\n",
        "print(f\"CO2 Emissions (CodeCarbon): {emissions:.6f} kg\")\n",
        "print(f\"Average GPU Power Consumption: {avg_gpu_power:.2f} W\")\n",
        "print(f\"Total GPU Energy Consumption: {energy_consumption_gpu:.6f} kWh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9AbF2M-tcu1",
        "outputId": "83c90acf-c393-41a5-c28a-aeceb255b9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-6-03598a6d5a61>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\"))  # Load the downloaded teacher model\n",
            "[codecarbon INFO @ 03:55:38] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 03:55:38] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 03:55:38] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 03:55:38] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 03:55:38] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 03:55:39] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 03:55:39] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 03:55:39] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 03:55:39]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 03:55:39]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 03:55:39]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 03:55:39]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 03:55:39]   CPU count: 2\n",
            "[codecarbon INFO @ 03:55:39]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 03:55:39]   GPU count: 1\n",
            "[codecarbon INFO @ 03:55:39]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 03:55:39] Saving emissions data to file /content/emissions.csv\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:55:54] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:55:55] Energy consumed for all GPUs : 0.000271 kWh. Total GPU Power : 64.85527316721625 W\n",
            "[codecarbon INFO @ 03:55:55] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:55:55] 0.000468 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:56:09] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:56:09] Energy consumed for all GPUs : 0.000552 kWh. Total GPU Power : 67.6433391395138 W\n",
            "[codecarbon INFO @ 03:56:09] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:56:09] 0.000946 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:56:24] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:56:25] Energy consumed for all GPUs : 0.000833 kWh. Total GPU Power : 67.4884044936364 W\n",
            "[codecarbon INFO @ 03:56:25] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:56:25] 0.001425 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:56:39] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:56:40] Energy consumed for all GPUs : 0.001112 kWh. Total GPU Power : 66.98455275301731 W\n",
            "[codecarbon INFO @ 03:56:40] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:56:40] 0.001900 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Loss: 6.3814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:56:55] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:56:55] Energy consumed for all GPUs : 0.001387 kWh. Total GPU Power : 65.97628938453347 W\n",
            "[codecarbon INFO @ 03:56:55] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:56:55] 0.002371 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:57:10] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:57:10] Energy consumed for all GPUs : 0.001666 kWh. Total GPU Power : 67.01332071392753 W\n",
            "[codecarbon INFO @ 03:57:10] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:57:10] 0.002848 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:57:25] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:57:25] Energy consumed for all GPUs : 0.001945 kWh. Total GPU Power : 67.02603873427324 W\n",
            "[codecarbon INFO @ 03:57:25] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:57:25] 0.003324 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:57:40] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:57:40] Energy consumed for all GPUs : 0.002225 kWh. Total GPU Power : 67.08959746827817 W\n",
            "[codecarbon INFO @ 03:57:40] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:57:40] 0.003800 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:57:40] 0.011052 g.CO2eq/s mean an estimation of 348.5476565923917 kg.CO2eq/year\n",
            "[codecarbon INFO @ 03:57:55] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:57:55] Energy consumed for all GPUs : 0.002504 kWh. Total GPU Power : 66.94962808924488 W\n",
            "[codecarbon INFO @ 03:57:55] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:57:55] 0.004276 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/25], Loss: 5.4015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:58:10] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:58:10] Energy consumed for all GPUs : 0.002778 kWh. Total GPU Power : 65.8722627631388 W\n",
            "[codecarbon INFO @ 03:58:10] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:58:10] 0.004747 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:58:25] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:58:25] Energy consumed for all GPUs : 0.003057 kWh. Total GPU Power : 67.00374269986874 W\n",
            "[codecarbon INFO @ 03:58:25] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:58:25] 0.005223 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:58:40] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:58:40] Energy consumed for all GPUs : 0.003337 kWh. Total GPU Power : 67.09706385883074 W\n",
            "[codecarbon INFO @ 03:58:40] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:58:40] 0.005700 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:58:55] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:58:55] Energy consumed for all GPUs : 0.003615 kWh. Total GPU Power : 66.92746178699565 W\n",
            "[codecarbon INFO @ 03:58:55] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:58:55] 0.006176 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:59:10] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:59:10] Energy consumed for all GPUs : 0.003896 kWh. Total GPU Power : 67.44927439050969 W\n",
            "[codecarbon INFO @ 03:59:10] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:59:10] 0.006653 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/25], Loss: 4.5325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 03:59:25] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:59:25] Energy consumed for all GPUs : 0.004171 kWh. Total GPU Power : 66.36714522968919 W\n",
            "[codecarbon INFO @ 03:59:25] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:59:25] 0.007125 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:59:40] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:59:40] Energy consumed for all GPUs : 0.004452 kWh. Total GPU Power : 67.24429335965485 W\n",
            "[codecarbon INFO @ 03:59:40] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:59:40] 0.007602 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 03:59:40] 0.011057 g.CO2eq/s mean an estimation of 348.688769186443 kg.CO2eq/year\n",
            "[codecarbon INFO @ 03:59:55] Energy consumed for RAM : 0.000336 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 03:59:55] Energy consumed for all GPUs : 0.004730 kWh. Total GPU Power : 66.8970819317131 W\n",
            "[codecarbon INFO @ 03:59:55] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:59:55] 0.008078 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:00:10] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:00:10] Energy consumed for all GPUs : 0.005008 kWh. Total GPU Power : 66.69983950877537 W\n",
            "[codecarbon INFO @ 04:00:10] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:00:10] 0.008553 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:00:25] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:00:25] Energy consumed for all GPUs : 0.005287 kWh. Total GPU Power : 67.00110895489355 W\n",
            "[codecarbon INFO @ 04:00:25] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:00:25] 0.009029 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/25], Loss: 3.8177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:00:40] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:00:40] Energy consumed for all GPUs : 0.005563 kWh. Total GPU Power : 66.1224236349756 W\n",
            "[codecarbon INFO @ 04:00:40] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:00:40] 0.009501 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:00:55] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:00:55] Energy consumed for all GPUs : 0.005842 kWh. Total GPU Power : 67.10010504265296 W\n",
            "[codecarbon INFO @ 04:00:55] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:00:55] 0.009977 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:01:10] Energy consumed for RAM : 0.000435 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:01:10] Energy consumed for all GPUs : 0.006121 kWh. Total GPU Power : 66.92928815566448 W\n",
            "[codecarbon INFO @ 04:01:10] Energy consumed for all CPUs : 0.003897 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:01:10] 0.010453 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:01:25] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:01:25] Energy consumed for all GPUs : 0.006400 kWh. Total GPU Power : 67.08359237481345 W\n",
            "[codecarbon INFO @ 04:01:25] Energy consumed for all CPUs : 0.004074 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:01:25] 0.010929 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:01:40] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:01:40] Energy consumed for all GPUs : 0.006679 kWh. Total GPU Power : 67.00454861038125 W\n",
            "[codecarbon INFO @ 04:01:40] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:01:40] 0.011405 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:01:40] 0.011062 g.CO2eq/s mean an estimation of 348.86034381631924 kg.CO2eq/year\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/25], Loss: 3.2315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:01:55] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:01:55] Energy consumed for all GPUs : 0.006953 kWh. Total GPU Power : 65.7377768284158 W\n",
            "[codecarbon INFO @ 04:01:55] Energy consumed for all CPUs : 0.004428 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:01:55] 0.011876 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:02:10] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:02:10] Energy consumed for all GPUs : 0.007231 kWh. Total GPU Power : 66.7272207528669 W\n",
            "[codecarbon INFO @ 04:02:10] Energy consumed for all CPUs : 0.004605 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:02:10] 0.012351 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:02:25] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:02:25] Energy consumed for all GPUs : 0.007509 kWh. Total GPU Power : 66.64012800468065 W\n",
            "[codecarbon INFO @ 04:02:25] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:02:25] 0.012826 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:02:40] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:02:40] Energy consumed for all GPUs : 0.007787 kWh. Total GPU Power : 66.68819597015944 W\n",
            "[codecarbon INFO @ 04:02:40] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:02:40] 0.013301 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:02:55] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:02:55] Energy consumed for all GPUs : 0.008066 kWh. Total GPU Power : 66.84048389516737 W\n",
            "[codecarbon INFO @ 04:02:55] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:02:55] 0.013776 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/25], Loss: 2.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:03:10] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:03:10] Energy consumed for all GPUs : 0.008342 kWh. Total GPU Power : 66.36761956518247 W\n",
            "[codecarbon INFO @ 04:03:10] Energy consumed for all CPUs : 0.005314 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:03:10] 0.014250 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:03:25] Energy consumed for RAM : 0.000613 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:03:25] Energy consumed for all GPUs : 0.008621 kWh. Total GPU Power : 66.8938682670601 W\n",
            "[codecarbon INFO @ 04:03:25] Energy consumed for all CPUs : 0.005491 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:03:25] 0.014725 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:03:40] Energy consumed for RAM : 0.000633 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:03:40] Energy consumed for all GPUs : 0.008899 kWh. Total GPU Power : 66.80650263837676 W\n",
            "[codecarbon INFO @ 04:03:40] Energy consumed for all CPUs : 0.005668 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:03:40] 0.015200 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:03:40] 0.011037 g.CO2eq/s mean an estimation of 348.0496665962817 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:03:55] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:03:55] Energy consumed for all GPUs : 0.009177 kWh. Total GPU Power : 66.87439971033194 W\n",
            "[codecarbon INFO @ 04:03:55] Energy consumed for all CPUs : 0.005845 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:03:55] 0.015676 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/25], Loss: 2.3204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:04:10] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:04:10] Energy consumed for all GPUs : 0.009453 kWh. Total GPU Power : 66.10659267456403 W\n",
            "[codecarbon INFO @ 04:04:10] Energy consumed for all CPUs : 0.006023 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:04:10] 0.016148 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:04:25] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:04:25] Energy consumed for all GPUs : 0.009731 kWh. Total GPU Power : 66.69529302230987 W\n",
            "[codecarbon INFO @ 04:04:25] Energy consumed for all CPUs : 0.006200 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:04:25] 0.016623 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:04:40] Energy consumed for RAM : 0.000712 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:04:40] Energy consumed for all GPUs : 0.010008 kWh. Total GPU Power : 66.59292555546163 W\n",
            "[codecarbon INFO @ 04:04:40] Energy consumed for all CPUs : 0.006377 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:04:40] 0.017097 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:04:55] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:04:55] Energy consumed for all GPUs : 0.010285 kWh. Total GPU Power : 66.53948378572757 W\n",
            "[codecarbon INFO @ 04:04:55] Energy consumed for all CPUs : 0.006554 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:04:55] 0.017571 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:05:10] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:05:10] Energy consumed for all GPUs : 0.010563 kWh. Total GPU Power : 66.67911419458115 W\n",
            "[codecarbon INFO @ 04:05:10] Energy consumed for all CPUs : 0.006731 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:05:10] 0.018046 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/25], Loss: 2.0647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:05:25] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:05:25] Energy consumed for all GPUs : 0.010837 kWh. Total GPU Power : 66.0268522731619 W\n",
            "[codecarbon INFO @ 04:05:25] Energy consumed for all CPUs : 0.006908 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:05:25] 0.018517 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:05:40] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:05:40] Energy consumed for all GPUs : 0.011115 kWh. Total GPU Power : 66.6522045249114 W\n",
            "[codecarbon INFO @ 04:05:40] Energy consumed for all CPUs : 0.007085 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:05:40] 0.018992 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:05:40] 0.011029 g.CO2eq/s mean an estimation of 347.80001016958437 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:05:55] Energy consumed for RAM : 0.000811 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:05:55] Energy consumed for all GPUs : 0.011392 kWh. Total GPU Power : 66.56634510382801 W\n",
            "[codecarbon INFO @ 04:05:55] Energy consumed for all CPUs : 0.007262 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:05:55] 0.019466 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:06:10] Energy consumed for RAM : 0.000831 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:06:10] Energy consumed for all GPUs : 0.011669 kWh. Total GPU Power : 66.56176866616768 W\n",
            "[codecarbon INFO @ 04:06:10] Energy consumed for all CPUs : 0.007439 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:06:10] 0.019940 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:06:25] Energy consumed for RAM : 0.000851 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:06:25] Energy consumed for all GPUs : 0.011948 kWh. Total GPU Power : 66.83305255139237 W\n",
            "[codecarbon INFO @ 04:06:25] Energy consumed for all CPUs : 0.007616 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:06:25] 0.020415 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/25], Loss: 1.8703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:06:40] Energy consumed for RAM : 0.000871 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:06:40] Energy consumed for all GPUs : 0.012223 kWh. Total GPU Power : 66.04369814224256 W\n",
            "[codecarbon INFO @ 04:06:40] Energy consumed for all CPUs : 0.007793 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:06:40] 0.020888 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:06:55] Energy consumed for RAM : 0.000891 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:06:55] Energy consumed for all GPUs : 0.012501 kWh. Total GPU Power : 66.56382541025233 W\n",
            "[codecarbon INFO @ 04:06:55] Energy consumed for all CPUs : 0.007971 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:06:55] 0.021362 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:07:10] Energy consumed for RAM : 0.000910 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:07:10] Energy consumed for all GPUs : 0.012778 kWh. Total GPU Power : 66.68757909439879 W\n",
            "[codecarbon INFO @ 04:07:10] Energy consumed for all CPUs : 0.008148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:07:10] 0.021836 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:07:25] Energy consumed for RAM : 0.000930 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:07:25] Energy consumed for all GPUs : 0.013056 kWh. Total GPU Power : 66.7155960491455 W\n",
            "[codecarbon INFO @ 04:07:25] Energy consumed for all CPUs : 0.008325 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:07:25] 0.022311 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:07:40] Energy consumed for RAM : 0.000950 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:07:40] Energy consumed for all GPUs : 0.013335 kWh. Total GPU Power : 66.90331320122579 W\n",
            "[codecarbon INFO @ 04:07:40] Energy consumed for all CPUs : 0.008502 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:07:40] 0.022787 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:07:40] 0.011039 g.CO2eq/s mean an estimation of 348.1195529183896 kg.CO2eq/year\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/25], Loss: 1.7699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:07:55] Energy consumed for RAM : 0.000970 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:07:55] Energy consumed for all GPUs : 0.013612 kWh. Total GPU Power : 66.37161122306233 W\n",
            "[codecarbon INFO @ 04:07:55] Energy consumed for all CPUs : 0.008680 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:07:55] 0.023261 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:08:10] Energy consumed for RAM : 0.000990 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:08:10] Energy consumed for all GPUs : 0.013890 kWh. Total GPU Power : 66.78877051348476 W\n",
            "[codecarbon INFO @ 04:08:10] Energy consumed for all CPUs : 0.008857 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:08:10] 0.023737 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:08:25] Energy consumed for RAM : 0.001009 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:08:25] Energy consumed for all GPUs : 0.014169 kWh. Total GPU Power : 66.82776249807378 W\n",
            "[codecarbon INFO @ 04:08:25] Energy consumed for all CPUs : 0.009034 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:08:25] 0.024212 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:08:40] Energy consumed for RAM : 0.001029 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:08:40] Energy consumed for all GPUs : 0.014449 kWh. Total GPU Power : 67.15351537892197 W\n",
            "[codecarbon INFO @ 04:08:40] Energy consumed for all CPUs : 0.009211 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:08:40] 0.024689 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:08:55] Energy consumed for RAM : 0.001049 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:08:55] Energy consumed for all GPUs : 0.014728 kWh. Total GPU Power : 66.97265311695607 W\n",
            "[codecarbon INFO @ 04:08:55] Energy consumed for all CPUs : 0.009388 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:08:55] 0.025165 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/25], Loss: 1.7434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:09:10] Energy consumed for RAM : 0.001069 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:09:10] Energy consumed for all GPUs : 0.015001 kWh. Total GPU Power : 65.51473851083976 W\n",
            "[codecarbon INFO @ 04:09:10] Energy consumed for all CPUs : 0.009565 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:09:10] 0.025635 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:09:25] Energy consumed for RAM : 0.001089 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:09:25] Energy consumed for all GPUs : 0.015279 kWh. Total GPU Power : 66.90000528608549 W\n",
            "[codecarbon INFO @ 04:09:25] Energy consumed for all CPUs : 0.009742 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:09:25] 0.026110 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:09:40] Energy consumed for RAM : 0.001108 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:09:40] Energy consumed for all GPUs : 0.015558 kWh. Total GPU Power : 67.03706089499576 W\n",
            "[codecarbon INFO @ 04:09:40] Energy consumed for all CPUs : 0.009920 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:09:40] 0.026586 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:09:40] 0.011046 g.CO2eq/s mean an estimation of 348.3612531875929 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:09:55] Energy consumed for RAM : 0.001128 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:09:55] Energy consumed for all GPUs : 0.015838 kWh. Total GPU Power : 67.11164220112853 W\n",
            "[codecarbon INFO @ 04:09:55] Energy consumed for all CPUs : 0.010097 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:09:55] 0.027063 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:10:10] Energy consumed for RAM : 0.001148 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:10:10] Energy consumed for all GPUs : 0.016117 kWh. Total GPU Power : 66.9024723580531 W\n",
            "[codecarbon INFO @ 04:10:10] Energy consumed for all CPUs : 0.010274 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:10:10] 0.027538 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/25], Loss: 1.7289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:10:25] Energy consumed for RAM : 0.001168 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:10:25] Energy consumed for all GPUs : 0.016393 kWh. Total GPU Power : 66.26652440261057 W\n",
            "[codecarbon INFO @ 04:10:25] Energy consumed for all CPUs : 0.010451 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:10:25] 0.028011 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:10:40] Energy consumed for RAM : 0.001188 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:10:40] Energy consumed for all GPUs : 0.016671 kWh. Total GPU Power : 66.85272640791942 W\n",
            "[codecarbon INFO @ 04:10:40] Energy consumed for all CPUs : 0.010628 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:10:40] 0.028487 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:10:55] Energy consumed for RAM : 0.001207 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:10:55] Energy consumed for all GPUs : 0.016949 kWh. Total GPU Power : 66.68550515989918 W\n",
            "[codecarbon INFO @ 04:10:55] Energy consumed for all CPUs : 0.010805 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:10:55] 0.028962 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:11:10] Energy consumed for RAM : 0.001227 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:11:10] Energy consumed for all GPUs : 0.017226 kWh. Total GPU Power : 66.66615733426795 W\n",
            "[codecarbon INFO @ 04:11:10] Energy consumed for all CPUs : 0.010982 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:11:10] 0.029436 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:11:25] Energy consumed for RAM : 0.001247 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:11:25] Energy consumed for all GPUs : 0.017504 kWh. Total GPU Power : 66.7456994714272 W\n",
            "[codecarbon INFO @ 04:11:25] Energy consumed for all CPUs : 0.011159 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:11:25] 0.029911 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/25], Loss: 1.7550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:11:40] Energy consumed for RAM : 0.001267 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:11:40] Energy consumed for all GPUs : 0.017780 kWh. Total GPU Power : 66.15895718406622 W\n",
            "[codecarbon INFO @ 04:11:40] Energy consumed for all CPUs : 0.011337 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:11:40] 0.030383 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:11:40] 0.011044 g.CO2eq/s mean an estimation of 348.2867800305573 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:11:55] Energy consumed for RAM : 0.001286 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:11:55] Energy consumed for all GPUs : 0.018059 kWh. Total GPU Power : 67.04828458419136 W\n",
            "[codecarbon INFO @ 04:11:55] Energy consumed for all CPUs : 0.011514 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:11:55] 0.030859 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:12:10] Energy consumed for RAM : 0.001306 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:12:10] Energy consumed for all GPUs : 0.018338 kWh. Total GPU Power : 67.04476020736007 W\n",
            "[codecarbon INFO @ 04:12:10] Energy consumed for all CPUs : 0.011691 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:12:10] 0.031335 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:12:25] Energy consumed for RAM : 0.001326 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:12:25] Energy consumed for all GPUs : 0.018618 kWh. Total GPU Power : 67.04169624047451 W\n",
            "[codecarbon INFO @ 04:12:25] Energy consumed for all CPUs : 0.011868 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:12:25] 0.031811 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/25], Loss: 1.7694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:12:40] Energy consumed for RAM : 0.001346 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:12:40] Energy consumed for all GPUs : 0.018893 kWh. Total GPU Power : 66.13675000947202 W\n",
            "[codecarbon INFO @ 04:12:40] Energy consumed for all CPUs : 0.012045 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:12:40] 0.032283 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:12:55] Energy consumed for RAM : 0.001366 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:12:55] Energy consumed for all GPUs : 0.019171 kWh. Total GPU Power : 66.82774903469593 W\n",
            "[codecarbon INFO @ 04:12:55] Energy consumed for all CPUs : 0.012222 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:12:55] 0.032759 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:13:10] Energy consumed for RAM : 0.001385 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:13:10] Energy consumed for all GPUs : 0.019450 kWh. Total GPU Power : 66.9909182326992 W\n",
            "[codecarbon INFO @ 04:13:10] Energy consumed for all CPUs : 0.012399 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:13:10] 0.033235 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:13:25] Energy consumed for RAM : 0.001405 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:13:25] Energy consumed for all GPUs : 0.019730 kWh. Total GPU Power : 67.1544219443773 W\n",
            "[codecarbon INFO @ 04:13:25] Energy consumed for all CPUs : 0.012576 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:13:25] 0.033711 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:13:40] Energy consumed for RAM : 0.001425 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:13:40] Energy consumed for all GPUs : 0.020008 kWh. Total GPU Power : 66.85371122304643 W\n",
            "[codecarbon INFO @ 04:13:40] Energy consumed for all CPUs : 0.012753 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:13:40] 0.034187 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:13:40] 0.011064 g.CO2eq/s mean an estimation of 348.92564776750413 kg.CO2eq/year\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/25], Loss: 1.8249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:13:55] Energy consumed for RAM : 0.001445 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:13:55] Energy consumed for all GPUs : 0.020284 kWh. Total GPU Power : 66.03559125632542 W\n",
            "[codecarbon INFO @ 04:13:55] Energy consumed for all CPUs : 0.012930 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:13:55] 0.034659 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:14:10] Energy consumed for RAM : 0.001465 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:14:10] Energy consumed for all GPUs : 0.020562 kWh. Total GPU Power : 66.9491954207824 W\n",
            "[codecarbon INFO @ 04:14:10] Energy consumed for all CPUs : 0.013107 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:14:10] 0.035135 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:14:25] Energy consumed for RAM : 0.001484 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:14:25] Energy consumed for all GPUs : 0.020841 kWh. Total GPU Power : 66.89426276962178 W\n",
            "[codecarbon INFO @ 04:14:25] Energy consumed for all CPUs : 0.013285 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:14:25] 0.035611 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:14:40] Energy consumed for RAM : 0.001504 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:14:40] Energy consumed for all GPUs : 0.021118 kWh. Total GPU Power : 66.27658807983389 W\n",
            "[codecarbon INFO @ 04:14:40] Energy consumed for all CPUs : 0.013462 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:14:40] 0.036084 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:14:55] Energy consumed for RAM : 0.001524 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:14:55] Energy consumed for all GPUs : 0.021396 kWh. Total GPU Power : 66.73409791010668 W\n",
            "[codecarbon INFO @ 04:14:55] Energy consumed for all CPUs : 0.013640 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:14:55] 0.036560 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/25], Loss: 1.8301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:15:10] Energy consumed for RAM : 0.001544 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:15:10] Energy consumed for all GPUs : 0.021670 kWh. Total GPU Power : 65.880051698024 W\n",
            "[codecarbon INFO @ 04:15:10] Energy consumed for all CPUs : 0.013817 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:15:10] 0.037031 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:15:25] Energy consumed for RAM : 0.001564 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:15:25] Energy consumed for all GPUs : 0.021948 kWh. Total GPU Power : 66.80097876973326 W\n",
            "[codecarbon INFO @ 04:15:25] Energy consumed for all CPUs : 0.013994 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:15:25] 0.037506 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:15:40] Energy consumed for RAM : 0.001583 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:15:40] Energy consumed for all GPUs : 0.022226 kWh. Total GPU Power : 66.75071337173874 W\n",
            "[codecarbon INFO @ 04:15:40] Energy consumed for all CPUs : 0.014171 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:15:40] 0.037981 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:15:40] 0.011030 g.CO2eq/s mean an estimation of 347.83150518226284 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:15:55] Energy consumed for RAM : 0.001603 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:15:55] Energy consumed for all GPUs : 0.022504 kWh. Total GPU Power : 66.6256675626509 W\n",
            "[codecarbon INFO @ 04:15:55] Energy consumed for all CPUs : 0.014348 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:15:55] 0.038456 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:16:10] Energy consumed for RAM : 0.001623 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:16:10] Energy consumed for all GPUs : 0.022781 kWh. Total GPU Power : 66.41766446011206 W\n",
            "[codecarbon INFO @ 04:16:10] Energy consumed for all CPUs : 0.014525 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:16:10] 0.038929 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/25], Loss: 1.8954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:16:25] Energy consumed for RAM : 0.001643 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:16:25] Energy consumed for all GPUs : 0.023054 kWh. Total GPU Power : 65.56053882995609 W\n",
            "[codecarbon INFO @ 04:16:25] Energy consumed for all CPUs : 0.014703 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:16:25] 0.039400 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:16:40] Energy consumed for RAM : 0.001663 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:16:40] Energy consumed for all GPUs : 0.023333 kWh. Total GPU Power : 66.97182155881615 W\n",
            "[codecarbon INFO @ 04:16:40] Energy consumed for all CPUs : 0.014880 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:16:40] 0.039876 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:16:55] Energy consumed for RAM : 0.001682 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:16:55] Energy consumed for all GPUs : 0.023610 kWh. Total GPU Power : 66.53276956190487 W\n",
            "[codecarbon INFO @ 04:16:55] Energy consumed for all CPUs : 0.015057 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:16:55] 0.040349 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:17:10] Energy consumed for RAM : 0.001702 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:17:10] Energy consumed for all GPUs : 0.023888 kWh. Total GPU Power : 66.70966298707118 W\n",
            "[codecarbon INFO @ 04:17:10] Energy consumed for all CPUs : 0.015234 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:17:10] 0.040824 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:17:25] Energy consumed for RAM : 0.001722 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:17:25] Energy consumed for all GPUs : 0.024166 kWh. Total GPU Power : 66.80007425144727 W\n",
            "[codecarbon INFO @ 04:17:25] Energy consumed for all CPUs : 0.015411 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:17:25] 0.041299 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/25], Loss: 1.8853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:17:40] Energy consumed for RAM : 0.001742 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:17:40] Energy consumed for all GPUs : 0.024441 kWh. Total GPU Power : 65.97424136117304 W\n",
            "[codecarbon INFO @ 04:17:40] Energy consumed for all CPUs : 0.015588 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:17:40] 0.041771 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:17:40] 0.011023 g.CO2eq/s mean an estimation of 347.6114578204408 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:17:55] Energy consumed for RAM : 0.001762 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:17:55] Energy consumed for all GPUs : 0.024720 kWh. Total GPU Power : 67.12545028228494 W\n",
            "[codecarbon INFO @ 04:17:55] Energy consumed for all CPUs : 0.015765 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:17:55] 0.042247 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:18:10] Energy consumed for RAM : 0.001781 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:18:10] Energy consumed for all GPUs : 0.024998 kWh. Total GPU Power : 66.7310765953999 W\n",
            "[codecarbon INFO @ 04:18:10] Energy consumed for all CPUs : 0.015942 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:18:10] 0.042722 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:18:25] Energy consumed for RAM : 0.001801 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:18:25] Energy consumed for all GPUs : 0.025277 kWh. Total GPU Power : 66.87962989915566 W\n",
            "[codecarbon INFO @ 04:18:25] Energy consumed for all CPUs : 0.016119 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:18:25] 0.043197 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:18:40] Energy consumed for RAM : 0.001821 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:18:40] Energy consumed for all GPUs : 0.025556 kWh. Total GPU Power : 67.06247744149515 W\n",
            "[codecarbon INFO @ 04:18:40] Energy consumed for all CPUs : 0.016296 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:18:40] 0.043673 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/25], Loss: 1.9189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:18:55] Energy consumed for RAM : 0.001841 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:18:55] Energy consumed for all GPUs : 0.025832 kWh. Total GPU Power : 66.23376338705181 W\n",
            "[codecarbon INFO @ 04:18:55] Energy consumed for all CPUs : 0.016473 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:18:55] 0.044146 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:19:10] Energy consumed for RAM : 0.001860 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:19:10] Energy consumed for all GPUs : 0.026109 kWh. Total GPU Power : 66.6755550266089 W\n",
            "[codecarbon INFO @ 04:19:10] Energy consumed for all CPUs : 0.016650 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:19:10] 0.044620 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:19:25] Energy consumed for RAM : 0.001880 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:19:25] Energy consumed for all GPUs : 0.026385 kWh. Total GPU Power : 66.27106024975413 W\n",
            "[codecarbon INFO @ 04:19:25] Energy consumed for all CPUs : 0.016828 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:19:25] 0.045093 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:19:40] Energy consumed for RAM : 0.001900 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:19:40] Energy consumed for all GPUs : 0.026662 kWh. Total GPU Power : 66.51498281134383 W\n",
            "[codecarbon INFO @ 04:19:40] Energy consumed for all CPUs : 0.017005 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:19:40] 0.045567 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:19:40] 0.011044 g.CO2eq/s mean an estimation of 348.26797030888054 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:19:55] Energy consumed for RAM : 0.001920 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:19:55] Energy consumed for all GPUs : 0.026940 kWh. Total GPU Power : 66.85665496163045 W\n",
            "[codecarbon INFO @ 04:19:55] Energy consumed for all CPUs : 0.017182 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:19:55] 0.046042 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/25], Loss: 1.8669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:20:10] Energy consumed for RAM : 0.001940 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:20:10] Energy consumed for all GPUs : 0.027217 kWh. Total GPU Power : 66.34008030139404 W\n",
            "[codecarbon INFO @ 04:20:10] Energy consumed for all CPUs : 0.017359 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:20:10] 0.046515 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:20:25] Energy consumed for RAM : 0.001959 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:20:25] Energy consumed for all GPUs : 0.027495 kWh. Total GPU Power : 66.73663327338642 W\n",
            "[codecarbon INFO @ 04:20:25] Energy consumed for all CPUs : 0.017536 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:20:25] 0.046990 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:20:40] Energy consumed for RAM : 0.001979 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:20:40] Energy consumed for all GPUs : 0.027773 kWh. Total GPU Power : 66.79931397151823 W\n",
            "[codecarbon INFO @ 04:20:40] Energy consumed for all CPUs : 0.017713 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:20:40] 0.047465 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:20:55] Energy consumed for RAM : 0.001999 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:20:55] Energy consumed for all GPUs : 0.028051 kWh. Total GPU Power : 66.84058800960506 W\n",
            "[codecarbon INFO @ 04:20:55] Energy consumed for all CPUs : 0.017890 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:20:55] 0.047940 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [21/25], Loss: 1.8148\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:21:10] Energy consumed for RAM : 0.002019 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:21:10] Energy consumed for all GPUs : 0.028328 kWh. Total GPU Power : 66.49213593943308 W\n",
            "[codecarbon INFO @ 04:21:10] Energy consumed for all CPUs : 0.018067 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:21:10] 0.048414 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:21:25] Energy consumed for RAM : 0.002039 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:21:25] Energy consumed for all GPUs : 0.028605 kWh. Total GPU Power : 66.38968460981765 W\n",
            "[codecarbon INFO @ 04:21:25] Energy consumed for all CPUs : 0.018245 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:21:25] 0.048888 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:21:40] Energy consumed for RAM : 0.002058 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:21:40] Energy consumed for all GPUs : 0.028882 kWh. Total GPU Power : 66.52390554339271 W\n",
            "[codecarbon INFO @ 04:21:40] Energy consumed for all CPUs : 0.018421 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:21:40] 0.049361 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:21:40] 0.011038 g.CO2eq/s mean an estimation of 348.09963922603055 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:21:55] Energy consumed for RAM : 0.002078 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:21:55] Energy consumed for all GPUs : 0.029159 kWh. Total GPU Power : 66.51960618268588 W\n",
            "[codecarbon INFO @ 04:21:55] Energy consumed for all CPUs : 0.018599 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:21:55] 0.049836 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:22:10] Energy consumed for RAM : 0.002098 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:22:10] Energy consumed for all GPUs : 0.029438 kWh. Total GPU Power : 66.956835083796 W\n",
            "[codecarbon INFO @ 04:22:10] Energy consumed for all CPUs : 0.018776 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:22:10] 0.050312 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [22/25], Loss: 1.7087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:22:25] Energy consumed for RAM : 0.002118 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:22:25] Energy consumed for all GPUs : 0.029714 kWh. Total GPU Power : 66.09839719121703 W\n",
            "[codecarbon INFO @ 04:22:25] Energy consumed for all CPUs : 0.018953 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:22:25] 0.050784 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:22:40] Energy consumed for RAM : 0.002137 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:22:40] Energy consumed for all GPUs : 0.029991 kWh. Total GPU Power : 66.78500601732453 W\n",
            "[codecarbon INFO @ 04:22:40] Energy consumed for all CPUs : 0.019130 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:22:40] 0.051259 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:22:55] Energy consumed for RAM : 0.002157 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:22:55] Energy consumed for all GPUs : 0.030269 kWh. Total GPU Power : 66.73008887630695 W\n",
            "[codecarbon INFO @ 04:22:55] Energy consumed for all CPUs : 0.019307 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:22:55] 0.051734 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:23:10] Energy consumed for RAM : 0.002177 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:23:10] Energy consumed for all GPUs : 0.030548 kWh. Total GPU Power : 66.9280107041219 W\n",
            "[codecarbon INFO @ 04:23:11] Energy consumed for all CPUs : 0.019484 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:23:11] 0.052210 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:23:25] Energy consumed for RAM : 0.002197 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:23:25] Energy consumed for all GPUs : 0.030825 kWh. Total GPU Power : 66.54244424877193 W\n",
            "[codecarbon INFO @ 04:23:25] Energy consumed for all CPUs : 0.019661 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:23:25] 0.052683 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [23/25], Loss: 1.6102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:23:40] Energy consumed for RAM : 0.002217 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:23:40] Energy consumed for all GPUs : 0.031100 kWh. Total GPU Power : 65.88136152456835 W\n",
            "[codecarbon INFO @ 04:23:41] Energy consumed for all CPUs : 0.019838 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:23:41] 0.053155 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:23:41] 0.011031 g.CO2eq/s mean an estimation of 347.870741927623 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:23:55] Energy consumed for RAM : 0.002236 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:23:56] Energy consumed for all GPUs : 0.031378 kWh. Total GPU Power : 66.82860352064664 W\n",
            "[codecarbon INFO @ 04:23:56] Energy consumed for all CPUs : 0.020016 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:23:56] 0.053630 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:24:10] Energy consumed for RAM : 0.002256 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:24:11] Energy consumed for all GPUs : 0.031655 kWh. Total GPU Power : 66.54129946350353 W\n",
            "[codecarbon INFO @ 04:24:11] Energy consumed for all CPUs : 0.020192 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:24:11] 0.054103 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:24:26] Energy consumed for RAM : 0.002276 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:24:26] Energy consumed for all GPUs : 0.031936 kWh. Total GPU Power : 67.37935165284657 W\n",
            "[codecarbon INFO @ 04:24:26] Energy consumed for all CPUs : 0.020370 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:24:26] 0.054582 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:24:41] Energy consumed for RAM : 0.002296 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:24:41] Energy consumed for all GPUs : 0.032213 kWh. Total GPU Power : 66.45926030498148 W\n",
            "[codecarbon INFO @ 04:24:41] Energy consumed for all CPUs : 0.020547 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:24:41] 0.055055 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [24/25], Loss: 1.4790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 04:24:56] Energy consumed for RAM : 0.002316 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:24:56] Energy consumed for all GPUs : 0.032488 kWh. Total GPU Power : 66.04941384539804 W\n",
            "[codecarbon INFO @ 04:24:56] Energy consumed for all CPUs : 0.020724 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:24:56] 0.055528 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:25:11] Energy consumed for RAM : 0.002335 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:25:11] Energy consumed for all GPUs : 0.032766 kWh. Total GPU Power : 66.9130657572824 W\n",
            "[codecarbon INFO @ 04:25:11] Energy consumed for all CPUs : 0.020901 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:25:11] 0.056002 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:25:26] Energy consumed for RAM : 0.002355 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:25:26] Energy consumed for all GPUs : 0.033045 kWh. Total GPU Power : 66.98003800073765 W\n",
            "[codecarbon INFO @ 04:25:26] Energy consumed for all CPUs : 0.021078 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:25:26] 0.056478 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:25:41] Energy consumed for RAM : 0.002375 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:25:41] Energy consumed for all GPUs : 0.033323 kWh. Total GPU Power : 66.6585274713933 W\n",
            "[codecarbon INFO @ 04:25:41] Energy consumed for all CPUs : 0.021255 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:25:41] 0.056953 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:25:41] 0.011049 g.CO2eq/s mean an estimation of 348.4307142475687 kg.CO2eq/year\n",
            "[codecarbon INFO @ 04:25:56] Energy consumed for RAM : 0.002395 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:25:56] Energy consumed for all GPUs : 0.033601 kWh. Total GPU Power : 66.899821484834 W\n",
            "[codecarbon INFO @ 04:25:56] Energy consumed for all CPUs : 0.021432 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:25:56] 0.057428 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:26:00] Energy consumed for RAM : 0.002401 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:26:00] Energy consumed for all GPUs : 0.033693 kWh. Total GPU Power : 67.75506238767863 W\n",
            "[codecarbon INFO @ 04:26:00] Energy consumed for all CPUs : 0.021490 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:26:00] 0.057584 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [25/25], Loss: 1.3560\n",
            "\n",
            "--- Model Analysis ---\n",
            "Parameter Count: 128807306\n",
            "Model Size: 491.37 MB\n",
            "FLOPs: 0.27 GFLOPs\n",
            "Training Time: 1820.96 seconds\n",
            "Average Inference Time: 3.505623 ms\n",
            "\n",
            "--- Energy and Emissions Report ---\n",
            "CO2 Emissions (CodeCarbon): 0.020110 kg\n",
            "Average GPU Power Consumption: 69.05 W\n",
            "Total GPU Energy Consumption: 34.928172 kWh\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from thop import profile\n",
        "from codecarbon import EmissionsTracker\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the transforms for the dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Split the full training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Define the teacher model (VGG16) and load pre-trained weights\n",
        "teacher_model = models.vgg16(pretrained=False)\n",
        "teacher_last_layer = teacher_model.classifier[6].in_features\n",
        "teacher_model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(teacher_last_layer, 10)\n",
        ")\n",
        "teacher_model.load_state_dict(torch.load(\"trained_vgg16_model.pth\"))  # Load the downloaded teacher model\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()  # Set the teacher model to evaluation mode\n",
        "\n",
        "# Define the student model (VGG11)\n",
        "student_model = models.vgg11(pretrained=False)\n",
        "student_last_layer = student_model.classifier[6].in_features\n",
        "student_model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(student_last_layer, 10)\n",
        ")\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "# Define the distillation loss\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=3.0, alpha=0.5):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        distillation_loss = nn.KLDivLoss(reduction='batchmean')(\n",
        "            torch.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            torch.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        ) * (self.temperature ** 2)\n",
        "        student_loss = self.criterion_ce(student_logits, labels)\n",
        "        return self.alpha * distillation_loss + (1 - self.alpha) * student_loss\n",
        "\n",
        "criterion = DistillationLoss(temperature=3.0, alpha=0.5)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# GPU power tracking\n",
        "def get_gpu_power():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,nounits,noheader'],\n",
        "            stdout=subprocess.PIPE, text=True\n",
        "        )\n",
        "        power_draws = [float(p) for p in result.stdout.strip().split('\\n')]\n",
        "        return sum(power_draws) / len(power_draws) if power_draws else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting GPU power: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Energy tracking\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Training loop\n",
        "epochs = 25\n",
        "gpu_power_readings = []\n",
        "start_training_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Teacher predictions\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher_model(inputs)\n",
        "\n",
        "        # Student predictions and loss computation\n",
        "        student_logits = student_model(inputs)\n",
        "        loss = criterion(student_logits, teacher_logits, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        gpu_power_readings.append(get_gpu_power())\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "end_training_time = time.time()\n",
        "training_time = end_training_time - start_training_time\n",
        "\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Energy metrics\n",
        "avg_gpu_power = sum(gpu_power_readings) / len(gpu_power_readings) if gpu_power_readings else 0.0\n",
        "total_energy_consumption = (avg_gpu_power * training_time) / 3600  # kWh\n",
        "\n",
        "# Model analysis\n",
        "param_count = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "torch.save(student_model.state_dict(), \"student_model_vgg11_temp.pth\")\n",
        "model_size_mb = os.path.getsize(\"student_model_vgg11_temp.pth\") / (1024 * 1024)\n",
        "\n",
        "# FLOPs and inference time\n",
        "example_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "flops, _ = profile(student_model, inputs=(example_input,), verbose=False)\n",
        "\n",
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    start_time = torch.cuda.Event(enable_timing=True)\n",
        "    end_time = torch.cuda.Event(enable_timing=True)\n",
        "    start_time.record()\n",
        "    for _ in range(100):\n",
        "        _ = student_model(example_input)\n",
        "    end_time.record()\n",
        "    torch.cuda.synchronize()\n",
        "inference_time_ms = start_time.elapsed_time(end_time) / 100  # Average in milliseconds\n",
        "\n",
        "# Output results\n",
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Average Inference Time: {inference_time_ms:.6f} ms\")\n",
        "\n",
        "print(\"\\n--- Energy and Emissions Report ---\")\n",
        "print(f\"CO2 Emissions (CodeCarbon): {emissions:.6f} kg\")\n",
        "print(f\"Average GPU Power Consumption: {avg_gpu_power:.2f} W\")\n",
        "print(f\"Total GPU Energy Consumption: {total_energy_consumption:.6f} kWh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgbaU5behUv-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeI7ZxFvlkOm",
        "outputId": "1cdef069-68ac-443b-f7b3-e0a1b828476d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n"
          ]
        }
      ],
      "source": [
        "#print(model)\n",
        "!pip install ptflops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkvzOl1kqva8",
        "outputId": "46af7093-91f1-462f-fb5c-63827bc3a7e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test set: 81.86%\n"
          ]
        }
      ],
      "source": [
        "# Test phase\n",
        "student_model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = student_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f'Accuracy of the model on the test set: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdlIAKt3og1P"
      },
      "outputs": [],
      "source": [
        "model = student_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmWmKWRk2iWj",
        "outputId": "752cae9d-3e58-4cee-8b62-dc3177d38473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test set: 81.86%\n"
          ]
        }
      ],
      "source": [
        "# Test phase\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f'Accuracy of the model on the test set: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UEwlRY3nbcu",
        "outputId": "2f707996-32fa-4768-d8d6-a2110920369b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 04:26:50] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 04:26:50] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 04:26:50] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 04:26:50] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 04:26:50] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 04:26:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 04:26:51] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 04:26:51] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 04:26:51]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 04:26:51]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 04:26:51]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 04:26:51]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 04:26:51]   CPU count: 2\n",
            "[codecarbon INFO @ 04:26:51]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 04:26:51]   GPU count: 1\n",
            "[codecarbon INFO @ 04:26:51]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 04:26:51] Saving emissions data to file /content/emissions.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/15 - Training:  32%|███▏      | 200/625 [00:12<00:26, 15.91it/s][codecarbon INFO @ 04:27:06] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:27:06] Energy consumed for all GPUs : 0.000270 kWh. Total GPU Power : 64.71666736682052 W\n",
            "[codecarbon INFO @ 04:27:06] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:27:06] 0.000467 kWh of electricity used since the beginning.\n",
            "Epoch 1/15 - Training:  70%|██████▉   | 436/625 [00:27<00:11, 15.91it/s][codecarbon INFO @ 04:27:21] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:27:21] Energy consumed for all GPUs : 0.000560 kWh. Total GPU Power : 69.75488604480972 W\n",
            "[codecarbon INFO @ 04:27:21] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:27:21] 0.000954 kWh of electricity used since the beginning.\n",
            "Epoch 1/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.63it/s]\n",
            "Epoch 2/15 - Training:   7%|▋         | 41/625 [00:02<00:36, 16.04it/s][codecarbon INFO @ 04:27:36] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:27:36] Energy consumed for all GPUs : 0.000848 kWh. Total GPU Power : 69.09766276022168 W\n",
            "[codecarbon INFO @ 04:27:36] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:27:36] 0.001439 kWh of electricity used since the beginning.\n",
            "Epoch 2/15 - Training:  45%|████▍     | 279/625 [00:17<00:21, 16.15it/s][codecarbon INFO @ 04:27:51] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:27:51] Energy consumed for all GPUs : 0.001139 kWh. Total GPU Power : 69.73841380769186 W\n",
            "[codecarbon INFO @ 04:27:51] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:27:51] 0.001926 kWh of electricity used since the beginning.\n",
            "Epoch 2/15 - Training:  83%|████████▎ | 521/625 [00:32<00:06, 15.97it/s][codecarbon INFO @ 04:28:06] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:28:06] Energy consumed for all GPUs : 0.001429 kWh. Total GPU Power : 69.7138702268518 W\n",
            "[codecarbon INFO @ 04:28:06] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:28:06] 0.002414 kWh of electricity used since the beginning.\n",
            "Epoch 2/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.86it/s]\n",
            "Epoch 3/15 - Training:  21%|██        | 129/625 [00:08<00:30, 16.06it/s][codecarbon INFO @ 04:28:21] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 3/15 - Training:  21%|██        | 131/625 [00:08<00:31, 15.87it/s][codecarbon INFO @ 04:28:21] Energy consumed for all GPUs : 0.001717 kWh. Total GPU Power : 69.0338517377414 W\n",
            "[codecarbon INFO @ 04:28:21] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:28:21] 0.002899 kWh of electricity used since the beginning.\n",
            "Epoch 3/15 - Training:  59%|█████▉    | 369/625 [00:23<00:16, 15.94it/s][codecarbon INFO @ 04:28:36] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:28:36] Energy consumed for all GPUs : 0.002007 kWh. Total GPU Power : 69.70118700042634 W\n",
            "[codecarbon INFO @ 04:28:36] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:28:36] 0.003386 kWh of electricity used since the beginning.\n",
            "Epoch 3/15 - Training:  97%|█████████▋| 609/625 [00:38<00:01, 15.71it/s][codecarbon INFO @ 04:28:51] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:28:51] Energy consumed for all GPUs : 0.002297 kWh. Total GPU Power : 69.67106819151583 W\n",
            "[codecarbon INFO @ 04:28:51] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:28:51] 0.003873 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:28:51] 0.011263 g.CO2eq/s mean an estimation of 355.17578579850084 kg.CO2eq/year\n",
            "Epoch 3/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.82it/s]\n",
            "Epoch 4/15 - Training:  35%|███▍      | 217/625 [00:13<00:25, 15.77it/s][codecarbon INFO @ 04:29:06] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:29:06] Energy consumed for all GPUs : 0.002586 kWh. Total GPU Power : 69.19478275971413 W\n",
            "[codecarbon INFO @ 04:29:06] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:29:06] 0.004359 kWh of electricity used since the beginning.\n",
            "Epoch 4/15 - Training:  73%|███████▎  | 455/625 [00:28<00:10, 15.62it/s][codecarbon INFO @ 04:29:21] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:29:21] Energy consumed for all GPUs : 0.002876 kWh. Total GPU Power : 69.79152208335753 W\n",
            "[codecarbon INFO @ 04:29:21] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:29:21] 0.004845 kWh of electricity used since the beginning.\n",
            "Epoch 4/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.78it/s]\n",
            "Epoch 5/15 - Training:   9%|▉         | 59/625 [00:04<00:35, 15.88it/s][codecarbon INFO @ 04:29:36] Energy consumed for RAM : 0.000218 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:29:36] Energy consumed for all GPUs : 0.003164 kWh. Total GPU Power : 69.2031930179514 W\n",
            "[codecarbon INFO @ 04:29:36] Energy consumed for all CPUs : 0.001949 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:29:36] 0.005331 kWh of electricity used since the beginning.\n",
            "Epoch 5/15 - Training:  48%|████▊     | 299/625 [00:19<00:20, 15.98it/s][codecarbon INFO @ 04:29:51] Energy consumed for RAM : 0.000238 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:29:51] Energy consumed for all GPUs : 0.003454 kWh. Total GPU Power : 69.74529308457784 W\n",
            "[codecarbon INFO @ 04:29:51] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:29:51] 0.005818 kWh of electricity used since the beginning.\n",
            "Epoch 5/15 - Training:  86%|████████▌ | 537/625 [00:34<00:05, 16.01it/s][codecarbon INFO @ 04:30:06] Energy consumed for RAM : 0.000257 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:30:06] Energy consumed for all GPUs : 0.003745 kWh. Total GPU Power : 69.64404696669364 W\n",
            "[codecarbon INFO @ 04:30:06] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:30:06] 0.006305 kWh of electricity used since the beginning.\n",
            "Epoch 5/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.69it/s]\n",
            "Epoch 6/15 - Training:  24%|██▍       | 149/625 [00:09<00:29, 15.91it/s][codecarbon INFO @ 04:30:21] Energy consumed for RAM : 0.000277 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:30:21] Energy consumed for all GPUs : 0.004034 kWh. Total GPU Power : 69.30599165186207 W\n",
            "[codecarbon INFO @ 04:30:21] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:30:21] 0.006791 kWh of electricity used since the beginning.\n",
            "Epoch 6/15 - Training:  62%|██████▏   | 387/625 [00:24<00:14, 16.04it/s][codecarbon INFO @ 04:30:36] Energy consumed for RAM : 0.000297 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:30:36] Energy consumed for all GPUs : 0.004324 kWh. Total GPU Power : 69.69613861566513 W\n",
            "[codecarbon INFO @ 04:30:36] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:30:36] 0.007278 kWh of electricity used since the beginning.\n",
            "Epoch 6/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.87it/s]\n",
            "Epoch 7/15 - Training:   0%|          | 0/625 [00:00<?, ?it/s][codecarbon INFO @ 04:30:51] Energy consumed for RAM : 0.000317 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:30:51] Energy consumed for all GPUs : 0.004614 kWh. Total GPU Power : 69.43441803733371 W\n",
            "[codecarbon INFO @ 04:30:51] Energy consumed for all CPUs : 0.002835 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:30:51] 0.007765 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:30:51] 0.011317 g.CO2eq/s mean an estimation of 356.8914492777413 kg.CO2eq/year\n",
            "Epoch 7/15 - Training:  38%|███▊      | 237/625 [00:15<00:24, 16.04it/s][codecarbon INFO @ 04:31:06] Energy consumed for RAM : 0.000337 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:31:06] Energy consumed for all GPUs : 0.004903 kWh. Total GPU Power : 69.4065244488362 W\n",
            "[codecarbon INFO @ 04:31:06] Energy consumed for all CPUs : 0.003012 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:31:06] 0.008251 kWh of electricity used since the beginning.\n",
            "Epoch 7/15 - Training:  76%|███████▋  | 477/625 [00:30<00:09, 15.97it/s][codecarbon INFO @ 04:31:21] Energy consumed for RAM : 0.000356 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:31:21] Energy consumed for all GPUs : 0.005193 kWh. Total GPU Power : 69.71326771478208 W\n",
            "[codecarbon INFO @ 04:31:21] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:31:21] 0.008739 kWh of electricity used since the beginning.\n",
            "Epoch 7/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.82it/s]\n",
            "Epoch 8/15 - Training:  14%|█▍        | 87/625 [00:05<00:33, 15.93it/s][codecarbon INFO @ 04:31:36] Energy consumed for RAM : 0.000376 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:31:36] Energy consumed for all GPUs : 0.005482 kWh. Total GPU Power : 69.38062873081913 W\n",
            "[codecarbon INFO @ 04:31:36] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:31:36] 0.009224 kWh of electricity used since the beginning.\n",
            "Epoch 8/15 - Training:  52%|█████▏    | 327/625 [00:20<00:18, 16.04it/s][codecarbon INFO @ 04:31:51] Energy consumed for RAM : 0.000396 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:31:51] Energy consumed for all GPUs : 0.005775 kWh. Total GPU Power : 70.16004184786502 W\n",
            "[codecarbon INFO @ 04:31:51] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:31:51] 0.009714 kWh of electricity used since the beginning.\n",
            "Epoch 8/15 - Training:  90%|█████████ | 565/625 [00:35<00:03, 15.90it/s][codecarbon INFO @ 04:32:07] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:32:07] Energy consumed for all GPUs : 0.006065 kWh. Total GPU Power : 69.58605020474393 W\n",
            "[codecarbon INFO @ 04:32:07] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:32:07] 0.010201 kWh of electricity used since the beginning.\n",
            "Epoch 8/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.84it/s]\n",
            "Epoch 9/15 - Training:  28%|██▊       | 177/625 [00:11<00:28, 15.84it/s][codecarbon INFO @ 04:32:22] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:32:22] Energy consumed for all GPUs : 0.006354 kWh. Total GPU Power : 69.3496823014464 W\n",
            "[codecarbon INFO @ 04:32:22] Energy consumed for all CPUs : 0.003898 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:32:22] 0.010687 kWh of electricity used since the beginning.\n",
            "Epoch 9/15 - Training:  66%|██████▋   | 415/625 [00:26<00:13, 15.76it/s][codecarbon INFO @ 04:32:37] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:32:37] Energy consumed for all GPUs : 0.006644 kWh. Total GPU Power : 69.77193877561636 W\n",
            "[codecarbon INFO @ 04:32:37] Energy consumed for all CPUs : 0.004075 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:32:37] 0.011174 kWh of electricity used since the beginning.\n",
            "Epoch 9/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.81it/s]\n",
            "Epoch 10/15 - Training:   4%|▎         | 23/625 [00:01<00:38, 15.67it/s][codecarbon INFO @ 04:32:52] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:32:52] Energy consumed for all GPUs : 0.006932 kWh. Total GPU Power : 69.11574520520792 W\n",
            "[codecarbon INFO @ 04:32:52] Energy consumed for all CPUs : 0.004252 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:32:52] 0.011658 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:32:52] 0.011323 g.CO2eq/s mean an estimation of 357.07847951080004 kg.CO2eq/year\n",
            "Epoch 10/15 - Training:  42%|████▏     | 261/625 [00:16<00:22, 15.95it/s][codecarbon INFO @ 04:33:07] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:33:07] Energy consumed for all GPUs : 0.007222 kWh. Total GPU Power : 69.71933881484432 W\n",
            "[codecarbon INFO @ 04:33:07] Energy consumed for all CPUs : 0.004429 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:33:07] 0.012145 kWh of electricity used since the beginning.\n",
            "Epoch 10/15 - Training:  80%|███████▉  | 499/625 [00:31<00:07, 16.09it/s][codecarbon INFO @ 04:33:22] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:33:22] Energy consumed for all GPUs : 0.007512 kWh. Total GPU Power : 69.62647292738164 W\n",
            "[codecarbon INFO @ 04:33:22] Energy consumed for all CPUs : 0.004606 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:33:22] 0.012633 kWh of electricity used since the beginning.\n",
            "Epoch 10/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.79it/s]\n",
            "Epoch 11/15 - Training:  18%|█▊        | 111/625 [00:07<00:31, 16.07it/s][codecarbon INFO @ 04:33:37] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:33:37] Energy consumed for all GPUs : 0.007801 kWh. Total GPU Power : 69.38631061726532 W\n",
            "[codecarbon INFO @ 04:33:37] Energy consumed for all CPUs : 0.004783 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:33:37] 0.013119 kWh of electricity used since the beginning.\n",
            "Epoch 11/15 - Training:  56%|█████▌    | 349/625 [00:22<00:17, 16.04it/s][codecarbon INFO @ 04:33:52] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:33:52] Energy consumed for all GPUs : 0.008091 kWh. Total GPU Power : 69.64953490867516 W\n",
            "[codecarbon INFO @ 04:33:52] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:33:52] 0.013605 kWh of electricity used since the beginning.\n",
            "Epoch 11/15 - Training:  94%|█████████▍| 587/625 [00:37<00:02, 15.96it/s][codecarbon INFO @ 04:34:07] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:34:07] Energy consumed for all GPUs : 0.008381 kWh. Total GPU Power : 69.7232124635411 W\n",
            "[codecarbon INFO @ 04:34:07] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:34:07] 0.014093 kWh of electricity used since the beginning.\n",
            "Epoch 11/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.83it/s]\n",
            "Epoch 12/15 - Training:  32%|███▏      | 199/625 [00:12<00:26, 15.94it/s][codecarbon INFO @ 04:34:22] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:34:22] Energy consumed for all GPUs : 0.008670 kWh. Total GPU Power : 69.34706312212289 W\n",
            "[codecarbon INFO @ 04:34:22] Energy consumed for all CPUs : 0.005314 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:34:22] 0.014579 kWh of electricity used since the beginning.\n",
            "Epoch 12/15 - Training:  70%|██████▉   | 437/625 [00:27<00:11, 15.99it/s][codecarbon INFO @ 04:34:37] Energy consumed for RAM : 0.000614 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:34:37] Energy consumed for all GPUs : 0.008963 kWh. Total GPU Power : 70.10839286236332 W\n",
            "[codecarbon INFO @ 04:34:37] Energy consumed for all CPUs : 0.005492 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:34:37] 0.015068 kWh of electricity used since the beginning.\n",
            "Epoch 12/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.83it/s]\n",
            "Epoch 13/15 - Training:   8%|▊         | 47/625 [00:03<00:36, 15.95it/s][codecarbon INFO @ 04:34:52] Energy consumed for RAM : 0.000633 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:34:52] Energy consumed for all GPUs : 0.009251 kWh. Total GPU Power : 69.19792621602389 W\n",
            "[codecarbon INFO @ 04:34:52] Energy consumed for all CPUs : 0.005669 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:34:52] 0.015554 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:34:52] 0.011326 g.CO2eq/s mean an estimation of 357.1759877085306 kg.CO2eq/year\n",
            "Epoch 13/15 - Training:  46%|████▌     | 287/625 [00:18<00:21, 16.05it/s][codecarbon INFO @ 04:35:07] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:35:07] Energy consumed for all GPUs : 0.009541 kWh. Total GPU Power : 69.67877515582427 W\n",
            "[codecarbon INFO @ 04:35:07] Energy consumed for all CPUs : 0.005846 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:35:07] 0.016040 kWh of electricity used since the beginning.\n",
            "Epoch 13/15 - Training:  84%|████████▍ | 525/625 [00:33<00:06, 16.05it/s][codecarbon INFO @ 04:35:22] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:35:22] Energy consumed for all GPUs : 0.009832 kWh. Total GPU Power : 69.67512192627638 W\n",
            "[codecarbon INFO @ 04:35:22] Energy consumed for all CPUs : 0.006023 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:35:22] 0.016528 kWh of electricity used since the beginning.\n",
            "Epoch 13/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.79it/s]\n",
            "Epoch 14/15 - Training:  22%|██▏       | 135/625 [00:08<00:31, 15.74it/s][codecarbon INFO @ 04:35:37] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:35:37] Energy consumed for all GPUs : 0.010120 kWh. Total GPU Power : 69.2850212459673 W\n",
            "[codecarbon INFO @ 04:35:37] Energy consumed for all CPUs : 0.006200 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:35:37] 0.017013 kWh of electricity used since the beginning.\n",
            "Epoch 14/15 - Training:  60%|█████▉    | 373/625 [00:23<00:15, 15.92it/s][codecarbon INFO @ 04:35:52] Energy consumed for RAM : 0.000713 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:35:52] Energy consumed for all GPUs : 0.010411 kWh. Total GPU Power : 69.70406829974621 W\n",
            "[codecarbon INFO @ 04:35:52] Energy consumed for all CPUs : 0.006377 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:35:52] 0.017501 kWh of electricity used since the beginning.\n",
            "Epoch 14/15 - Training:  98%|█████████▊| 611/625 [00:38<00:00, 15.41it/s][codecarbon INFO @ 04:36:07] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "Epoch 14/15 - Training:  98%|█████████▊| 613/625 [00:38<00:00, 15.37it/s][codecarbon INFO @ 04:36:07] Energy consumed for all GPUs : 0.010701 kWh. Total GPU Power : 69.6004645381914 W\n",
            "[codecarbon INFO @ 04:36:07] Energy consumed for all CPUs : 0.006555 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:36:07] 0.017988 kWh of electricity used since the beginning.\n",
            "Epoch 14/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.80it/s]\n",
            "Epoch 15/15 - Training:  35%|███▌      | 219/625 [00:14<00:26, 15.46it/s][codecarbon INFO @ 04:36:22] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:36:22] Energy consumed for all GPUs : 0.010988 kWh. Total GPU Power : 69.17918177040804 W\n",
            "[codecarbon INFO @ 04:36:22] Energy consumed for all CPUs : 0.006731 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:36:22] 0.018472 kWh of electricity used since the beginning.\n",
            "Epoch 15/15 - Training:  73%|███████▎  | 457/625 [00:29<00:10, 15.70it/s][codecarbon INFO @ 04:36:37] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:36:37] Energy consumed for all GPUs : 0.011278 kWh. Total GPU Power : 69.72757628889993 W\n",
            "[codecarbon INFO @ 04:36:37] Energy consumed for all CPUs : 0.006908 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:36:37] 0.018959 kWh of electricity used since the beginning.\n",
            "Epoch 15/15 - Training: 100%|██████████| 625/625 [00:39<00:00, 15.75it/s]\n",
            "[codecarbon INFO @ 04:36:51] Energy consumed for RAM : 0.000791 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 04:36:51] Energy consumed for all GPUs : 0.011552 kWh. Total GPU Power : 67.70119068567189 W\n",
            "[codecarbon INFO @ 04:36:51] Energy consumed for all CPUs : 0.007080 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 04:36:51] 0.019423 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 04:36:51] 0.011300 g.CO2eq/s mean an estimation of 356.34999552482196 kg.CO2eq/year\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model Analysis After Pruning ---\n",
            "Non-zero Parameter Count: 12890591\n",
            "Model Size: 491.37 MB\n",
            "Training Time: 9.89 minutes\n",
            "Inference Time per Image: 0.025262 seconds\n",
            "FLOPs: 0.27 GFLOPs\n",
            "CO2 Emissions: 0.006783 kg\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import models, datasets, transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "from ptflops import get_model_complexity_info\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import codecarbon\n",
        "\n",
        "# Initialize CodeCarbon tracker\n",
        "tracker = codecarbon.EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the transforms for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Load the pretrained VGG16 model and modify the classifier\n",
        "# model = models.vgg16(pretrained=True)\n",
        "# input_last_layer = model.classifier[6].in_features\n",
        "# model.classifier[6] = nn.Linear(input_last_layer, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Function to apply L1 unstructured pruning\n",
        "def apply_pruning(model, amount=0.90):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "\n",
        "# Function to remove pruning masks and make pruning permanent\n",
        "def remove_pruning(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'weight_mask'):\n",
        "                prune.remove(module, 'weight')\n",
        "    return model\n",
        "\n",
        "# Apply pruning to the model\n",
        "apply_pruning(model, amount=0.90)\n",
        "\n",
        "# Measure FLOPs using ptflops\n",
        "def calculate_flops(model):\n",
        "    model.eval()\n",
        "    flops, params = get_model_complexity_info(model, (3, 32, 32), as_strings=False, print_per_layer_stat=False)\n",
        "    return flops, params\n",
        "\n",
        "# Measure inference time\n",
        "def measure_inference_time(model, dataloader):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = images.to(device)\n",
        "            _ = model(images)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / len(dataloader)\n",
        "\n",
        "# Count non-zero parameters after pruning\n",
        "def count_non_zero_params(model):\n",
        "    non_zero_count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            non_zero_count += torch.count_nonzero(param).item()\n",
        "    return non_zero_count\n",
        "\n",
        "# Training loop\n",
        "train_start_time = time.time()\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "train_end_time = time.time()\n",
        "training_time = (train_end_time - train_start_time) / 60\n",
        "\n",
        "# Remove pruning masks and make pruning permanent\n",
        "model = remove_pruning(model)\n",
        "\n",
        "# Calculate FLOPs, parameter count, and inference time after pruning\n",
        "flops, params = calculate_flops(model)\n",
        "param_count = count_non_zero_params(model)\n",
        "inference_time = measure_inference_time(model, test_loader)\n",
        "\n",
        "# Stop CodeCarbon tracker\n",
        "emissions = tracker.stop()\n",
        "\n",
        "# Save the pruned model\n",
        "torch.save(model.state_dict(), 'pruned_vgg16_model.pth')\n",
        "\n",
        "# Calculate model size\n",
        "model_size = os.path.getsize('pruned_vgg16_model.pth') / (1024 * 1024)\n",
        "\n",
        "# Print the final results\n",
        "print(\"\\n--- Model Analysis After Pruning ---\")\n",
        "print(f\"Non-zero Parameter Count: {param_count}\")\n",
        "print(f\"Model Size: {model_size:.2f} MB\")\n",
        "print(f\"Training Time: {training_time:.2f} minutes\")\n",
        "print(f\"Inference Time per Image: {inference_time:.6f} seconds\")\n",
        "print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "print(f\"CO2 Emissions: {emissions:.6f} kg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7V4W8nVre9H",
        "outputId": "4f14185b-6545-46d4-ba8e-362c9a55610a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.4803,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.1471,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.6428]]],\n",
            "\n",
            "\n",
            "        [[[-0.8888, -0.0000, -0.0000],\n",
            "          [ 0.7784,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.4308,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0458, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0034, -0.0000],\n",
            "          [-0.0000, -0.0283, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.1917],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.1825,  0.2759,  0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0107,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0538,  0.0000]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.2532,  0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.1131,  0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0622]],\n",
            "\n",
            "         [[ 0.0000,  0.2223,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0785, -0.1015, -0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0636,  0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0000, -0.0807, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0818,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0876, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0814, -0.1424, -0.0000],\n",
            "          [-0.0000,  0.0000, -0.1612]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0675, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0657, -0.0000],\n",
            "          [-0.0910, -0.0755, -0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.0000,  0.0000],\n",
            "          [-0.1220, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0823,  0.1018,  0.0000]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000,  0.0888],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.2238,  0.0000,  0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0707,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0632]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000],\n",
            "          [-0.0990,  0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0824, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000],\n",
            "          [-0.0674, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000,  0.0000],\n",
            "          [-0.1153, -0.0000, -0.0000],\n",
            "          [-0.1268, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000, -0.0000, -0.0000],\n",
            "          [-0.1003, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0560,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0386, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0421, -0.0000,  0.1121],\n",
            "          [ 0.0000, -0.0000, -0.0000]],\n",
            "\n",
            "         [[-0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000]]]], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(model.features[0].weight)\n",
        "print(model.features[3].weight)\n",
        "#print(model.features[2].weight)\n",
        "#print(model.features[5].weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGJUeMfFnbHJ",
        "outputId": "6d1ec39f-bd19-4439-eead-ec03349b6c4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test set: 73.71%\n"
          ]
        }
      ],
      "source": [
        "# Test phase\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f'Accuracy of the model on the test set: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNfQ8tKqna9K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}